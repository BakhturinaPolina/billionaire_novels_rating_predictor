{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOutWw9nXy1rfm1IC7jhap4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n","# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/pip-install.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qrG1Q7pPQpWw","executionInfo":{"status":"ok","timestamp":1730559294278,"user_tz":-60,"elapsed":3409,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"aade5078-0f9e-4b52-edc3-83170925747e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n","Installing RAPIDS remaining 24.10.* libraries\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Requirement already satisfied: cudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.1)\n","Requirement already satisfied: cuml-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: cugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: cuspatial-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: cuproj-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: cuxfilter-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: cucim-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: pylibraft-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: raft-dask-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: nx-cugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (5.5.0)\n","Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (12.2.1)\n","Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (12.2.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (2024.10.0)\n","Requirement already satisfied: libcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.1)\n","Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.60.0)\n","Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (1.26.4)\n","Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.2.10)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.1)\n","Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (2.2.2)\n","Requirement already satisfied: pyarrow<18.0.0a0,>=14.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (17.0.0)\n","Requirement already satisfied: pylibcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.1)\n","Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (13.9.3)\n","Requirement already satisfied: rmm-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (4.12.2)\n","Requirement already satisfied: cuvs-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: dask-cuda==24.10.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: dask-cudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (24.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (1.4.2)\n","Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (12.6.3.3)\n","Requirement already satisfied: nvidia-cufft-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (12.5.4.2)\n","Requirement already satisfied: rapids-dask-dependency==24.10.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (1.13.1)\n","Requirement already satisfied: treelite==4.3.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (4.3.0)\n","Requirement already satisfied: pylibcugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cugraph-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: ucx-py-cu12==0.40.* in /usr/local/lib/python3.10/dist-packages (from cugraph-cu12==24.10.*) (0.40.0)\n","Requirement already satisfied: geopandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.10.*) (1.0.1)\n","Requirement already satisfied: libcuspatial-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.10.*) (24.10.0)\n","Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (3.4.3)\n","Requirement already satisfied: datashader>=0.15 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (0.16.3)\n","Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (1.19.1)\n","Requirement already satisfied: jupyter-server-proxy in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (4.4.0)\n","Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (1.4.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (8.1.7)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (0.4)\n","Requirement already satisfied: scikit-image<0.25.0a0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (0.24.0)\n","Requirement already satisfied: distributed-ucxx-cu12==0.40.* in /usr/local/lib/python3.10/dist-packages (from raft-dask-cu12==24.10.*) (0.40.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12==24.10.*) (3.4.2)\n","Requirement already satisfied: pynvml<11.5,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.10.*->cuml-cu12==24.10.*) (11.4.1)\n","Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.10.*->cuml-cu12==24.10.*) (3.0.0)\n","Requirement already satisfied: ucxx-cu12==0.40.* in /usr/local/lib/python3.10/dist-packages (from distributed-ucxx-cu12==0.40.*->raft-dask-cu12==24.10.*) (0.40.0)\n","Requirement already satisfied: dask==2024.9.0 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (2024.9.0)\n","Requirement already satisfied: distributed==2024.9.0 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (2024.9.0)\n","Requirement already satisfied: dask-expr==1.1.14 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.1.14)\n","Requirement already satisfied: libucx-cu12<1.18,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from ucx-py-cu12==0.40.*->cugraph-cu12==24.10.*) (1.17.0)\n","Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.1.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (6.0.2)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (8.5.0)\n","Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.1.4)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.1.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (5.9.5)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.0.0)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (6.3.3)\n","Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (2.2.3)\n","Requirement already satisfied: libucxx-cu12==0.40.* in /usr/local/lib/python3.10/dist-packages (from ucxx-cu12==0.40.*->distributed-ucxx-cu12==0.40.*->raft-dask-cu12==24.10.*) (0.40.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n","Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (1.3.0)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (10.4.0)\n","Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (2024.9.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12==24.10.*) (3.0.11)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.10.*) (0.8.2)\n","Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (3.1.0)\n","Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (1.0.0)\n","Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2.1.1)\n","Requirement already satisfied: pyct in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (0.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2.32.3)\n","Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2024.10.0)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (0.10.0)\n","Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (3.7.0)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (2.0.6)\n","Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.10.*) (3.0.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12==24.10.*) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2024.2)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (3.7)\n","Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (3.0.0)\n","Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (2.0.3)\n","Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (0.4.2)\n","Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (4.66.6)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (6.2.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.25.0a0,>=0.19.0->cucim-cu12==24.10.*) (2.36.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.25.0a0,>=0.19.0->cucim-cu12==24.10.*) (2024.9.20)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n","Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.24.0)\n","Requirement already satisfied: simpervisor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.0.0)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.7.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cufft-cu12->cuml-cu12==24.10.*) (12.6.77)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12==24.10.*) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.0.2)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (3.7.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (23.1.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.7.2)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (7.16.4)\n","Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.10.4)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.21.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (24.0.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.18.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.8.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.10.*) (0.1.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas>=1.0.0->cuspatial-cu12==24.10.*) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (1.16.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.10.*) (0.5.1)\n","Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.10.*) (1.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.10.*) (3.4.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.2.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.20.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.3.6)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.12.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (3.0.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.4.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.23.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (21.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.20.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.15.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.22)\n","\n","        ***********************************************************************\n","        The pip install of RAPIDS is complete.\n","        \n","        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n","        \n","        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n","\n","        Troubleshooting:\n","            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n","            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n","        ***********************************************************************\n","        \n"]}]},{"cell_type":"code","source":["# Install other required libraries\n","!pip install bertopic==0.16.3\n","!pip install sentence-transformers\n","!pip install gensim\n","!pip install nltk\n","!pip install scikit-learn==1.0.2\n","\n","# Import necessary libraries\n","import cudf\n","import cuml\n","import cupy as cp\n","from cuml.manifold import UMAP\n","from cuml.cluster import HDBSCAN\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","import os\n","import re\n","import pandas as pd\n","import numpy as np\n","import json\n","import warnings\n","\n","# Import BERTopic and its components\n","import bertopic  # Import the entire bertopic module\n","from bertopic import BERTopic\n","from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n","\n","from sentence_transformers import SentenceTransformer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim import corpora\n","from gensim.models import CoherenceModel\n","\n","# Verify BERTopic version\n","print(f\"BERTopic version: {bertopic.__version__}\")\n","\n","# Define the paths for datasets, models, and resources\n","dataset_path = '/content/drive/MyDrive/processed_novels_sentences_new.csv'\n","additional_stop_words_characters_names = '/content/drive/MyDrive/character_names.txt'\n","dir_with_trained_models = '/content/drive/MyDrive/models_best_ten'\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 1. Load additional stop words (character names) and standard English stop words\n","print(\"Loading additional stop words and standard English stop words...\")\n","with open(additional_stop_words_characters_names, 'r', encoding='utf-8') as file:\n","    custom_stop_words = file.read().splitlines()\n","stop_words = set(stopwords.words('english'))\n","stop_words.update(custom_stop_words)\n","print(f\"Stop words loaded. Total stop words: {len(stop_words)}\")\n","\n","# 2. Load and preprocess the dataset\n","print(\"Loading dataset...\")\n","df = pd.read_csv(dataset_path)\n","print(\"Dataset loaded.\")\n","\n","print(\"Preprocessing sentences...\")\n","# Remove newline characters and extra spaces, convert to lowercase\n","df['Sentence'] = df['Sentence'].astype(str).apply(lambda x: re.sub(r'\\n+', ' ', x))\n","df['Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip().lower())\n","print(\"Sentences preprocessed.\")\n","\n","# List of sentence strings\n","dataset_as_list_of_strings = df['Sentence'].tolist()\n","print(f\"Total sentences in dataset: {df.shape[0]}\")\n","\n","# 3. Tokenize sentences and remove stop words\n","print(\"Tokenizing sentences and removing stop words...\")\n","processed_docs = []\n","for sentence in dataset_as_list_of_strings:\n","    tokens = word_tokenize(sentence)\n","    # Keep only alphabetic tokens and remove stop words\n","    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n","    processed_docs.append(tokens)\n","print(\"Tokenization and stop word removal completed.\")\n","\n","# 4. Create a dictionary and corpus for coherence model\n","print(\"Creating dictionary and corpus for coherence model...\")\n","dictionary = corpora.Dictionary(processed_docs)\n","corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","print(\"Dictionary and corpus created.\")\n","\n","# 5. Define model parameters and create a DataFrame mapping models to their parameters\n","# Model filenames\n","model_files = [\n","    'bertopic_model_0_iter_66_20241101_231419.pkl',\n","    'bertopic_model_1_iter_14_20241101_232426.pkl',\n","    'bertopic_model_2_iter_75_20241101_233115.pkl',\n","    'bertopic_model_3_iter_0_20241101_234120.pkl',\n","    'bertopic_model_4_iter_19_20241101_234832.pkl',\n","    'bertopic_model_5_iter_13_20241101_235837.pkl',\n","    'bertopic_model_6_iter_23_20241102_000842.pkl',\n","    'bertopic_model_7_iter_67_20241102_001547.pkl',\n","    'bertopic_model_8_iter_28_20241102_002548.pkl',\n","    'bertopic_model_9_iter_11_20241102_003549.pkl'\n","]\n","\n","# Corresponding embedding models\n","embedding_models = [\n","    'all-MiniLM-L12-v2',\n","    'paraphrase-mpnet-base-v2',\n","    'all-MiniLM-L12-v2',\n","    'paraphrase-mpnet-base-v2',\n","    'paraphrase-MiniLM-L6-v2',\n","    'paraphrase-mpnet-base-v2',\n","    'multi-qa-mpnet-base-cos-v1',\n","    'all-MiniLM-L12-v2',\n","    'multi-qa-mpnet-base-cos-v1',\n","    'multi-qa-mpnet-base-cos-v1'\n","]\n","\n","# Iterations\n","iterations = [66, 14, 75, 0, 19, 13, 23, 67, 28, 11]\n","\n","# 'bertopic__top_n_words' parameter for each model (set to 10 for all to avoid mismatches)\n","bertopic_top_n_words = [10] * 10\n","\n","# UMAP and HDBSCAN parameters\n","umap_n_neighbors = [7, 11, 15, 11, 44, 9, 19, 42, 18, 14]\n","umap_n_components = [2, 9, 5, 10, 9, 8, 9, 7, 9, 8]\n","umap_min_dist = [0.005022, 0.077818, 0.004634, 0.058341, 0.085702, 0.086975, 0.095922, 0.004852, 0.008103, 0.022149]\n","hdbscan_min_cluster_size = [281, 500, 473, 494, 143, 497, 492, 258, 427, 497]\n","hdbscan_min_samples = [72, 72, 14, 28, 32, 32, 12, 37, 11, 13]\n","\n","# Ensure that all lists are of the same length\n","assert len(model_files) == len(embedding_models) == len(iterations) == len(bertopic_top_n_words) == len(umap_n_neighbors) == len(umap_n_components) == len(umap_min_dist) == len(hdbscan_min_cluster_size) == len(hdbscan_min_samples), \"List lengths do not match.\"\n","\n","# Create a DataFrame with model information\n","data = {\n","    'model_name': model_files,\n","    'embedding_model': embedding_models,\n","    'iteration': iterations,\n","    'bertopic_top_n_words': bertopic_top_n_words,\n","    'umap__n_neighbors': umap_n_neighbors,\n","    'umap__n_components': umap_n_components,\n","    'umap__min_dist': umap_min_dist,\n","    'hdbscan__min_cluster_size': hdbscan_min_cluster_size,\n","    'hdbscan__min_samples': hdbscan_min_samples\n","}\n","\n","model_embedding_df = pd.DataFrame(data)\n","print(\"Model and Embedding DataFrame created:\\n\", model_embedding_df)\n","\n","# 6. Initialize a DataFrame to store metrics for all models\n","all_models_metrics = pd.DataFrame()\n","\n","# 7. Process each model\n","print(\"Starting post-processing for each model...\")\n","for index, row in model_embedding_df.iterrows():\n","    model_filename = row['model_name']\n","    embedding_model_name = row['embedding_model']\n","    iteration = row['iteration']\n","    top_n_words = row['bertopic_top_n_words']\n","    print(f\"\\nProcessing model: {model_filename}\")\n","    model_path = os.path.join(dir_with_trained_models, model_filename)\n","\n","    print(f\"Using embedding model '{embedding_model_name}' for model '{model_filename}'\")\n","\n","    # Load the embedding model\n","    try:\n","        print(\"Loading embedding model...\")\n","        embedding_model = SentenceTransformer(embedding_model_name)\n","        print(\"Embedding model loaded.\")\n","    except Exception as e:\n","        print(f\"Error loading embedding model '{embedding_model_name}': {e}\")\n","        continue  # Skip to the next model\n","\n","    # Load the BERTopic model with the embedding model\n","    try:\n","        print(\"Loading BERTopic model...\")\n","        topic_model = BERTopic.load(model_path, embedding_model=embedding_model)\n","        print(\"BERTopic model loaded.\")\n","\n","        # Recreate UMAP and HDBSCAN models with parameters\n","        umap_params = {\n","            'n_neighbors': int(row['umap__n_neighbors']),\n","            'n_components': int(row['umap__n_components']),\n","            'min_dist': float(row['umap__min_dist']),\n","            'metric': 'cosine',\n","            'random_state': 42\n","        }\n","        hdbscan_params = {\n","            'min_cluster_size': int(row['hdbscan__min_cluster_size']),\n","            'min_samples': int(row['hdbscan__min_samples']),\n","            'cluster_selection_method': 'eom',\n","            'prediction_data': True,\n","            'gen_min_span_tree': True\n","        }\n","        umap_model = UMAP(**umap_params)\n","        hdbscan_model = HDBSCAN(**hdbscan_params)\n","        topic_model.umap_model = umap_model\n","        topic_model.hdbscan_model = hdbscan_model\n","    except Exception as e:\n","        print(f\"Error loading BERTopic model '{model_filename}': {e}\")\n","        continue  # Skip to the next model\n","\n","    # Proceed with updating topics\n","    try:\n","        print(\"Updating topics in the topic model...\")\n","        topic_model.update_topics(dataset_as_list_of_strings)\n","        print(\"Topics updated.\")\n","    except Exception as e:\n","        print(f\"Error updating topics for model '{model_filename}': {e}\")\n","        continue  # Skip to the next model\n","\n","    # Initialize a list to store metrics for this model\n","    model_metrics = []\n","\n","    # Apply KeyBERT-Inspired representation\n","    print(\"Applying KeyBERT-Inspired representation...\")\n","    keybert_repr = KeyBERTInspired(top_n_words=top_n_words)\n","    try:\n","        topic_model.update_topics(dataset_as_list_of_strings, representation_model=keybert_repr)\n","        keybert_topics = topic_model.get_topics()\n","    except Exception as e:\n","        print(f\"Error applying KeyBERT-Inspired representation for model '{model_filename}': {e}\")\n","        keybert_topics = {}\n","\n","    # Save KeyBERT-Inspired topics if available\n","    if keybert_topics:\n","        # Directory to save processed topics for this model\n","        output_dir = f\"/content/drive/MyDrive/processed_topics_{model_filename.split('.')[0]}\"\n","        os.makedirs(output_dir, exist_ok=True)\n","        print(f\"Created directory for processed topics: {output_dir}\")\n","\n","        # Save KeyBERT-Inspired processed topics\n","        repr_name = 'keybert_inspired'\n","        topics = keybert_topics\n","        print(f\"Saving {repr_name} processed topics...\")\n","        csv_data = []\n","        json_data = {}\n","        for topic_id, topic_words in topics.items():\n","            # Filter out any empty strings or non-alphabetic tokens\n","            words = [word for word, _ in topic_words if word.isalpha()]\n","            if not words:\n","                continue  # Skip topics with no valid words\n","            csv_data.append({\n","                \"model_name\": model_filename,\n","                \"protocol\": repr_name,\n","                \"topic_id\": topic_id,\n","                \"topic_words\": \", \".join(words)\n","            })\n","            json_data[str(topic_id)] = words\n","\n","        # Save CSV\n","        csv_df = pd.DataFrame(csv_data)\n","        csv_output_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{repr_name}.csv\")\n","        csv_df.to_csv(csv_output_path, index=False)\n","\n","        # Save JSON\n","        json_output_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{repr_name}.json\")\n","        with open(json_output_path, \"w\") as json_file:\n","            json.dump(json_data, json_file, indent=4)\n","        print(f\"{repr_name} processed topics saved.\")\n","    else:\n","        print(f\"No KeyBERT-Inspired topics available for model '{model_filename}'. Skipping saving.\")\n","\n","    # Apply Maximal Marginal Relevance representation\n","    print(\"Applying Maximal Marginal Relevance representation...\")\n","    mmr_repr = MaximalMarginalRelevance(top_n_words=top_n_words)\n","    try:\n","        topic_model.update_topics(dataset_as_list_of_strings, representation_model=mmr_repr)\n","        mmr_topics = topic_model.get_topics()\n","    except Exception as e:\n","        print(f\"Error applying Maximal Marginal Relevance representation for model '{model_filename}': {e}\")\n","        mmr_topics = {}\n","\n","    # Save MMR processed topics if available\n","    if mmr_topics:\n","        repr_name = 'mmr'\n","        topics = mmr_topics\n","        print(f\"Saving {repr_name} processed topics...\")\n","        csv_data = []\n","        json_data = {}\n","        for topic_id, topic_words in topics.items():\n","            # Filter out any empty strings or non-alphabetic tokens\n","            words = [word for word, _ in topic_words if word.isalpha()]\n","            if not words:\n","                continue  # Skip topics with no valid words\n","            csv_data.append({\n","                \"model_name\": model_filename,\n","                \"protocol\": repr_name,\n","                \"topic_id\": topic_id,\n","                \"topic_words\": \", \".join(words)\n","            })\n","            json_data[str(topic_id)] = words\n","\n","        # Save CSV\n","        csv_df = pd.DataFrame(csv_data)\n","        csv_output_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{repr_name}.csv\")\n","        csv_df.to_csv(csv_output_path, index=False)\n","\n","        # Save JSON\n","        json_output_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{repr_name}.json\")\n","        with open(json_output_path, \"w\") as json_file:\n","            json.dump(json_data, json_file, indent=4)\n","        print(f\"{repr_name} processed topics saved.\")\n","    else:\n","        print(f\"No MMR topics available for model '{model_filename}'. Skipping saving.\")\n","\n","    # Apply POS filters and save for each protocol\n","    pos_protocols = {\n","        'nouns': [\"NOUN\"],\n","        'nouns_verbs': [\"NOUN\", \"VERB\"],\n","        'nouns_adjectives': [\"NOUN\", \"ADJ\"]\n","    }\n","\n","    # List of representations to process\n","    representations = ['keybert_inspired', 'mmr'] + list(pos_protocols.keys())\n","    # Dictionary to store topics for each representation\n","    topics_dict = {'keybert_inspired': keybert_topics, 'mmr': mmr_topics}\n","\n","    for protocol_name, pos_tags in pos_protocols.items():\n","        print(f\"Applying POS filtering protocol: {protocol_name}\")\n","        # Create pos_patterns from pos_tags\n","        pos_patterns = [[{\"POS\": tag}] for tag in pos_tags]\n","        # Create a PartOfSpeech representation model with specified POS patterns\n","        pos_repr = PartOfSpeech(top_n_words=top_n_words, pos_patterns=pos_patterns)\n","        try:\n","            topic_model.update_topics(dataset_as_list_of_strings, representation_model=pos_repr)\n","            # Get the topics after applying POS filtering\n","            pos_topics = topic_model.get_topics()\n","            topics_dict[protocol_name] = pos_topics\n","        except Exception as e:\n","            print(f\"Error applying PartOfSpeech representation '{protocol_name}' for model '{model_filename}': {e}\")\n","            topics_dict[protocol_name] = {}\n","            continue  # Skip to the next representation\n","\n","        # Save POS filtered topics if available\n","        if pos_topics:\n","            csv_data = []\n","            json_data = {}\n","            for topic_id, topic_words in pos_topics.items():\n","                # Filter out any empty strings or non-alphabetic tokens\n","                words = [word for word, _ in topic_words if word.isalpha()]\n","                if not words:\n","                    continue  # Skip topics with no valid words\n","                csv_data.append({\n","                    \"model_name\": model_filename,\n","                    \"protocol\": protocol_name,\n","                    \"topic_id\": topic_id,\n","                    \"topic_words\": \", \".join(words)\n","                })\n","                json_data[str(topic_id)] = words\n","\n","            # Save CSV file for the protocol\n","            pos_filtered_path_csv = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{protocol_name}.csv\")\n","            csv_df = pd.DataFrame(csv_data)\n","            csv_df.to_csv(pos_filtered_path_csv, index=False)\n","\n","            # Save JSON file for the protocol\n","            pos_filtered_path_json = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{protocol_name}.json\")\n","            with open(pos_filtered_path_json, \"w\") as json_file:\n","                json.dump(json_data, json_file, indent=4)\n","\n","            print(f\"POS filtered topics saved for protocol: {protocol_name}\")\n","        else:\n","            print(f\"No POS filtered topics available for protocol '{protocol_name}' in model '{model_filename}'. Skipping saving.\")\n","\n","    print(f\"Post-processing completed and saved for model: {model_filename}\")\n","\n","    # Calculate coherence and diversity metrics\n","    print(f\"\\nCalculating coherence and diversity for model: {model_filename}\")\n","    try:\n","        for repr_name in representations:\n","            print(f\"Calculating metrics for representation: {repr_name}\")\n","            # Get the topics for the current representation\n","            topics = topics_dict.get(repr_name, {})\n","\n","            if not topics:\n","                print(f\"No topics found for representation: {repr_name}\")\n","                continue  # Skip if no topics are available\n","\n","            # Prepare the topic words using the top_n_words parameter\n","            topic_words_list = []\n","            per_topic_metrics = []  # List to store per-topic metrics\n","\n","            for topic_id, words in topics.items():\n","                # Get the top N words for the topic and filter out invalid words\n","                topic_words = [word for word, _ in words[:top_n_words] if word.isalpha()]\n","                if not topic_words:\n","                    continue  # Skip topics with no valid words\n","                topic_words_list.append(topic_words)\n","\n","                # Calculate per-topic coherence using Gensim's CoherenceModel with 'c_v'\n","                coherence_model_topic = CoherenceModel(topics=[topic_words],\n","                                                       texts=processed_docs,\n","                                                       dictionary=dictionary,\n","                                                       coherence='c_v')\n","                coherence_score_topic = coherence_model_topic.get_coherence()\n","\n","                # Calculate per-topic diversity (number of unique words over total words in the topic)\n","                unique_words_topic = set(topic_words)\n","                total_words_topic = len(topic_words)\n","                diversity_score_topic = len(unique_words_topic) / total_words_topic if total_words_topic > 0 else 0\n","\n","                # Save per-topic metrics\n","                per_topic_metrics.append({\n","                    'model_name': model_filename,\n","                    'protocol': repr_name,\n","                    'topic_id': topic_id,\n","                    'coherence': coherence_score_topic,\n","                    'diversity': diversity_score_topic\n","                })\n","\n","            if not topic_words_list:\n","                print(f\"No valid topics found for representation: {repr_name}\")\n","                continue  # Skip if no valid topics are available\n","\n","            # Calculate overall coherence using Gensim's CoherenceModel with 'c_v'\n","            coherence_model = CoherenceModel(topics=topic_words_list,\n","                                             texts=processed_docs,\n","                                             dictionary=dictionary,\n","                                             coherence='c_v')\n","            coherence_score = coherence_model.get_coherence()\n","\n","            # Calculate overall diversity\n","            unique_words = set()\n","            total_words = 0\n","            for words in topic_words_list:\n","                unique_words.update(words)\n","                total_words += len(words)\n","            diversity_score = len(unique_words) / total_words if total_words > 0 else 0\n","\n","            # Save overall metrics\n","            metrics_data = {\n","                'model_name': model_filename,\n","                'protocol': repr_name,\n","                'coherence': coherence_score,\n","                'diversity': diversity_score\n","            }\n","            model_metrics.append(metrics_data)\n","            print(f\"Overall Coherence: {coherence_score}, Overall Diversity: {diversity_score}\")\n","\n","            # Save per-topic metrics to a CSV file\n","            per_topic_metrics_df = pd.DataFrame(per_topic_metrics)\n","            per_topic_metrics_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_{repr_name}_per_topic_metrics.csv\")\n","            per_topic_metrics_df.to_csv(per_topic_metrics_path, index=False)\n","            print(f\"Per-topic metrics saved for representation: {repr_name}\")\n","\n","    except Exception as e:\n","        print(f\"Error calculating metrics for model '{model_filename}': {e}\")\n","        continue\n","\n","    # Add model metrics to the overall DataFrame\n","    if model_metrics:\n","        model_metrics_df = pd.DataFrame(model_metrics)\n","        all_models_metrics = pd.concat([all_models_metrics, model_metrics_df], ignore_index=True)\n","\n","        # Save metrics for the model\n","        metrics_output_path = os.path.join(output_dir, f\"{model_filename.split('.')[0]}_metrics.csv\")\n","        model_metrics_df.to_csv(metrics_output_path, index=False)\n","        print(f\"Metrics saved for model: {model_filename}\")\n","    else:\n","        print(f\"No metrics calculated for model: {model_filename}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDsmYAUZeezX","outputId":"29f49124-aec7-485c-b2be-1f8ed4d6b734"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bertopic==0.16.3 in /usr/local/lib/python3.10/dist-packages (0.16.3)\n","Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (0.8.39)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (5.24.1)\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (1.0.2)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (3.2.1)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (4.66.6)\n","Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic==0.16.3) (0.5.7)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic==0.16.3) (1.13.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic==0.16.3) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2024.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic==0.16.3) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic==0.16.3) (24.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic==0.16.3) (3.5.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (4.44.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (2.5.0+cu121)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (0.24.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (10.4.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic==0.16.3) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic==0.16.3) (0.5.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2024.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (4.12.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic==0.16.3) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic==0.16.3) (1.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2024.8.30)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n","Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["BERTopic version: 0.16.3\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loading additional stop words and standard English stop words...\n","Stop words loaded. Total stop words: 7704\n","Loading dataset...\n","Dataset loaded.\n","Preprocessing sentences...\n","Sentences preprocessed.\n","Total sentences in dataset: 680822\n","Tokenizing sentences and removing stop words...\n","Tokenization and stop word removal completed.\n","Creating dictionary and corpus for coherence model...\n","Dictionary and corpus created.\n","Model and Embedding DataFrame created:\n","                                      model_name             embedding_model  \\\n","0  bertopic_model_0_iter_66_20241101_231419.pkl           all-MiniLM-L12-v2   \n","1  bertopic_model_1_iter_14_20241101_232426.pkl    paraphrase-mpnet-base-v2   \n","2  bertopic_model_2_iter_75_20241101_233115.pkl           all-MiniLM-L12-v2   \n","3   bertopic_model_3_iter_0_20241101_234120.pkl    paraphrase-mpnet-base-v2   \n","4  bertopic_model_4_iter_19_20241101_234832.pkl     paraphrase-MiniLM-L6-v2   \n","5  bertopic_model_5_iter_13_20241101_235837.pkl    paraphrase-mpnet-base-v2   \n","6  bertopic_model_6_iter_23_20241102_000842.pkl  multi-qa-mpnet-base-cos-v1   \n","7  bertopic_model_7_iter_67_20241102_001547.pkl           all-MiniLM-L12-v2   \n","8  bertopic_model_8_iter_28_20241102_002548.pkl  multi-qa-mpnet-base-cos-v1   \n","9  bertopic_model_9_iter_11_20241102_003549.pkl  multi-qa-mpnet-base-cos-v1   \n","\n","   iteration  bertopic_top_n_words  umap__n_neighbors  umap__n_components  \\\n","0         66                    10                  7                   2   \n","1         14                    10                 11                   9   \n","2         75                    10                 15                   5   \n","3          0                    10                 11                  10   \n","4         19                    10                 44                   9   \n","5         13                    10                  9                   8   \n","6         23                    10                 19                   9   \n","7         67                    10                 42                   7   \n","8         28                    10                 18                   9   \n","9         11                    10                 14                   8   \n","\n","   umap__min_dist  hdbscan__min_cluster_size  hdbscan__min_samples  \n","0        0.005022                        281                    72  \n","1        0.077818                        500                    72  \n","2        0.004634                        473                    14  \n","3        0.058341                        494                    28  \n","4        0.085702                        143                    32  \n","5        0.086975                        497                    32  \n","6        0.095922                        492                    12  \n","7        0.004852                        258                    37  \n","8        0.008103                        427                    11  \n","9        0.022149                        497                    13  \n","Starting post-processing for each model...\n","\n","Processing model: bertopic_model_0_iter_66_20241101_231419.pkl\n","Using embedding model 'all-MiniLM-L12-v2' for model 'bertopic_model_0_iter_66_20241101_231419.pkl'\n","Loading embedding model...\n","Embedding model loaded.\n","Loading BERTopic model...\n","BERTopic model loaded.\n","Updating topics in the topic model...\n","Topics updated.\n","Applying KeyBERT-Inspired representation...\n","Created directory for processed topics: /content/drive/MyDrive/processed_topics_bertopic_model_0_iter_66_20241101_231419\n","Saving keybert_inspired processed topics...\n","keybert_inspired processed topics saved.\n","Applying Maximal Marginal Relevance representation...\n","Error applying Maximal Marginal Relevance representation for model 'bertopic_model_0_iter_66_20241101_231419.pkl': Length of weights not compatible with specified axis.\n","No MMR topics available for model 'bertopic_model_0_iter_66_20241101_231419.pkl'. Skipping saving.\n","Applying POS filtering protocol: nouns\n","POS filtered topics saved for protocol: nouns\n","Applying POS filtering protocol: nouns_verbs\n","POS filtered topics saved for protocol: nouns_verbs\n","Applying POS filtering protocol: nouns_adjectives\n","POS filtered topics saved for protocol: nouns_adjectives\n","Post-processing completed and saved for model: bertopic_model_0_iter_66_20241101_231419.pkl\n","\n","Calculating coherence and diversity for model: bertopic_model_0_iter_66_20241101_231419.pkl\n","Calculating metrics for representation: keybert_inspired\n","Error calculating metrics for model 'bertopic_model_0_iter_66_20241101_231419.pkl': unable to interpret topic as either a list of tokens or a list of ids\n","\n","Processing model: bertopic_model_1_iter_14_20241101_232426.pkl\n","Using embedding model 'paraphrase-mpnet-base-v2' for model 'bertopic_model_1_iter_14_20241101_232426.pkl'\n","Loading embedding model...\n","Embedding model loaded.\n","Loading BERTopic model...\n","BERTopic model loaded.\n","Updating topics in the topic model...\n","Topics updated.\n","Applying KeyBERT-Inspired representation...\n","Created directory for processed topics: /content/drive/MyDrive/processed_topics_bertopic_model_1_iter_14_20241101_232426\n","Saving keybert_inspired processed topics...\n","keybert_inspired processed topics saved.\n","Applying Maximal Marginal Relevance representation...\n","Error applying Maximal Marginal Relevance representation for model 'bertopic_model_1_iter_14_20241101_232426.pkl': Length of weights not compatible with specified axis.\n","No MMR topics available for model 'bertopic_model_1_iter_14_20241101_232426.pkl'. Skipping saving.\n","Applying POS filtering protocol: nouns\n","POS filtered topics saved for protocol: nouns\n","Applying POS filtering protocol: nouns_verbs\n","POS filtered topics saved for protocol: nouns_verbs\n","Applying POS filtering protocol: nouns_adjectives\n","POS filtered topics saved for protocol: nouns_adjectives\n","Post-processing completed and saved for model: bertopic_model_1_iter_14_20241101_232426.pkl\n","\n","Calculating coherence and diversity for model: bertopic_model_1_iter_14_20241101_232426.pkl\n","Calculating metrics for representation: keybert_inspired\n","Error calculating metrics for model 'bertopic_model_1_iter_14_20241101_232426.pkl': unable to interpret topic as either a list of tokens or a list of ids\n","\n","Processing model: bertopic_model_2_iter_75_20241101_233115.pkl\n","Using embedding model 'all-MiniLM-L12-v2' for model 'bertopic_model_2_iter_75_20241101_233115.pkl'\n","Loading embedding model...\n","Embedding model loaded.\n","Loading BERTopic model...\n","BERTopic model loaded.\n","Updating topics in the topic model...\n","Topics updated.\n","Applying KeyBERT-Inspired representation...\n","Created directory for processed topics: /content/drive/MyDrive/processed_topics_bertopic_model_2_iter_75_20241101_233115\n","Saving keybert_inspired processed topics...\n","keybert_inspired processed topics saved.\n","Applying Maximal Marginal Relevance representation...\n","Error applying Maximal Marginal Relevance representation for model 'bertopic_model_2_iter_75_20241101_233115.pkl': Length of weights not compatible with specified axis.\n","No MMR topics available for model 'bertopic_model_2_iter_75_20241101_233115.pkl'. Skipping saving.\n","Applying POS filtering protocol: nouns\n","POS filtered topics saved for protocol: nouns\n","Applying POS filtering protocol: nouns_verbs\n","POS filtered topics saved for protocol: nouns_verbs\n","Applying POS filtering protocol: nouns_adjectives\n","POS filtered topics saved for protocol: nouns_adjectives\n","Post-processing completed and saved for model: bertopic_model_2_iter_75_20241101_233115.pkl\n","\n","Calculating coherence and diversity for model: bertopic_model_2_iter_75_20241101_233115.pkl\n","Calculating metrics for representation: keybert_inspired\n","Error calculating metrics for model 'bertopic_model_2_iter_75_20241101_233115.pkl': unable to interpret topic as either a list of tokens or a list of ids\n","\n","Processing model: bertopic_model_3_iter_0_20241101_234120.pkl\n","Using embedding model 'paraphrase-mpnet-base-v2' for model 'bertopic_model_3_iter_0_20241101_234120.pkl'\n","Loading embedding model...\n","Embedding model loaded.\n","Loading BERTopic model...\n","BERTopic model loaded.\n","Updating topics in the topic model...\n","Topics updated.\n","Applying KeyBERT-Inspired representation...\n","Created directory for processed topics: /content/drive/MyDrive/processed_topics_bertopic_model_3_iter_0_20241101_234120\n","Saving keybert_inspired processed topics...\n","keybert_inspired processed topics saved.\n","Applying Maximal Marginal Relevance representation...\n","Error applying Maximal Marginal Relevance representation for model 'bertopic_model_3_iter_0_20241101_234120.pkl': Length of weights not compatible with specified axis.\n","No MMR topics available for model 'bertopic_model_3_iter_0_20241101_234120.pkl'. Skipping saving.\n","Applying POS filtering protocol: nouns\n","POS filtered topics saved for protocol: nouns\n","Applying POS filtering protocol: nouns_verbs\n","POS filtered topics saved for protocol: nouns_verbs\n","Applying POS filtering protocol: nouns_adjectives\n","POS filtered topics saved for protocol: nouns_adjectives\n","Post-processing completed and saved for model: bertopic_model_3_iter_0_20241101_234120.pkl\n","\n","Calculating coherence and diversity for model: bertopic_model_3_iter_0_20241101_234120.pkl\n","Calculating metrics for representation: keybert_inspired\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:gensim.topic_coherence.text_analysis:stats accumulation interrupted; <= -37193729 documents processed\n","WARNING:gensim.topic_coherence.text_analysis:stats accumulation interrupted; <= -65992466 documents processed\n","WARNING:gensim.topic_coherence.text_analysis:stats accumulation interrupted; <= -56010054 documents processed\n","/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n","  m_lr_i = np.log(numerator / denominator)\n","/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n","  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Overall Coherence: 0.4856402947301775, Overall Diversity: 0.6509803921568628\n","Per-topic metrics saved for representation: keybert_inspired\n","Calculating metrics for representation: mmr\n","No topics found for representation: mmr\n","Calculating metrics for representation: nouns\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:gensim.topic_coherence.text_analysis:stats accumulation interrupted; <= -33458894 documents processed\n","WARNING:gensim.topic_coherence.text_analysis:stats accumulation interrupted; <= -40297276 documents processed\n"]}]}]}