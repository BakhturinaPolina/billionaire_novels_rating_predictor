{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pareto Efficiency Analysis for Topic Modeling\n",
        "\n",
        "This notebook performs Pareto efficiency analysis on topic modeling results to identify optimal model configurations balancing coherence and topic diversity.\n",
        "\n",
        "## Overview\n",
        "- **Input**: Model evaluation results from OCTIS optimization\n",
        "- **Analysis**: Pareto efficiency identification with two weighting strategies\n",
        "- **Output**: Top models, visualizations, and hyperparameter analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import math\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Define paths - calculate project root from current working directory\n",
        "# Notebook is in notebooks/05_selection/, project root is 2 levels up\n",
        "current_dir = Path.cwd()\n",
        "if 'notebooks' in str(current_dir):\n",
        "    # Running from notebook directory\n",
        "    project_root = current_dir.parent.parent if '05_selection' in str(current_dir) else current_dir.parent\n",
        "else:\n",
        "    # Running from project root or elsewhere - try to find project root\n",
        "    project_root = current_dir\n",
        "    # Look for results directory to confirm we're in the right place\n",
        "    if not (project_root / \"results\").exists():\n",
        "        # Try going up one level\n",
        "        project_root = current_dir.parent\n",
        "        if not (project_root / \"results\").exists():\n",
        "            # Last resort: assume we're in project root\n",
        "            project_root = current_dir\n",
        "\n",
        "input_csv = project_root / \"results/experiments/bill_novels_octis_model_results_all_models.csv\"\n",
        "output_base = project_root / \"results/pareto\"\n",
        "\n",
        "# Create output directories\n",
        "(output_base / \"figures\").mkdir(parents=True, exist_ok=True)\n",
        "(output_base / \"tables\").mkdir(parents=True, exist_ok=True)\n",
        "(output_base / \"top_models\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Input file: {input_csv}\")\n",
        "print(f\"Input file exists: {input_csv.exists()}\")\n",
        "print(f\"Output directory: {output_base}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model evaluation results\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "print(f\"Loaded {len(df)} model evaluation results\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning\n",
        "\n",
        "Remove outliers using the same approach as the original analysis:\n",
        "1. Remove runs where Topic_Diversity or Coherence equals 1.0 (failed runs)\n",
        "2. Remove outliers beyond 2 standard deviations from the mean (both upper and lower bounds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove runs where Topic Diversity or Coherence is exactly 1 (failed runs)\n",
        "df_clean = df[(df['Topic_Diversity'] < 1.0) & (df['Coherence'] < 1.0)].copy()\n",
        "print(f\"After removing failed runs (== 1.0): {len(df_clean)} models\")\n",
        "\n",
        "# Compute mean and standard deviation for outlier detection\n",
        "coherence_mean = df_clean['Coherence'].mean()\n",
        "coherence_std = df_clean['Coherence'].std()\n",
        "topic_diversity_mean = df_clean['Topic_Diversity'].mean()\n",
        "topic_diversity_std = df_clean['Topic_Diversity'].std()\n",
        "\n",
        "# Define bounds (2 standard deviations)\n",
        "coherence_lower = coherence_mean - 2 * coherence_std\n",
        "coherence_upper = coherence_mean + 2 * coherence_std\n",
        "topic_diversity_lower = topic_diversity_mean - 2 * topic_diversity_std\n",
        "topic_diversity_upper = topic_diversity_mean + 2 * topic_diversity_std\n",
        "\n",
        "print(f\"\\nCoherence bounds: [{coherence_lower:.4f}, {coherence_upper:.4f}]\")\n",
        "print(f\"Topic Diversity bounds: [{topic_diversity_lower:.4f}, {topic_diversity_upper:.4f}]\")\n",
        "\n",
        "# Plot distributions with cutoff points\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Coherence distribution\n",
        "axes[0].hist(df_clean['Coherence'], bins=20, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(coherence_lower, color='red', linestyle='dashed', linewidth=2, label='Lower Cutoff')\n",
        "axes[0].axvline(coherence_upper, color='green', linestyle='dashed', linewidth=2, label='Upper Cutoff')\n",
        "axes[0].set_title('Coherence Distribution with Cutoff Points')\n",
        "axes[0].set_xlabel('Coherence')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Topic Diversity distribution\n",
        "axes[1].hist(df_clean['Topic_Diversity'], bins=20, edgecolor='black', alpha=0.7)\n",
        "axes[1].axvline(topic_diversity_lower, color='red', linestyle='dashed', linewidth=2, label='Lower Cutoff')\n",
        "axes[1].axvline(topic_diversity_upper, color='green', linestyle='dashed', linewidth=2, label='Upper Cutoff')\n",
        "axes[1].set_title('Topic Diversity Distribution with Cutoff Points')\n",
        "axes[1].set_xlabel('Topic Diversity')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_base / \"figures/distribution_with_cutoffs.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Remove outliers\n",
        "df_clean = df_clean[\n",
        "    (df_clean['Coherence'] >= coherence_lower) & \n",
        "    (df_clean['Coherence'] <= coherence_upper) &\n",
        "    (df_clean['Topic_Diversity'] >= topic_diversity_lower) & \n",
        "    (df_clean['Topic_Diversity'] <= topic_diversity_upper)\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nAfter removing outliers (2 std dev): {len(df_clean)} models\")\n",
        "print(\"\\nDescriptive statistics after cleaning:\")\n",
        "df_clean[['Coherence', 'Topic_Diversity']].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Metric Normalization\n",
        "\n",
        "Apply Z-score normalization to make Coherence and Topic Diversity comparable for the combined score calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Z-score normalization\n",
        "scaler = StandardScaler()\n",
        "df_clean[['Coherence_norm', 'Topic_Diversity_norm']] = scaler.fit_transform(\n",
        "    df_clean[['Coherence', 'Topic_Diversity']]\n",
        ")\n",
        "\n",
        "print(\"Normalized metrics (sample):\")\n",
        "df_clean[['Coherence', 'Coherence_norm', 'Topic_Diversity', 'Topic_Diversity_norm']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Combined Score Calculation (Equal Weights)\n",
        "\n",
        "Calculate combined score with equal weights for coherence and topic diversity (0.5 each).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define weights for equal weighting strategy\n",
        "weight_coherence = 0.5\n",
        "weight_topic_diversity = 0.5\n",
        "\n",
        "# Calculate combined score\n",
        "df_clean['Combined_Score'] = (\n",
        "    weight_coherence * df_clean['Coherence_norm'] + \n",
        "    weight_topic_diversity * df_clean['Topic_Diversity_norm']\n",
        ")\n",
        "\n",
        "# Rank by combined score\n",
        "df_sorted = df_clean.sort_values(by='Combined_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"Top 10 Models Based on Combined Score (Equal Weights):\")\n",
        "top_10_equal = df_sorted.head(10)\n",
        "display(top_10_equal[['Embeddings_Model', 'Iteration', 'Coherence', 'Topic_Diversity', \n",
        "                       'Coherence_norm', 'Topic_Diversity_norm', 'Combined_Score']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pareto Efficiency Analysis (Equal Weights)\n",
        "\n",
        "Identify Pareto-efficient models - models where no other model is better in both metrics simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_pareto(df, metrics):\n",
        "    \"\"\"\n",
        "    Identify Pareto-efficient points for given metrics.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing the data points\n",
        "    metrics (list): List of column names to use for Pareto analysis\n",
        "    \n",
        "    Returns:\n",
        "    np.array: Boolean array indicating whether each row is Pareto-efficient\n",
        "    \"\"\"\n",
        "    pareto_efficient = np.ones(df.shape[0], dtype=bool)\n",
        "    for i, row in df.iterrows():\n",
        "        # If there are any other points that are strictly better, mark as not Pareto-efficient\n",
        "        pareto_efficient[i] = not np.any(\n",
        "            np.all(df[metrics].values >= row[metrics].values, axis=1) & \n",
        "            np.any(df[metrics].values > row[metrics].values, axis=1)\n",
        "        )\n",
        "    return pareto_efficient\n",
        "\n",
        "# Identify Pareto-efficient models (overall)\n",
        "df_clean['Pareto_Efficient_All'] = identify_pareto(\n",
        "    df_clean, ['Coherence_norm', 'Topic_Diversity_norm']\n",
        ")\n",
        "\n",
        "# Identify Pareto-efficient models per embedding model\n",
        "df_clean['Pareto_Efficient_PerModel'] = False\n",
        "for model_name, group in df_clean.groupby('Embeddings_Model'):\n",
        "    pareto_flags = identify_pareto(group, ['Coherence_norm', 'Topic_Diversity_norm'])\n",
        "    df_clean.loc[group.index, 'Pareto_Efficient_PerModel'] = pareto_flags\n",
        "\n",
        "pareto_all_count = df_clean['Pareto_Efficient_All'].sum()\n",
        "pareto_per_model_count = df_clean['Pareto_Efficient_PerModel'].sum()\n",
        "\n",
        "print(f\"Pareto-efficient models (overall): {pareto_all_count}\")\n",
        "print(f\"Pareto-efficient models (per model): {pareto_per_model_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive scatter plot showing Pareto front\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot all models\n",
        "sns.scatterplot(\n",
        "    data=df_clean, \n",
        "    x='Topic_Diversity', \n",
        "    y='Coherence', \n",
        "    hue='Embeddings_Model',\n",
        "    palette='Set2', \n",
        "    s=70, \n",
        "    alpha=0.7,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Highlight Pareto-efficient models\n",
        "pareto_models = df_clean[df_clean['Pareto_Efficient_All']]\n",
        "ax.scatter(\n",
        "    pareto_models['Topic_Diversity'], \n",
        "    pareto_models['Coherence'],\n",
        "    facecolors='none', \n",
        "    edgecolors='red', \n",
        "    s=200, \n",
        "    linewidths=2, \n",
        "    label='Pareto-efficient',\n",
        "    zorder=10\n",
        ")\n",
        "\n",
        "ax.set_title('Pareto Front: Coherence vs. Topic Diversity (Equal Weights)', fontsize=14)\n",
        "ax.set_xlabel('Topic Diversity', fontsize=12)\n",
        "ax.set_ylabel('Coherence', fontsize=12)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_base / \"figures/pareto_front_equal_weights.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create per-model Pareto front plots\n",
        "unique_models = df_clean['Embeddings_Model'].unique()\n",
        "num_models = len(unique_models)\n",
        "cols = 2\n",
        "rows = math.ceil(num_models / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "axes = axes.flatten() if num_models > 1 else [axes]\n",
        "\n",
        "for i, model_name in enumerate(unique_models):\n",
        "    subset = df_clean[df_clean['Embeddings_Model'] == model_name]\n",
        "    \n",
        "    # Scatter plot of all runs\n",
        "    axes[i].scatter(\n",
        "        subset['Topic_Diversity'], \n",
        "        subset['Coherence'], \n",
        "        label='All Runs', \n",
        "        color='blue', \n",
        "        alpha=0.6,\n",
        "        s=50\n",
        "    )\n",
        "    \n",
        "    # Highlight Pareto-efficient runs\n",
        "    pareto_subset = subset[subset['Pareto_Efficient_PerModel']]\n",
        "    if len(pareto_subset) > 0:\n",
        "        axes[i].scatter(\n",
        "            pareto_subset['Topic_Diversity'], \n",
        "            pareto_subset['Coherence'], \n",
        "            label='Pareto Efficient', \n",
        "            color='red', \n",
        "            s=100,\n",
        "            edgecolors='darkred',\n",
        "            linewidths=1.5\n",
        "        )\n",
        "    \n",
        "    axes[i].set_title(f'Pareto Front for {model_name}', fontsize=12)\n",
        "    axes[i].set_xlabel('Topic Diversity', fontsize=10)\n",
        "    axes[i].set_ylabel('Coherence', fontsize=10)\n",
        "    axes[i].legend(fontsize=9)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplots\n",
        "for i in range(num_models, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_base / \"figures/pareto_fronts_per_model.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics by embedding model for Pareto-efficient models\n",
        "pareto_efficient_models = df_clean[df_clean['Pareto_Efficient_All']]\n",
        "pareto_grouped = pareto_efficient_models.groupby('Embeddings_Model').agg({\n",
        "    'Coherence_norm': ['mean', 'std', 'count'],\n",
        "    'Topic_Diversity_norm': ['mean', 'std', 'count'],\n",
        "    'Combined_Score': ['mean', 'std', 'count']\n",
        "}).reset_index()\n",
        "\n",
        "pareto_grouped.columns = [\n",
        "    'Embeddings_Model',\n",
        "    'Coherence_mean', 'Coherence_std', 'Coherence_count',\n",
        "    'Diversity_mean', 'Diversity_std', 'Diversity_count',\n",
        "    'Combined_mean', 'Combined_std', 'Combined_count'\n",
        "]\n",
        "\n",
        "print(\"Performance of Embedding Models in Pareto-Efficient Models:\")\n",
        "display(pareto_grouped)\n",
        "\n",
        "# Select top 10 Pareto-efficient models\n",
        "top_10_equal_weights = pareto_efficient_models.nlargest(10, 'Combined_Score')\n",
        "print(f\"\\nTop 10 Pareto-efficient Models (Equal Weights):\")\n",
        "display(top_10_equal_weights[['Embeddings_Model', 'Iteration', 'Coherence', 'Topic_Diversity',\n",
        "                              'Coherence_norm', 'Topic_Diversity_norm', 'Combined_Score']])\n",
        "\n",
        "# Save top 10 models\n",
        "top_10_equal_weights.to_csv(output_base / \"top_models/top_10_equal_weights.csv\", index=False)\n",
        "print(f\"\\nSaved top 10 models to: {output_base / 'top_models/top_10_equal_weights.csv'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Combined Score Calculation (Coherence Priority)\n",
        "\n",
        "Recalculate combined score with higher weight on coherence (0.7) and lower weight on topic diversity (0.3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define weights for coherence priority strategy\n",
        "weight_coherence = 0.7\n",
        "weight_topic_diversity = 0.3\n",
        "\n",
        "# Recalculate combined score\n",
        "df_clean['Combined_Score'] = (\n",
        "    weight_coherence * df_clean['Coherence_norm'] + \n",
        "    weight_topic_diversity * df_clean['Topic_Diversity_norm']\n",
        ")\n",
        "\n",
        "# Re-identify Pareto-efficient models\n",
        "df_clean['Pareto_Efficient'] = identify_pareto(\n",
        "    df_clean, ['Coherence_norm', 'Topic_Diversity_norm']\n",
        ")\n",
        "\n",
        "# Rank by combined score\n",
        "df_sorted_priority = df_clean.sort_values(by='Combined_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"Top 10 Models Based on Combined Score (Coherence Priority):\")\n",
        "display(df_sorted_priority.head(10)[['Embeddings_Model', 'Iteration', 'Coherence', 'Topic_Diversity',\n",
        "                                      'Coherence_norm', 'Topic_Diversity_norm', 'Combined_Score']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pareto Efficiency Analysis (Coherence Priority)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Pareto-efficient models\n",
        "pareto_efficient_priority = df_clean[df_clean['Pareto_Efficient']]\n",
        "pareto_count_priority = len(pareto_efficient_priority)\n",
        "\n",
        "print(f\"Number of Pareto-efficient Models (Coherence Priority): {pareto_count_priority}\")\n",
        "\n",
        "# Create scatter plot for coherence-priority Pareto front\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=df_clean, \n",
        "    x='Topic_Diversity', \n",
        "    y='Coherence', \n",
        "    hue='Embeddings_Model',\n",
        "    palette='Set2', \n",
        "    s=70, \n",
        "    alpha=0.7,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Highlight Pareto-efficient models\n",
        "pareto_models_priority = df_clean[df_clean['Pareto_Efficient']]\n",
        "ax.scatter(\n",
        "    pareto_models_priority['Topic_Diversity'], \n",
        "    pareto_models_priority['Coherence'],\n",
        "    facecolors='none', \n",
        "    edgecolors='red', \n",
        "    s=200, \n",
        "    linewidths=2, \n",
        "    label='Pareto-efficient (Coherence Priority)',\n",
        "    zorder=10\n",
        ")\n",
        "\n",
        "ax.set_title('Pareto Front: Coherence vs. Topic Diversity (Coherence Prioritized)', fontsize=14)\n",
        "ax.set_xlabel('Topic Diversity', fontsize=12)\n",
        "ax.set_ylabel('Coherence', fontsize=12)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_base / \"figures/pareto_front_coherence_priority.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Select top 10 Pareto-efficient models\n",
        "top_10_coherence_priority = pareto_efficient_priority.nlargest(10, 'Combined_Score')\n",
        "print(f\"\\nTop 10 Pareto-efficient Models (Coherence Priority):\")\n",
        "display(top_10_coherence_priority[['Embeddings_Model', 'Iteration', 'Coherence', 'Topic_Diversity',\n",
        "                                    'Coherence_norm', 'Topic_Diversity_norm', 'Combined_Score']])\n",
        "\n",
        "# Save top 10 models\n",
        "top_10_coherence_priority.to_csv(output_base / \"top_models/top_10_coherence_priority.csv\", index=False)\n",
        "print(f\"\\nSaved top 10 models to: {output_base / 'top_models/top_10_coherence_priority.csv'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Hyperparameter Analysis\n",
        "\n",
        "Analyze relationships between hyperparameters and performance metrics for the top models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved top 10 model sets\n",
        "df_equal_weights = pd.read_csv(output_base / \"top_models/top_10_equal_weights.csv\")\n",
        "df_coherence_priority = pd.read_csv(output_base / \"top_models/top_10_coherence_priority.csv\")\n",
        "\n",
        "# Define hyperparameters\n",
        "hyperparameters = [\n",
        "    'bertopic__min_topic_size',\n",
        "    'bertopic__top_n_words',\n",
        "    'hdbscan__min_cluster_size',\n",
        "    'hdbscan__min_samples',\n",
        "    'umap__min_dist',\n",
        "    'umap__n_neighbors',\n",
        "    'vectorizer__min_df'\n",
        "]\n",
        "\n",
        "# Performance metrics\n",
        "performance_metrics = ['Coherence_norm', 'Topic_Diversity_norm', 'Combined_Score']\n",
        "\n",
        "print(\"Descriptive Statistics for Hyperparameters (Equal Weights):\")\n",
        "display(df_equal_weights[hyperparameters].describe())\n",
        "\n",
        "print(\"\\nDescriptive Statistics for Hyperparameters (Coherence Priority):\")\n",
        "display(df_coherence_priority[hyperparameters].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrices\n",
        "df_equal_filtered = df_equal_weights[hyperparameters + performance_metrics]\n",
        "df_priority_filtered = df_coherence_priority[hyperparameters + performance_metrics]\n",
        "\n",
        "correlation_equal = df_equal_filtered.corr()\n",
        "correlation_priority = df_priority_filtered.corr()\n",
        "\n",
        "print(\"Correlation Matrix (Equal Weights):\")\n",
        "display(correlation_equal)\n",
        "\n",
        "print(\"\\nCorrelation Matrix (Coherence Priority):\")\n",
        "display(correlation_priority)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create normalized hyperparameter boxplots\n",
        "scaler_hp = MinMaxScaler()\n",
        "df_normalized_eq = df_equal_weights.copy()\n",
        "df_normalized_pri = df_coherence_priority.copy()\n",
        "\n",
        "df_normalized_eq[hyperparameters] = scaler_hp.fit_transform(df_equal_weights[hyperparameters])\n",
        "df_normalized_pri[hyperparameters] = scaler_hp.fit_transform(df_coherence_priority[hyperparameters])\n",
        "\n",
        "# Melt for plotting\n",
        "df_melted_eq = df_normalized_eq.melt(\n",
        "    value_vars=hyperparameters,\n",
        "    var_name='Hyperparameter',\n",
        "    value_name='Value'\n",
        ")\n",
        "df_melted_eq['Strategy'] = 'Equal Weights'\n",
        "\n",
        "df_melted_pri = df_normalized_pri.melt(\n",
        "    value_vars=hyperparameters,\n",
        "    var_name='Hyperparameter',\n",
        "    value_name='Value'\n",
        ")\n",
        "df_melted_pri['Strategy'] = 'Coherence Priority'\n",
        "\n",
        "df_melted = pd.concat([df_melted_eq, df_melted_pri], ignore_index=True)\n",
        "\n",
        "# Create boxplots\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "sns.boxplot(\n",
        "    data=df_melted,\n",
        "    x='Value',\n",
        "    y='Hyperparameter',\n",
        "    hue='Strategy',\n",
        "    orient='h',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title('Boxplots of Normalized Hyperparameters', fontsize=16)\n",
        "ax.set_xlabel('Normalized Values', fontsize=12)\n",
        "ax.set_ylabel('Hyperparameter', fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_base / \"figures/hyperparameter_boxplots.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    \"\"\"Calculate Cohen's d for two groups.\"\"\"\n",
        "    diff_mean = np.mean(group1) - np.mean(group2)\n",
        "    pooled_std = np.sqrt(((np.std(group1, ddof=1) ** 2) + (np.std(group2, ddof=1) ** 2)) / 2)\n",
        "    return diff_mean / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "# Function to choose correlation test\n",
        "def choose_test(df, parameter, metric):\n",
        "    \"\"\"Choose between Pearson or Spearman based on data distribution.\"\"\"\n",
        "    if abs(df[parameter].skew()) < 1:\n",
        "        corr, p_value = pearsonr(df[parameter], df[metric])\n",
        "        test_type = \"Pearson\"\n",
        "    else:\n",
        "        corr, p_value = spearmanr(df[parameter], df[metric])\n",
        "        test_type = \"Spearman\"\n",
        "    return corr, p_value, test_type\n",
        "\n",
        "# Function to calculate Cohen's d between groups\n",
        "def calculate_cohens_d(df, parameter, metric):\n",
        "    \"\"\"Split by median and calculate Cohen's d.\"\"\"\n",
        "    median_value = df[parameter].median()\n",
        "    group1 = df[df[parameter] <= median_value][metric]\n",
        "    group2 = df[df[parameter] > median_value][metric]\n",
        "    return cohen_d(group1, group2)\n",
        "\n",
        "# Analyze correlations and effect sizes\n",
        "results_equal = {'Hyperparameter': [], 'Metric': [], 'Correlation': [], 'p-value': [], 'Test_Type': [], 'Cohen_d': []}\n",
        "results_priority = {'Hyperparameter': [], 'Metric': [], 'Correlation': [], 'p-value': [], 'Test_Type': [], 'Cohen_d': []}\n",
        "\n",
        "for param in hyperparameters:\n",
        "    for metric in performance_metrics:\n",
        "        # Equal weights\n",
        "        corr, p_val, test_type = choose_test(df_equal_weights, param, metric)\n",
        "        cohens_d_val = calculate_cohens_d(df_equal_weights, param, metric)\n",
        "        results_equal['Hyperparameter'].append(param)\n",
        "        results_equal['Metric'].append(metric)\n",
        "        results_equal['Correlation'].append(corr)\n",
        "        results_equal['p-value'].append(p_val)\n",
        "        results_equal['Test_Type'].append(test_type)\n",
        "        results_equal['Cohen_d'].append(cohens_d_val)\n",
        "        \n",
        "        # Coherence priority\n",
        "        corr, p_val, test_type = choose_test(df_coherence_priority, param, metric)\n",
        "        cohens_d_val = calculate_cohens_d(df_coherence_priority, param, metric)\n",
        "        results_priority['Hyperparameter'].append(param)\n",
        "        results_priority['Metric'].append(metric)\n",
        "        results_priority['Correlation'].append(corr)\n",
        "        results_priority['p-value'].append(p_val)\n",
        "        results_priority['Test_Type'].append(test_type)\n",
        "        results_priority['Cohen_d'].append(cohens_d_val)\n",
        "\n",
        "# Convert to DataFrames\n",
        "correlation_results_equal = pd.DataFrame(results_equal)\n",
        "correlation_results_priority = pd.DataFrame(results_priority)\n",
        "\n",
        "print(\"Correlation and Cohen's d Results (Equal Weights):\")\n",
        "display(correlation_results_equal)\n",
        "\n",
        "print(\"\\nCorrelation and Cohen's d Results (Coherence Priority):\")\n",
        "display(correlation_results_priority)\n",
        "\n",
        "# Save results\n",
        "correlation_results_equal.to_csv(output_base / \"tables/correlation_analysis_equal_weights.csv\", index=False)\n",
        "correlation_results_priority.to_csv(output_base / \"tables/correlation_analysis_coherence_priority.csv\", index=False)\n",
        "\n",
        "print(f\"\\nSaved correlation analysis to:\")\n",
        "print(f\"  - {output_base / 'tables/correlation_analysis_equal_weights.csv'}\")\n",
        "print(f\"  - {output_base / 'tables/correlation_analysis_coherence_priority.csv'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
