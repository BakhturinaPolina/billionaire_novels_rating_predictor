{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Quality EDA and Noisy Topic Detection\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) on BERTopic model topics and flags candidate noisy topics for manual inspection before proceeding with LLM-generated topic labeling.\n",
    "\n",
    "## Overview\n",
    "- **Input**: Retrained BERTopic model (wrapper or native)\n",
    "- **Analysis**: Topic size, POS representation stats, per-topic POS coherence\n",
    "- **Output**: Topic quality table with noise candidate flags and inspection labels\n",
    "- **Purpose**: Identify noisy topics before LLM labeling stage\n",
    "\n",
    "## Key Features\n",
    "- Keeps the model as-is (no topics removed)\n",
    "- EDA on POS representation (size, POS length, per-topic POS coherence)\n",
    "- Flags candidate noisy topics with label + reason for manual inspection\n",
    "- Uses the same loading & batching logic as `explore_retrained_model.py`\n",
    "\n",
    "## Model Loading Strategy\n",
    "\n",
    "**Default: Wrapper (`model_*.pkl`)** - Recommended for EDA\n",
    "- ✅ Contains exact training dataset (`wrapper.dataset_as_list_of_strings`)\n",
    "- ✅ Guaranteed to use same text the model was trained on\n",
    "- ✅ No mismatch between training corpus and EDA corpus\n",
    "- ✅ Works seamlessly with `prepare_documents(wrapper, ...)`\n",
    "\n",
    "**Alternative: Native BERTopic (`model_{rank}/` directory)**\n",
    "- Use `USE_NATIVE = True` if you prefer standard BERTopic serialization\n",
    "- Requires providing `DATASET_CSV` separately\n",
    "- Better for sharing/deployment, but requires ensuring dataset matches training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "**Important**: Always run this notebook with the project's virtual environment activated.\n",
    "\n",
    "```bash\n",
    "# Activate venv before launching Jupyter\n",
    "source venv/bin/activate  # On Linux/Mac\n",
    "# or\n",
    "venv\\Scripts\\activate  # On Windows\n",
    "\n",
    "# Then launch Jupyter\n",
    "jupyter notebook notebooks/06_labeling/topic_quality_eda.ipynb\n",
    "```\n",
    "\n",
    "## Cell 1: Verify venv and imports & logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using venv: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/venv/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:27:54.798] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n",
      "✅ RAPIDS (cuML) is available and functional\n",
      "Logger name: stage06_topics_exploration\n",
      "Project root: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Verify venv and imports & logging setup\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify we're using the venv Python\n",
    "venv_path = Path.cwd().parent.parent / \"venv\"\n",
    "if venv_path.exists():\n",
    "    expected_prefix = str(venv_path.resolve())\n",
    "    if expected_prefix not in sys.prefix:\n",
    "        print(f\"⚠️  WARNING: Not using venv!\")\n",
    "        print(f\"   Current Python: {sys.executable}\")\n",
    "        print(f\"   Expected venv: {venv_path}\")\n",
    "        print(f\"   Please activate venv: source venv/bin/activate\")\n",
    "    else:\n",
    "        print(f\"✓ Using venv: {sys.executable}\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: venv directory not found at {venv_path}\")\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.stage06_exploration.explore_retrained_model import (\n",
    "    LOGGER,\n",
    "    DEFAULT_BASE_DIR,\n",
    "    DEFAULT_EMBEDDING_MODEL,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    DEFAULT_CHAPTERS_CSV,\n",
    "    DEFAULT_CHAPTERS_SUBSET_CSV,\n",
    "    DEFAULT_CORPUS_PATH,\n",
    "    load_retrained_wrapper,\n",
    "    load_native_bertopic_model,\n",
    "    prepare_documents,\n",
    "    load_dictionary_from_corpus,\n",
    "    extract_all_topics,\n",
    "    backup_existing_file,\n",
    "    stage_timer,\n",
    ")\n",
    "\n",
    "from src.stage06_eda.topic_quality_analysis import (\n",
    "    build_topic_quality_table,\n",
    "    apply_noise_labels_to_model,\n",
    ")\n",
    "\n",
    "# Ensure INFO-level logs from our logger\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "print(\"Logger name:\", LOGGER.name)\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration (paths, thresholds, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Configuration Summary\n",
      "================================================================================\n",
      "BASE_DIR: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained\n",
      "EMBEDDING_MODEL: paraphrase-MiniLM-L6-v2\n",
      "PARETO_RANK: 1\n",
      "Model pickle path: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1.pkl\n",
      "Model exists: True\n",
      "Using native BERTopic: False (False = load wrapper with training dataset)\n",
      "USE_WRAPPER_DOCS_ONLY: True (True = use training dataset from wrapper)\n",
      "Dataset CSV (fallback): None\n",
      "Dictionary corpus path: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/data/interim/octis/corpus.tsv\n",
      "Dictionary pickle path: None\n",
      "Output dir: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/results/stage06_eda\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "# \n",
    "# This configuration matches stage06_exploration defaults:\n",
    "# - Uses the saved model at models/retrained/paraphrase-MiniLM-L6-v2/model_1.pkl\n",
    "# - Loads the same dataset that was used for training (from wrapper cache)\n",
    "# - Uses the same dictionary corpus as stage06_exploration\n",
    "\n",
    "# Model configuration - matches stage06_exploration defaults\n",
    "BASE_DIR = DEFAULT_BASE_DIR   # models/retrained\n",
    "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL  # \"paraphrase-MiniLM-L6-v2\"\n",
    "PARETO_RANK = 1\n",
    "\n",
    "# Model loading strategy:\n",
    "# USE_NATIVE = False (default): Load wrapper pickle - RECOMMENDED for EDA\n",
    "#   - Contains exact training dataset (wrapper.dataset_as_list_of_strings)\n",
    "#   - Guaranteed to use same text the model was trained on\n",
    "#   - No mismatch between training corpus and EDA corpus\n",
    "# USE_NATIVE = True: Load native BERTopic directory\n",
    "#   - Standard BERTopic serialization (more portable)\n",
    "#   - Requires providing DATASET_CSV separately\n",
    "#   - Better for sharing/deployment, but need to ensure dataset matches training data\n",
    "USE_NATIVE = False  # False => load wrapper pickle (contains training dataset)\n",
    "\n",
    "# Dataset configuration\n",
    "# IMPORTANT: USE_WRAPPER_DOCS_ONLY = True ensures we use the exact same dataset\n",
    "# that was used for training (stored in the wrapper's dataset_as_list_of_strings)\n",
    "# \n",
    "# If USE_NATIVE = True, set USE_WRAPPER_DOCS_ONLY = False and provide DATASET_CSV\n",
    "USE_WRAPPER_DOCS_ONLY = True     # True => use docs from wrapper (training dataset)\n",
    "FALLBACK_DATASET = \"chapters\"    # \"chapters\" or \"subset\" (only used if wrapper unavailable)\n",
    "DATASET_CSV = None               # explicit Path if you want to override (required if USE_NATIVE=True)\n",
    "\n",
    "if DATASET_CSV is None and not USE_WRAPPER_DOCS_ONLY:\n",
    "    if FALLBACK_DATASET == \"subset\":\n",
    "        DATASET_CSV = DEFAULT_CHAPTERS_SUBSET_CSV\n",
    "    else:\n",
    "        DATASET_CSV = DEFAULT_CHAPTERS_CSV\n",
    "\n",
    "# Dictionary / corpus configuration\n",
    "# This is the OCTIS corpus TSV used in Stage 06 (same as stage06_exploration)\n",
    "DICTIONARY_CORPUS_PATH = DEFAULT_CORPUS_PATH\n",
    "# OPTIONAL: if you have a previously saved Dictionary, put its path here\n",
    "DICTIONARY_PICKLE_PATH = None  # e.g. Path(\"results/dictionary.pkl\")\n",
    "\n",
    "BATCH_SIZE = DEFAULT_BATCH_SIZE  # 50,000 (same as stage06_exploration)\n",
    "LIMIT_DOCS = None  # e.g. 100_000 for experiments, or None to use all\n",
    "\n",
    "# Topic extraction / EDA configuration\n",
    "TOP_K = 10  # number of top words per topic to consider\n",
    "MIN_TOPIC_SIZE = 30         # docs per topic\n",
    "MIN_POS_WORDS = 3           # POS words per topic\n",
    "MIN_POS_COHERENCE = 0.0     # per-topic POS coherence threshold\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = project_root / \"results\" / \"stage06_eda\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Will be used for filenames\n",
    "MODEL_NAME_SAFE = EMBEDDING_MODEL.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "# Verify model file exists\n",
    "model_pickle_path = BASE_DIR / EMBEDDING_MODEL / f\"model_{PARETO_RANK}.pkl\"\n",
    "print(\"=\" * 80)\n",
    "print(\"Configuration Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\n",
    "print(f\"PARETO_RANK: {PARETO_RANK}\")\n",
    "print(f\"Model pickle path: {model_pickle_path}\")\n",
    "print(f\"Model exists: {model_pickle_path.exists()}\")\n",
    "print(f\"Using native BERTopic: {USE_NATIVE} (False = load wrapper with training dataset)\")\n",
    "print(f\"USE_WRAPPER_DOCS_ONLY: {USE_WRAPPER_DOCS_ONLY} (True = use training dataset from wrapper)\")\n",
    "print(f\"Dataset CSV (fallback): {DATASET_CSV}\")\n",
    "print(f\"Dictionary corpus path: {DICTIONARY_CORPUS_PATH}\")\n",
    "print(f\"Dictionary pickle path: {DICTIONARY_PICKLE_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load model (wrapper / native) with diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:27:55,392] [INFO] ▶ Loading retrained model | start\n",
      "[2025-12-05 16:27:55,393] [INFO] ▶ Loading pickle wrapper: model_1.pkl | start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Model Metadata\n",
      "================================================================================\n",
      "{\n",
      "  \"embedding_model\": \"paraphrase-MiniLM-L6-v2\",\n",
      "  \"pareto_rank\": 1,\n",
      "  \"hyperparameters\": {\n",
      "    \"bertopic__min_topic_size\": 64,\n",
      "    \"bertopic__top_n_words\": 27,\n",
      "    \"hdbscan__min_cluster_size\": 143,\n",
      "    \"hdbscan__min_samples\": 32,\n",
      "    \"umap__min_dist\": 0.08570171053,\n",
      "    \"umap__n_components\": 9,\n",
      "    \"umap__n_neighbors\": 44,\n",
      "    \"vectorizer__min_df\": 0.005931605066\n",
      "  },\n",
      "  \"coherence\": 0.4252370503,\n",
      "  \"topic_diversity\": 0.94,\n",
      "  \"combined_score\": 1.6505950772167353,\n",
      "  \"iteration\": 19,\n",
      "  \"num_topics\": 9936,\n",
      "  \"training_timestamp\": \"2025-11-29T18:00:22.427871\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:28:05,822] [INFO] ■ Loading pickle wrapper: model_1.pkl | completed in 10.43 s\n",
      "[2025-12-05 16:28:05,823] [WARNING] ⚠️  TOPIC COUNT MISMATCH: Metadata says 9936 topics, but model has 368 topics. This may indicate the model was reduced during training or saving.\n",
      "[2025-12-05 16:28:05,824] [INFO] ■ Loading retrained model | completed in 10.43 s\n",
      "[2025-12-05 16:28:05,825] [INFO] === Model Diagnostics (Loaded State) ===\n",
      "[2025-12-05 16:28:05,826] [INFO] Topic IDs in topic_representations_: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[2025-12-05 16:28:05,827] [INFO] Total topics (excluding -1): 368\n",
      "[2025-12-05 16:28:05,881] [INFO] Topic info shape: (369, 8)\n",
      "[2025-12-05 16:28:05,882] [INFO] Topic info columns: Index(['Topic', 'Count', 'Name', 'Representation', 'KeyBERT', 'MMR', 'POS',\n",
      "       'Representative_Docs'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Model Loading Summary\n",
      "================================================================================\n",
      "Wrapper loaded: True\n",
      "✓ Using training dataset from wrapper (exact same as used for training)\n",
      "  Wrapper has dataset_as_list_of_strings: True\n",
      "  Number of documents in wrapper: 680822\n",
      "topic_model type: <class 'bertopic._bertopic.BERTopic'>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load retrained BERTopic model and metadata\n",
    "\n",
    "from src.stage06_exploration.explore_retrained_model import load_metadata\n",
    "import json\n",
    "\n",
    "# Load metadata first to see model info\n",
    "metadata = load_metadata(\n",
    "    base_dir=BASE_DIR,\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    pareto_rank=PARETO_RANK,\n",
    ")\n",
    "\n",
    "if metadata:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Model Metadata\")\n",
    "    print(\"=\" * 80)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠️  No metadata file found (this is okay, model will still load)\")\n",
    "\n",
    "wrapper = None\n",
    "\n",
    "with stage_timer(\"Loading retrained model\"):\n",
    "    if USE_NATIVE:\n",
    "        topic_model = load_native_bertopic_model(\n",
    "            base_dir=BASE_DIR,\n",
    "            embedding_model=EMBEDDING_MODEL,\n",
    "            pareto_rank=PARETO_RANK,\n",
    "        )\n",
    "    else:\n",
    "        wrapper, topic_model = load_retrained_wrapper(\n",
    "            base_dir=BASE_DIR,\n",
    "            embedding_model=EMBEDDING_MODEL,\n",
    "            pareto_rank=PARETO_RANK,\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model Loading Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Wrapper loaded: {wrapper is not None}\")\n",
    "if wrapper:\n",
    "    print(f\"✓ Using training dataset from wrapper (exact same as used for training)\")\n",
    "    print(f\"  Wrapper has dataset_as_list_of_strings: {hasattr(wrapper, 'dataset_as_list_of_strings')}\")\n",
    "    if hasattr(wrapper, 'dataset_as_list_of_strings'):\n",
    "        print(f\"  Number of documents in wrapper: {len(wrapper.dataset_as_list_of_strings)}\")\n",
    "print(f\"topic_model type: {type(topic_model)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Diagnostics similar to explore_retrained_model.main()\n",
    "LOGGER.info(\"=== Model Diagnostics (Loaded State) ===\")\n",
    "if hasattr(topic_model, \"topic_representations_\") and topic_model.topic_representations_:\n",
    "    topic_ids = [tid for tid in topic_model.topic_representations_.keys() if tid != -1]\n",
    "    LOGGER.info(\"Topic IDs in topic_representations_: %s\", sorted(topic_ids)[:20])\n",
    "    LOGGER.info(\"Total topics (excluding -1): %d\", len(topic_ids))\n",
    "\n",
    "if hasattr(topic_model, \"get_topic_info\"):\n",
    "    try:\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        LOGGER.info(\"Topic info shape: %s\", getattr(topic_info, \"shape\", \"N/A\"))\n",
    "        LOGGER.info(\"Topic info columns: %s\", getattr(topic_info, \"columns\", \"N/A\"))\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(\"Could not get topic_info: %s\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load documents (batched, same as Stage 06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:28:05,897] [INFO] Wrapper docs in-memory | batch 1 => rows 1-50000 / 680822\n",
      "[2025-12-05 16:28:05,899] [INFO] Wrapper docs in-memory | batch 2 => rows 50001-100000 / 680822\n",
      "[2025-12-05 16:28:05,900] [INFO] Wrapper docs in-memory | batch 3 => rows 100001-150000 / 680822\n",
      "[2025-12-05 16:28:05,901] [INFO] Wrapper docs in-memory | batch 4 => rows 150001-200000 / 680822\n",
      "[2025-12-05 16:28:05,902] [INFO] Wrapper docs in-memory | batch 5 => rows 200001-250000 / 680822\n",
      "[2025-12-05 16:28:05,903] [INFO] Wrapper docs in-memory | batch 6 => rows 250001-300000 / 680822\n",
      "[2025-12-05 16:28:05,903] [INFO] Wrapper docs in-memory | batch 7 => rows 300001-350000 / 680822\n",
      "[2025-12-05 16:28:05,904] [INFO] Wrapper docs in-memory | batch 8 => rows 350001-400000 / 680822\n",
      "[2025-12-05 16:28:05,905] [INFO] Wrapper docs in-memory | batch 9 => rows 400001-450000 / 680822\n",
      "[2025-12-05 16:28:05,905] [INFO] Wrapper docs in-memory | batch 10 => rows 450001-500000 / 680822\n",
      "[2025-12-05 16:28:05,906] [INFO] Wrapper docs in-memory | batch 11 => rows 500001-550000 / 680822\n",
      "[2025-12-05 16:28:05,907] [INFO] Wrapper docs in-memory | batch 12 => rows 550001-600000 / 680822\n",
      "[2025-12-05 16:28:05,908] [INFO] Wrapper docs in-memory | batch 13 => rows 600001-650000 / 680822\n",
      "[2025-12-05 16:28:05,909] [INFO] Wrapper docs in-memory | batch 14 => rows 650001-680822 / 680822\n",
      "[2025-12-05 16:28:05,910] [INFO] Wrapper dataset | docs=680822, tokens=680822 (limit=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading Documents\n",
      "================================================================================\n",
      "✓ Loading from wrapper (training dataset)\n",
      "================================================================================\n",
      "✓ Loaded documents: 680,822\n",
      "✓ Loaded tokens:    680,822\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load documents & tokens in batches (wrapper or CSV)\n",
    "# \n",
    "# IMPORTANT: If wrapper is loaded, this uses the EXACT same dataset that was used for training.\n",
    "# The wrapper stores the training dataset in dataset_as_list_of_strings.\n",
    "\n",
    "if USE_WRAPPER_DOCS_ONLY and wrapper is None:\n",
    "    raise ValueError(\"USE_WRAPPER_DOCS_ONLY=True but wrapper is None. Set USE_WRAPPER_DOCS_ONLY=False or load wrapper.\")\n",
    "\n",
    "dataset_csv = None\n",
    "if wrapper is None:\n",
    "    dataset_csv = DATASET_CSV\n",
    "    if dataset_csv is None:\n",
    "        raise ValueError(\"No wrapper and no DATASET_CSV provided.\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading Documents\")\n",
    "print(\"=\" * 80)\n",
    "if wrapper and USE_WRAPPER_DOCS_ONLY:\n",
    "    print(\"✓ Loading from wrapper (training dataset)\")\n",
    "else:\n",
    "    print(f\"⚠️  Loading from CSV: {dataset_csv}\")\n",
    "\n",
    "docs, docs_tokens = prepare_documents(\n",
    "    wrapper if USE_WRAPPER_DOCS_ONLY else None,\n",
    "    dataset_csv=dataset_csv,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    limit=LIMIT_DOCS,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Loaded documents: {len(docs):,}\")\n",
    "print(f\"✓ Loaded tokens:    {len(docs_tokens):,}\")\n",
    "if LIMIT_DOCS:\n",
    "    print(f\"  (Limited to {LIMIT_DOCS:,} documents)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load or build dictionary (with optional reuse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:28:05,926] [INFO] ▶ Dictionary build: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/data/interim/octis/corpus.tsv | start\n",
      "[2025-12-05 16:28:05,927] [INFO] ▶ Dictionary corpus streaming: corpus.tsv | start\n",
      "[2025-12-05 16:28:07,857] [INFO] Dictionary stream | processed 50000 rows\n",
      "[2025-12-05 16:28:07,858] [INFO] adding document #0 to Dictionary<0 unique tokens: []>\n",
      "[2025-12-05 16:28:08,342] [INFO] adding document #10000 to Dictionary<17137 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:08,902] [INFO] adding document #20000 to Dictionary<25789 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:09,386] [INFO] adding document #30000 to Dictionary<33185 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:09,822] [INFO] adding document #40000 to Dictionary<38507 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:10,231] [INFO] built Dictionary<42460 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 50000 documents (total 665407 corpus positions)\n",
      "[2025-12-05 16:28:10,232] [INFO] Dictionary build | added 50000 docs (size=42460)\n",
      "[2025-12-05 16:28:11,205] [INFO] Dictionary stream | processed 100000 rows\n",
      "[2025-12-05 16:28:11,206] [INFO] adding document #0 to Dictionary<42460 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:11,634] [INFO] adding document #10000 to Dictionary<46720 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:12,070] [INFO] adding document #20000 to Dictionary<50310 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:12,526] [INFO] adding document #30000 to Dictionary<53124 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:12,964] [INFO] adding document #40000 to Dictionary<58131 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:13,401] [INFO] built Dictionary<61942 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 100000 documents (total 1347089 corpus positions)\n",
      "[2025-12-05 16:28:13,402] [INFO] Dictionary build | added 50000 docs (size=61942)\n",
      "[2025-12-05 16:28:14,213] [INFO] Dictionary stream | processed 150000 rows\n",
      "[2025-12-05 16:28:14,214] [INFO] adding document #0 to Dictionary<61942 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:14,756] [INFO] adding document #10000 to Dictionary<68803 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:15,213] [INFO] adding document #20000 to Dictionary<72890 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:15,623] [INFO] adding document #30000 to Dictionary<77037 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:16,109] [INFO] adding document #40000 to Dictionary<79521 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:16,582] [INFO] built Dictionary<82089 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 150000 documents (total 1914398 corpus positions)\n",
      "[2025-12-05 16:28:16,582] [INFO] Dictionary build | added 50000 docs (size=82089)\n",
      "[2025-12-05 16:28:17,418] [INFO] Dictionary stream | processed 200000 rows\n",
      "[2025-12-05 16:28:17,418] [INFO] adding document #0 to Dictionary<82089 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:17,792] [INFO] adding document #10000 to Dictionary<85214 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:18,241] [INFO] adding document #20000 to Dictionary<87954 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:18,771] [INFO] adding document #30000 to Dictionary<89330 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:19,316] [INFO] adding document #40000 to Dictionary<90985 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:19,771] [INFO] built Dictionary<93675 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 200000 documents (total 2551089 corpus positions)\n",
      "[2025-12-05 16:28:19,772] [INFO] Dictionary build | added 50000 docs (size=93675)\n",
      "[2025-12-05 16:28:20,927] [INFO] Dictionary stream | processed 250000 rows\n",
      "[2025-12-05 16:28:20,928] [INFO] adding document #0 to Dictionary<93675 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:21,457] [INFO] adding document #10000 to Dictionary<96162 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:21,972] [INFO] adding document #20000 to Dictionary<98496 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:22,495] [INFO] adding document #30000 to Dictionary<100542 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:23,006] [INFO] adding document #40000 to Dictionary<102353 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:23,392] [INFO] built Dictionary<103787 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 250000 documents (total 3147860 corpus positions)\n",
      "[2025-12-05 16:28:23,393] [INFO] Dictionary build | added 50000 docs (size=103787)\n",
      "[2025-12-05 16:28:24,436] [INFO] Dictionary stream | processed 300000 rows\n",
      "[2025-12-05 16:28:24,436] [INFO] adding document #0 to Dictionary<103787 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:24,734] [INFO] adding document #10000 to Dictionary<105238 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:25,112] [INFO] adding document #20000 to Dictionary<106567 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:25,454] [INFO] adding document #30000 to Dictionary<108602 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:25,890] [INFO] adding document #40000 to Dictionary<110714 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:26,270] [INFO] built Dictionary<113331 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 300000 documents (total 3742356 corpus positions)\n",
      "[2025-12-05 16:28:26,271] [INFO] Dictionary build | added 50000 docs (size=113331)\n",
      "[2025-12-05 16:28:27,219] [INFO] Dictionary stream | processed 350000 rows\n",
      "[2025-12-05 16:28:27,220] [INFO] adding document #0 to Dictionary<113331 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:27,660] [INFO] adding document #10000 to Dictionary<116676 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:28,087] [INFO] adding document #20000 to Dictionary<118797 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:28,624] [INFO] adding document #30000 to Dictionary<120424 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:29,083] [INFO] adding document #40000 to Dictionary<122207 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:29,475] [INFO] built Dictionary<123483 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 350000 documents (total 4459722 corpus positions)\n",
      "[2025-12-05 16:28:29,476] [INFO] Dictionary build | added 50000 docs (size=123483)\n",
      "[2025-12-05 16:28:30,471] [INFO] Dictionary stream | processed 400000 rows\n",
      "[2025-12-05 16:28:30,471] [INFO] adding document #0 to Dictionary<123483 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:31,003] [INFO] adding document #10000 to Dictionary<124975 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:31,542] [INFO] adding document #20000 to Dictionary<126246 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:32,118] [INFO] adding document #30000 to Dictionary<127254 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:32,721] [INFO] adding document #40000 to Dictionary<128156 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:33,204] [INFO] built Dictionary<129411 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 400000 documents (total 5087832 corpus positions)\n",
      "[2025-12-05 16:28:33,205] [INFO] Dictionary build | added 50000 docs (size=129411)\n",
      "[2025-12-05 16:28:34,671] [INFO] Dictionary stream | processed 450000 rows\n",
      "[2025-12-05 16:28:34,671] [INFO] adding document #0 to Dictionary<129411 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:35,061] [INFO] adding document #10000 to Dictionary<130986 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:35,452] [INFO] adding document #20000 to Dictionary<132800 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:35,751] [INFO] adding document #30000 to Dictionary<135400 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:36,008] [INFO] adding document #40000 to Dictionary<137986 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:36,316] [INFO] built Dictionary<140364 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 450000 documents (total 5616645 corpus positions)\n",
      "[2025-12-05 16:28:36,317] [INFO] Dictionary build | added 50000 docs (size=140364)\n",
      "[2025-12-05 16:28:37,180] [INFO] Dictionary stream | processed 500000 rows\n",
      "[2025-12-05 16:28:37,181] [INFO] adding document #0 to Dictionary<140364 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:37,477] [INFO] adding document #10000 to Dictionary<142940 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:37,981] [INFO] adding document #20000 to Dictionary<144086 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:38,394] [INFO] adding document #30000 to Dictionary<145264 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:38,783] [INFO] adding document #40000 to Dictionary<147149 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:39,179] [INFO] built Dictionary<149145 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 500000 documents (total 6233938 corpus positions)\n",
      "[2025-12-05 16:28:39,179] [INFO] Dictionary build | added 50000 docs (size=149145)\n",
      "[2025-12-05 16:28:40,181] [INFO] Dictionary stream | processed 550000 rows\n",
      "[2025-12-05 16:28:40,182] [INFO] adding document #0 to Dictionary<149145 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:40,614] [INFO] adding document #10000 to Dictionary<152916 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:41,111] [INFO] adding document #20000 to Dictionary<155714 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:41,517] [INFO] adding document #30000 to Dictionary<157832 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:41,969] [INFO] adding document #40000 to Dictionary<160340 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:42,358] [INFO] built Dictionary<162877 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 550000 documents (total 6923166 corpus positions)\n",
      "[2025-12-05 16:28:42,359] [INFO] Dictionary build | added 50000 docs (size=162877)\n",
      "[2025-12-05 16:28:43,145] [INFO] Dictionary stream | processed 600000 rows\n",
      "[2025-12-05 16:28:43,146] [INFO] adding document #0 to Dictionary<162877 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:43,475] [INFO] adding document #10000 to Dictionary<165209 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:43,796] [INFO] adding document #20000 to Dictionary<167304 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:44,178] [INFO] adding document #30000 to Dictionary<170021 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:44,523] [INFO] adding document #40000 to Dictionary<171671 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:44,969] [INFO] built Dictionary<173629 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 600000 documents (total 7454665 corpus positions)\n",
      "[2025-12-05 16:28:44,970] [INFO] Dictionary build | added 50000 docs (size=173629)\n",
      "[2025-12-05 16:28:46,186] [INFO] Dictionary stream | processed 650000 rows\n",
      "[2025-12-05 16:28:46,187] [INFO] adding document #0 to Dictionary<173629 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:46,757] [INFO] adding document #10000 to Dictionary<176011 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:47,328] [INFO] adding document #20000 to Dictionary<178221 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:47,830] [INFO] adding document #30000 to Dictionary<179850 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:48,295] [INFO] adding document #40000 to Dictionary<181725 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:48,835] [INFO] built Dictionary<183291 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 650000 documents (total 8107430 corpus positions)\n",
      "[2025-12-05 16:28:48,836] [INFO] Dictionary build | added 50000 docs (size=183291)\n",
      "[2025-12-05 16:28:49,556] [INFO] ■ Dictionary corpus streaming: corpus.tsv | completed in 43.63 s\n",
      "[2025-12-05 16:28:49,556] [INFO] adding document #0 to Dictionary<183291 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:49,980] [INFO] adding document #10000 to Dictionary<184624 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:50,443] [INFO] adding document #20000 to Dictionary<185285 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:50,946] [INFO] adding document #30000 to Dictionary<185842 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...>\n",
      "[2025-12-05 16:28:50,990] [INFO] built Dictionary<185903 unique tokens: ['e', 'h', 'prologue', 'tired.', 'was']...> from 680822 documents (total 8536057 corpus positions)\n",
      "[2025-12-05 16:28:50,991] [INFO] Dictionary build | final flush 30822 docs (size=185903)\n",
      "[2025-12-05 16:28:51,246] [INFO] Dictionary build | completed with 185903 tokens\n",
      "[2025-12-05 16:28:51,247] [INFO] ■ Dictionary build: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/data/interim/octis/corpus.tsv | completed in 45.32 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 185903\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load or build Gensim dictionary (streaming/batched)\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "if DICTIONARY_PICKLE_PATH is not None and Path(DICTIONARY_PICKLE_PATH).is_file():\n",
    "    with stage_timer(f\"Loading Dictionary from pickle: {Path(DICTIONARY_PICKLE_PATH).name}\"):\n",
    "        with open(DICTIONARY_PICKLE_PATH, \"rb\") as f:\n",
    "            dictionary: Dictionary = pickle.load(f)\n",
    "        LOGGER.info(\"Loaded dictionary with %d tokens\", len(dictionary))\n",
    "else:\n",
    "    dictionary = load_dictionary_from_corpus(\n",
    "        corpus_path=DICTIONARY_CORPUS_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    # Optional: save for future runs\n",
    "    # DICTIONARY_PICKLE_PATH = OUTPUT_DIR / f\"dictionary_{MODEL_NAME_SAFE}.pkl\"\n",
    "    # with stage_timer(f\"Saving Dictionary to {DICTIONARY_PICKLE_PATH.name}\"):\n",
    "    #     with open(DICTIONARY_PICKLE_PATH, \"wb\") as f:\n",
    "    #         pickle.dump(dictionary, f)\n",
    "\n",
    "print(\"Dictionary size:\", len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Build topic quality table + noise candidate labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:28:51,284] [INFO] ▶ Running topic EDA + noise candidate labeling | start\n",
      "[2025-12-05 16:28:51,286] [INFO] ▶ Building topic quality table | start\n",
      "[2025-12-05 16:28:51,310] [INFO] ▶ Extracting all topic representations | start\n",
      "[2025-12-05 16:28:51,310] [INFO] Extracting 'Main' representation from topic_representations_\n",
      "[2025-12-05 16:28:51,317] [INFO] Extracted 363 topics for 'Main' representation\n",
      "[2025-12-05 16:28:51,318] [INFO] Extracting additional aspects from topic_aspects_\n",
      "[2025-12-05 16:28:51,323] [INFO] Extracted 360 topics for 'KeyBERT' representation\n",
      "[2025-12-05 16:28:51,328] [INFO] Extracted 363 topics for 'MMR' representation\n",
      "[2025-12-05 16:28:51,334] [INFO] Extracted 361 topics for 'POS' representation\n",
      "[2025-12-05 16:28:51,334] [INFO] ■ Extracting all topic representations | completed in 0.02 s\n",
      "[2025-12-05 16:28:51,343] [INFO] ▶ Extracting all topic representations | start\n",
      "[2025-12-05 16:28:51,344] [INFO] Extracting 'Main' representation from topic_representations_\n",
      "[2025-12-05 16:28:51,349] [INFO] Extracted 363 topics for 'Main' representation\n",
      "[2025-12-05 16:28:51,349] [INFO] Extracting additional aspects from topic_aspects_\n",
      "[2025-12-05 16:28:51,354] [INFO] Extracted 360 topics for 'KeyBERT' representation\n",
      "[2025-12-05 16:28:51,360] [INFO] Extracted 363 topics for 'MMR' representation\n",
      "[2025-12-05 16:28:51,365] [INFO] Extracted 361 topics for 'POS' representation\n",
      "[2025-12-05 16:28:51,366] [INFO] ■ Extracting all topic representations | completed in 0.02 s\n",
      "[2025-12-05 16:28:51,369] [INFO] ▶ Computing per-topic POS coherence (c_v) | start\n",
      "[2025-12-05 16:28:51,464] [INFO] using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows\n",
      "[2025-12-05 16:29:10,755] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,755] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,754] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,754] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,755] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,762] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,759] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,760] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,761] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,761] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,764] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,762] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,766] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,763] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,766] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,771] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,774] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,775] [INFO] serializing accumulator to return to master...\n",
      "[2025-12-05 16:29:10,791] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,760] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,765] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:10,773] [INFO] accumulator serialized\n",
      "[2025-12-05 16:29:15,524] [INFO] 11 accumulators retrieved from output queue\n",
      "[2025-12-05 16:29:16,835] [INFO] accumulated word occurrence stats for 681048 virtual documents\n",
      "[2025-12-05 16:29:45,806] [INFO] ■ Computing per-topic POS coherence (c_v) | completed in 54.44 s\n",
      "[2025-12-05 16:29:45,856] [INFO] ■ Building topic quality table | completed in 54.57 s\n",
      "[2025-12-05 16:29:45,857] [INFO] ■ Running topic EDA + noise candidate labeling | completed in 54.57 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topics (excl. -1): 368\n",
      "Candidate noisy topics: 13\n",
      "\n",
      "=== 20 topics with lowest POS coherence ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>coherence_c_v_pos</th>\n",
       "      <th>n_pos_words</th>\n",
       "      <th>noise_candidate</th>\n",
       "      <th>noise_reason</th>\n",
       "      <th>inspection_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>298</td>\n",
       "      <td>180</td>\n",
       "      <td>0.112269</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>298_aren_jail_nightlife_means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>340</td>\n",
       "      <td>155</td>\n",
       "      <td>0.116221</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>340_reida_ha_mouths_samuela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>304</td>\n",
       "      <td>178</td>\n",
       "      <td>0.118944</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>304_jar_barista_trance_suggests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>324</td>\n",
       "      <td>163</td>\n",
       "      <td>0.133797</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>324_sensory_overreacting_ita_discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>295</td>\n",
       "      <td>0.138756</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>180_ode_deserve_believed_ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>222</td>\n",
       "      <td>246</td>\n",
       "      <td>0.141362</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>222_yeah_benjamina_aftera_trial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>346</td>\n",
       "      <td>152</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>346_clairea_admirable_passionate_unenthusiasti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>321</td>\n",
       "      <td>165</td>\n",
       "      <td>0.148150</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>321_cilliana_antidote_maine_evon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>282</td>\n",
       "      <td>0.148865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>190_et_religion_priest_deity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>81</td>\n",
       "      <td>535</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>81_parents_children_kids_families</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>196</td>\n",
       "      <td>270</td>\n",
       "      <td>0.149844</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>196_attractive_hea_incredibly_refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>154</td>\n",
       "      <td>328</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>154_pregnancy_test_unplanned_weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>420</td>\n",
       "      <td>0.168192</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>112_rain_winter_wet_raining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>325</td>\n",
       "      <td>163</td>\n",
       "      <td>0.168937</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>325_owl_leta_herea_repeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72</td>\n",
       "      <td>571</td>\n",
       "      <td>0.182126</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>72_natea_hockey_game_bankers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>342</td>\n",
       "      <td>154</td>\n",
       "      <td>0.184178</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>342_wrong_somethinga_terribly_wrongs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>320</td>\n",
       "      <td>165</td>\n",
       "      <td>0.184476</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>320_feelsa_feela_better_feels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>83</td>\n",
       "      <td>524</td>\n",
       "      <td>0.184480</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>83_goats_dogs_animal_puppies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>302</td>\n",
       "      <td>179</td>\n",
       "      <td>0.186621</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>302_gun_weapon_bullets_pistol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>191</td>\n",
       "      <td>280</td>\n",
       "      <td>0.190679</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>191_eddiea_wasn_onboarding_didn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count  coherence_c_v_pos  n_pos_words  noise_candidate  \\\n",
       "13    298    180           0.112269         10.0            False   \n",
       "14    340    155           0.116221         10.0            False   \n",
       "15    304    178           0.118944         10.0            False   \n",
       "16    324    163           0.133797         10.0            False   \n",
       "17    180    295           0.138756         10.0            False   \n",
       "18    222    246           0.141362          4.0            False   \n",
       "19    346    152           0.142617         10.0            False   \n",
       "20    321    165           0.148150         10.0            False   \n",
       "21    190    282           0.148865         10.0            False   \n",
       "22     81    535           0.149391         10.0            False   \n",
       "23    196    270           0.149844         10.0            False   \n",
       "24    154    328           0.157500         10.0            False   \n",
       "25    112    420           0.168192         10.0            False   \n",
       "26    325    163           0.168937         10.0            False   \n",
       "27     72    571           0.182126         10.0            False   \n",
       "28    342    154           0.184178         10.0            False   \n",
       "29    320    165           0.184476         10.0            False   \n",
       "30     83    524           0.184480         10.0            False   \n",
       "31    302    179           0.186621         10.0            False   \n",
       "32    191    280           0.190679         10.0            False   \n",
       "\n",
       "   noise_reason                                   inspection_label  \n",
       "13                                   298_aren_jail_nightlife_means  \n",
       "14                                     340_reida_ha_mouths_samuela  \n",
       "15                                 304_jar_barista_trance_suggests  \n",
       "16                         324_sensory_overreacting_ita_discussion  \n",
       "17                                  180_ode_deserve_believed_ended  \n",
       "18                                 222_yeah_benjamina_aftera_trial  \n",
       "19               346_clairea_admirable_passionate_unenthusiasti...  \n",
       "20                                321_cilliana_antidote_maine_evon  \n",
       "21                                    190_et_religion_priest_deity  \n",
       "22                               81_parents_children_kids_families  \n",
       "23                           196_attractive_hea_incredibly_refined  \n",
       "24                              154_pregnancy_test_unplanned_weeks  \n",
       "25                                     112_rain_winter_wet_raining  \n",
       "26                                      325_owl_leta_herea_repeats  \n",
       "27                                    72_natea_hockey_game_bankers  \n",
       "28                            342_wrong_somethinga_terribly_wrongs  \n",
       "29                                   320_feelsa_feela_better_feels  \n",
       "30                                    83_goats_dogs_animal_puppies  \n",
       "31                                   302_gun_weapon_bullets_pistol  \n",
       "32                                 191_eddiea_wasn_onboarding_didn  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 20 candidate noisy topics (for manual inspection) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>coherence_c_v_pos</th>\n",
       "      <th>n_pos_words</th>\n",
       "      <th>noise_reason</th>\n",
       "      <th>inspection_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>182</td>\n",
       "      <td>291</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>2.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 182_tough_umma_jer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 347_yessss___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262</td>\n",
       "      <td>204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 262_imply_clever_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 241_okaya_wella_ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183</td>\n",
       "      <td>290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 183_division_stick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>141</td>\n",
       "      <td>360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>few_pos&lt;3</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3] 141_rush___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>359</td>\n",
       "      <td>146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 359_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>283</td>\n",
       "      <td>188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 283_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>224</td>\n",
       "      <td>245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 224____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>186</td>\n",
       "      <td>286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 186____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>132</td>\n",
       "      <td>376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 132____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>2062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 18____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>2064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>few_pos&lt;3;low_coh&lt;0.00</td>\n",
       "      <td>[NOISE_CANDIDATE:few_pos&lt;3;low_coh&lt;0.00] 17____</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count  coherence_c_v_pos  n_pos_words            noise_reason  \\\n",
       "0     182    291           0.345404          2.0               few_pos<3   \n",
       "1     347    150           1.000000          1.0               few_pos<3   \n",
       "2     262    204           1.000000          1.0               few_pos<3   \n",
       "3     241    220           1.000000          1.0               few_pos<3   \n",
       "4     183    290           1.000000          1.0               few_pos<3   \n",
       "5     141    360           1.000000          1.0               few_pos<3   \n",
       "6     359    146                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "7     283    188                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "8     224    245                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "9     186    286                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "10    132    376                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "11     18   2062                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "12     17   2064                NaN          NaN  few_pos<3;low_coh<0.00   \n",
       "\n",
       "                                     inspection_label  \n",
       "0   [NOISE_CANDIDATE:few_pos<3] 182_tough_umma_jer...  \n",
       "1           [NOISE_CANDIDATE:few_pos<3] 347_yessss___  \n",
       "2   [NOISE_CANDIDATE:few_pos<3] 262_imply_clever_h...  \n",
       "3   [NOISE_CANDIDATE:few_pos<3] 241_okaya_wella_ar...  \n",
       "4   [NOISE_CANDIDATE:few_pos<3] 183_division_stick...  \n",
       "5             [NOISE_CANDIDATE:few_pos<3] 141_rush___  \n",
       "6   [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 359_p...  \n",
       "7   [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 283_i...  \n",
       "8    [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 224____  \n",
       "9    [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 186____  \n",
       "10   [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 132____  \n",
       "11    [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 18____  \n",
       "12    [NOISE_CANDIDATE:few_pos<3;low_coh<0.00] 17____  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Build topic quality table and flag noisy candidates\n",
    "\n",
    "with stage_timer(\"Running topic EDA + noise candidate labeling\"):\n",
    "    quality_df = build_topic_quality_table(\n",
    "        topic_model,\n",
    "        docs_tokens=docs_tokens,\n",
    "        dictionary=dictionary,\n",
    "        min_size=MIN_TOPIC_SIZE,\n",
    "        min_pos_words=MIN_POS_WORDS,\n",
    "        min_pos_coherence=MIN_POS_COHERENCE,\n",
    "        top_k=TOP_K,\n",
    "    )\n",
    "\n",
    "print(\"Total topics (excl. -1):\", len(quality_df))\n",
    "print(\"Candidate noisy topics:\", int(quality_df[\"noise_candidate\"].sum()))\n",
    "\n",
    "# Quick peek: worst topics by POS coherence\n",
    "print(\"\\n=== 20 topics with lowest POS coherence ===\")\n",
    "display(\n",
    "    quality_df.sort_values(\"coherence_c_v_pos\").head(20)[\n",
    "        [\"Topic\", \"Count\", \"coherence_c_v_pos\", \"n_pos_words\", \"noise_candidate\", \"noise_reason\", \"inspection_label\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Quick peek: candidate noisy topics\n",
    "print(\"\\n=== 20 candidate noisy topics (for manual inspection) ===\")\n",
    "display(\n",
    "    quality_df[quality_df[\"noise_candidate\"]].head(20)[\n",
    "        [\"Topic\", \"Count\", \"coherence_c_v_pos\", \"n_pos_words\", \"noise_reason\", \"inspection_label\"]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save EDA results for manual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Label noisy topics (candidates + POS < 10)\n",
    "\n",
    "This cell labels topics as noisy based on:\n",
    "1. All existing noise candidates (from previous analysis)\n",
    "2. All topics with POS words < 10\n",
    "\n",
    "These labels will be applied to the model for visualization and can be used in downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Noisy Topic Labeling Summary\n",
      "================================================================================\n",
      "Existing noise candidates: 13\n",
      "Topics with POS words < 10: 20\n",
      "Total unique noisy topics to label: 20\n",
      "================================================================================\n",
      "\n",
      "=== Topics to be labeled as noisy ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>n_pos_words</th>\n",
       "      <th>coherence_c_v_pos</th>\n",
       "      <th>inspection_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>2064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 17____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>2062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 18____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>106</td>\n",
       "      <td>433</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.201386</td>\n",
       "      <td>[NOISE:pos&lt;10(6)] 106_wasa_thata_likea_sa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>120</td>\n",
       "      <td>399</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.538068</td>\n",
       "      <td>[NOISE:pos&lt;10(4)] 120_aye_string_parts_fianca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>132</td>\n",
       "      <td>376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 132____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>141</td>\n",
       "      <td>360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(1)] 141_rush___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>182</td>\n",
       "      <td>291</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(2)] 182_tough_um...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183</td>\n",
       "      <td>290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(1)] 183_division...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>186</td>\n",
       "      <td>286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 186____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>205</td>\n",
       "      <td>260</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.232126</td>\n",
       "      <td>[NOISE:pos&lt;10(8)] 205_fuckup_rule_horny_rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>222</td>\n",
       "      <td>246</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.141362</td>\n",
       "      <td>[NOISE:pos&lt;10(4)] 222_yeah_benjamina_aftera_trial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>224</td>\n",
       "      <td>245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 224____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>238</td>\n",
       "      <td>226</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.471486</td>\n",
       "      <td>[NOISE:pos&lt;10(3)] 238_rumbles_load_huh_point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(1)] 241_okaya_we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262</td>\n",
       "      <td>204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(1)] 262_imply_cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>283</td>\n",
       "      <td>188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 283_ina___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>300</td>\n",
       "      <td>179</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.318707</td>\n",
       "      <td>[NOISE:pos&lt;10(9)] 300_buts_steadied_tycoon_claraa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>336</td>\n",
       "      <td>157</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.292060</td>\n",
       "      <td>[NOISE:pos&lt;10(4)] 336_alternatives_automatical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347</td>\n",
       "      <td>150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[NOISE:noise_candidate;pos&lt;10(1)] 347_yessss___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>359</td>\n",
       "      <td>146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NOISE:noise_candidate] 359_pun_th_intended_de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count  n_pos_words  coherence_c_v_pos  \\\n",
       "12      17   2064          NaN                NaN   \n",
       "11      18   2062          NaN                NaN   \n",
       "43     106    433          6.0           0.201386   \n",
       "359    120    399          4.0           0.538068   \n",
       "10     132    376          NaN                NaN   \n",
       "5      141    360          1.0           1.000000   \n",
       "0      182    291          2.0           0.345404   \n",
       "4      183    290          1.0           1.000000   \n",
       "9      186    286          NaN                NaN   \n",
       "76     205    260          8.0           0.232126   \n",
       "18     222    246          4.0           0.141362   \n",
       "8      224    245          NaN                NaN   \n",
       "354    238    226          3.0           0.471486   \n",
       "3      241    220          1.0           1.000000   \n",
       "2      262    204          1.0           1.000000   \n",
       "7      283    188          NaN                NaN   \n",
       "236    300    179          9.0           0.318707   \n",
       "178    336    157          4.0           0.292060   \n",
       "1      347    150          1.0           1.000000   \n",
       "6      359    146          NaN                NaN   \n",
       "\n",
       "                                      inspection_label  \n",
       "12                      [NOISE:noise_candidate] 17____  \n",
       "11                      [NOISE:noise_candidate] 18____  \n",
       "43           [NOISE:pos<10(6)] 106_wasa_thata_likea_sa  \n",
       "359      [NOISE:pos<10(4)] 120_aye_string_parts_fianca  \n",
       "10                     [NOISE:noise_candidate] 132____  \n",
       "5        [NOISE:noise_candidate;pos<10(1)] 141_rush___  \n",
       "0    [NOISE:noise_candidate;pos<10(2)] 182_tough_um...  \n",
       "4    [NOISE:noise_candidate;pos<10(1)] 183_division...  \n",
       "9                      [NOISE:noise_candidate] 186____  \n",
       "76       [NOISE:pos<10(8)] 205_fuckup_rule_horny_rules  \n",
       "18   [NOISE:pos<10(4)] 222_yeah_benjamina_aftera_trial  \n",
       "8                      [NOISE:noise_candidate] 224____  \n",
       "354       [NOISE:pos<10(3)] 238_rumbles_load_huh_point  \n",
       "3    [NOISE:noise_candidate;pos<10(1)] 241_okaya_we...  \n",
       "2    [NOISE:noise_candidate;pos<10(1)] 262_imply_cl...  \n",
       "7                   [NOISE:noise_candidate] 283_ina___  \n",
       "236  [NOISE:pos<10(9)] 300_buts_steadied_tycoon_claraa  \n",
       "178  [NOISE:pos<10(4)] 336_alternatives_automatical...  \n",
       "1      [NOISE:noise_candidate;pos<10(1)] 347_yessss___  \n",
       "6    [NOISE:noise_candidate] 359_pun_th_intended_de...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:29:46,565] [INFO] ▶ Setting custom labels for noisy topics | start\n",
      "[2025-12-05 16:29:46,611] [INFO] Updated labels for 20 topics (noisy + existing)\n",
      "[2025-12-05 16:29:46,611] [INFO] ■ Setting custom labels for noisy topics | completed in 0.04 s\n",
      "[2025-12-05 16:29:46,613] [INFO] ▶ Saving native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels | start\n",
      "2025-12-05 16:29:46,614 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 20 labels for noisy topics\n",
      "\n",
      "✓ Applied labels to 20 noisy topics\n",
      "✓ Total labels in model: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:29:58,475] [INFO] Saved native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels\n",
      "[2025-12-05 16:29:58,476] [INFO] ■ Saving native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels | completed in 11.86 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved native BERTopic model to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:30:03,496] [INFO] Backed up existing file: model_1_with_noise_labels.pkl -> model_1_with_noise_labels_backup_20251205_162958.pkl\n",
      "[2025-12-05 16:30:03,497] [INFO] ▶ Saving wrapper with noise labels to model_1_with_noise_labels.pkl | start\n",
      "[2025-12-05 16:30:17,894] [INFO] Saved wrapper with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels.pkl\n",
      "[2025-12-05 16:30:17,895] [INFO] ■ Saving wrapper with noise labels to model_1_with_noise_labels.pkl | completed in 14.40 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved wrapper pickle to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Label noisy topics (candidates + POS < 10)\n",
    "\n",
    "# Identify all topics to label as noisy:\n",
    "# 1. Existing noise candidates\n",
    "# 2. Topics with POS words < 10\n",
    "\n",
    "noise_candidates = quality_df[quality_df[\"noise_candidate\"]].copy()\n",
    "topics_few_pos = quality_df[quality_df[\"n_pos_words\"].fillna(0) < 10].copy()\n",
    "\n",
    "# Combine both sets (remove duplicates)\n",
    "all_noisy_topics = pd.concat([noise_candidates, topics_few_pos]).drop_duplicates(subset=[\"Topic\"])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Noisy Topic Labeling Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Existing noise candidates: {len(noise_candidates)}\")\n",
    "print(f\"Topics with POS words < 10: {len(topics_few_pos)}\")\n",
    "print(f\"Total unique noisy topics to label: {len(all_noisy_topics)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create inspection labels for all noisy topics\n",
    "def create_noise_label(row) -> str:\n",
    "    \"\"\"Create inspection label for a noisy topic.\"\"\"\n",
    "    base_name = str(row.get(\"Name\", \"\") or \"\").strip()\n",
    "    reasons = []\n",
    "    \n",
    "    # Check why it's noisy\n",
    "    if row.get(\"noise_candidate\", False):\n",
    "        reasons.append(\"noise_candidate\")\n",
    "    if pd.notna(row.get(\"n_pos_words\")) and row[\"n_pos_words\"] < 10:\n",
    "        reasons.append(f\"pos<10({int(row['n_pos_words'])})\")\n",
    "    \n",
    "    reason_str = \";\".join(reasons)\n",
    "    if not reason_str:\n",
    "        return base_name\n",
    "    \n",
    "    return f\"[NOISE:{reason_str}] {base_name}\"\n",
    "\n",
    "all_noisy_topics[\"inspection_label\"] = all_noisy_topics.apply(create_noise_label, axis=1)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== Topics to be labeled as noisy ===\")\n",
    "display(\n",
    "    all_noisy_topics[[\"Topic\", \"Count\", \"n_pos_words\", \"coherence_c_v_pos\", \"inspection_label\"]]\n",
    "    .sort_values(\"Topic\")\n",
    ")\n",
    "\n",
    "# Apply labels to model\n",
    "noise_labels = {\n",
    "    int(row.Topic): str(row.inspection_label)\n",
    "    for row in all_noisy_topics.itertuples(index=False)\n",
    "}\n",
    "\n",
    "print(f\"\\nPrepared {len(noise_labels)} labels for noisy topics\")\n",
    "\n",
    "# Merge with existing labels\n",
    "existing_labels = getattr(topic_model, \"custom_labels_\", None)\n",
    "labels_dict = {}\n",
    "\n",
    "if isinstance(existing_labels, dict):\n",
    "    labels_dict.update(existing_labels)\n",
    "    print(f\"Found {len(existing_labels)} existing labels\")\n",
    "\n",
    "labels_dict.update(noise_labels)\n",
    "\n",
    "with stage_timer(\"Setting custom labels for noisy topics\"):\n",
    "    topic_model.set_topic_labels(labels_dict)\n",
    "    LOGGER.info(\"Updated labels for %d topics (noisy + existing)\", len(labels_dict))\n",
    "\n",
    "print(f\"\\n✓ Applied labels to {len(noise_labels)} noisy topics\")\n",
    "print(f\"✓ Total labels in model: {len(labels_dict)}\")\n",
    "\n",
    "# Save updated model in BOTH formats: native BERTopic directory AND wrapper pickle\n",
    "save_model = True  # Set to False if you don't want to save yet\n",
    "if save_model:\n",
    "    # 1. Save as native BERTopic model (directory format)\n",
    "    native_model_dir = BASE_DIR / EMBEDDING_MODEL / f\"model_{PARETO_RANK}_with_noise_labels\"\n",
    "    # Remove directory if it exists (BERTopic.save() will recreate it)\n",
    "    if native_model_dir.exists() and native_model_dir.is_dir():\n",
    "        import shutil\n",
    "        shutil.rmtree(native_model_dir)\n",
    "    \n",
    "    with stage_timer(f\"Saving native BERTopic model with noise labels to {native_model_dir}\"):\n",
    "        topic_model.save(str(native_model_dir))\n",
    "        LOGGER.info(\"Saved native BERTopic model with noise labels to %s\", native_model_dir)\n",
    "        print(f\"✓ Saved native BERTopic model to: {native_model_dir}\")\n",
    "    \n",
    "    # 2. Save as wrapper pickle (file format) - only if wrapper was loaded\n",
    "    if wrapper is not None:\n",
    "        wrapper_pickle_path = BASE_DIR / EMBEDDING_MODEL / f\"model_{PARETO_RANK}_with_noise_labels.pkl\"\n",
    "        backup_existing_file(wrapper_pickle_path)\n",
    "        \n",
    "        with stage_timer(f\"Saving wrapper with noise labels to {wrapper_pickle_path.name}\"):\n",
    "            with open(wrapper_pickle_path, \"wb\") as f:\n",
    "                pickle.dump(wrapper, f)\n",
    "            LOGGER.info(\"Saved wrapper with noise labels to %s\", wrapper_pickle_path)\n",
    "            print(f\"✓ Saved wrapper pickle to: {wrapper_pickle_path}\")\n",
    "    else:\n",
    "        print(\"⚠️  Wrapper not available (loaded native model), skipping wrapper save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:30:17,916] [INFO] Backed up existing file: topic_quality_paraphrase-MiniLM-L6-v2.csv -> topic_quality_paraphrase-MiniLM-L6-v2_backup_20251205_163017.csv\n",
      "[2025-12-05 16:30:17,917] [INFO] ▶ Saving topic_quality_paraphrase-MiniLM-L6-v2.csv | start\n",
      "[2025-12-05 16:30:17,941] [INFO] Saved 368 rows to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/results/stage06_eda/topic_quality_paraphrase-MiniLM-L6-v2.csv\n",
      "[2025-12-05 16:30:17,942] [INFO] ■ Saving topic_quality_paraphrase-MiniLM-L6-v2.csv | completed in 0.02 s\n",
      "[2025-12-05 16:30:17,943] [INFO] Backed up existing file: topic_noise_candidates_paraphrase-MiniLM-L6-v2.csv -> topic_noise_candidates_paraphrase-MiniLM-L6-v2_backup_20251205_163017.csv\n",
      "[2025-12-05 16:30:17,944] [INFO] ▶ Saving topic_noise_candidates_paraphrase-MiniLM-L6-v2.csv | start\n",
      "[2025-12-05 16:30:17,947] [INFO] Saved 13 rows to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/results/stage06_eda/topic_noise_candidates_paraphrase-MiniLM-L6-v2.csv\n",
      "[2025-12-05 16:30:17,948] [INFO] ■ Saving topic_noise_candidates_paraphrase-MiniLM-L6-v2.csv | completed in 0.00 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full quality table to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/results/stage06_eda/topic_quality_paraphrase-MiniLM-L6-v2.csv\n",
      "Saved noise candidates to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/results/stage06_eda/topic_noise_candidates_paraphrase-MiniLM-L6-v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Save topic quality table + noisy-topic subset\n",
    "\n",
    "quality_path_full = OUTPUT_DIR / f\"topic_quality_{MODEL_NAME_SAFE}.csv\"\n",
    "quality_path_noise = OUTPUT_DIR / f\"topic_noise_candidates_{MODEL_NAME_SAFE}.csv\"\n",
    "\n",
    "for path, df_to_save in [\n",
    "    (quality_path_full, quality_df),\n",
    "    (quality_path_noise, quality_df[quality_df[\"noise_candidate\"]]),\n",
    "]:\n",
    "    backup_existing_file(path)\n",
    "    with stage_timer(f\"Saving {path.name}\"):\n",
    "        df_to_save.to_csv(path, index=False)\n",
    "        LOGGER.info(\"Saved %d rows to %s\", len(df_to_save), path)\n",
    "\n",
    "print(\"Saved full quality table to:\", quality_path_full)\n",
    "print(\"Saved noise candidates to:\", quality_path_noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 (Optional): Add labels to noisy topics in the model\n",
    "\n",
    "This cell is optional. It marks noisy topics inside the BERTopic model (for visualizations, etc.) but does **not** delete topics. You can later overwrite these when you add LLM-generated labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:30:17,974] [INFO] Prepared 13 labels for noise candidates\n",
      "[2025-12-05 16:30:17,976] [INFO] ▶ Setting custom labels for noisy topics | start\n",
      "[2025-12-05 16:30:18,010] [INFO] Updated labels for 13 topics (noise candidates + existing)\n",
      "[2025-12-05 16:30:18,011] [INFO] ■ Setting custom labels for noisy topics | completed in 0.03 s\n",
      "[2025-12-05 16:30:18,012] [INFO] ▶ Saving native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels | start\n",
      "2025-12-05 16:30:18,013 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 13 labels for candidate noisy topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:30:32,619] [INFO] Saved native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels\n",
      "[2025-12-05 16:30:32,621] [INFO] ■ Saving native BERTopic model with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels | completed in 14.61 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved native BERTopic model to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 16:30:38,695] [INFO] Backed up existing file: model_1_with_noise_labels.pkl -> model_1_with_noise_labels_backup_20251205_163032.pkl\n",
      "[2025-12-05 16:30:38,696] [INFO] ▶ Saving wrapper with noise labels to model_1_with_noise_labels.pkl | start\n",
      "[2025-12-05 16:31:05,555] [INFO] Saved wrapper with noise labels to /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels.pkl\n",
      "[2025-12-05 16:31:05,557] [INFO] ■ Saving wrapper with noise labels to model_1_with_noise_labels.pkl | completed in 26.86 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved wrapper pickle to: /home/polina/Documents/goodreads_romance_research_cursor/billionaire_novels_rating_predictor/models/retrained/paraphrase-MiniLM-L6-v2/model_1_with_noise_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 (optional): Push inspection labels into BERTopic for noisy topics\n",
    "\n",
    "# Build a label dict only for candidate noisy topics\n",
    "noise_candidates_df = quality_df[quality_df[\"noise_candidate\"]]\n",
    "\n",
    "noise_labels = apply_noise_labels_to_model(\n",
    "    topic_model,\n",
    "    quality_df,\n",
    "    only_noise_candidates=True,\n",
    ")\n",
    "\n",
    "print(f\"Prepared {len(noise_labels)} labels for candidate noisy topics\")\n",
    "\n",
    "# Option 1: Only label noisy topics (other topics keep their existing labels / names)\n",
    "# We need to merge with any existing labels; otherwise, set_topic_labels would overwrite them all.\n",
    "# If you haven't set custom labels before, this will just set labels for noisy topics.\n",
    "\n",
    "existing_labels = getattr(topic_model, \"custom_labels_\", None)\n",
    "labels_dict = {}\n",
    "\n",
    "if isinstance(existing_labels, dict):\n",
    "    labels_dict.update(existing_labels)\n",
    "\n",
    "labels_dict.update(noise_labels)\n",
    "\n",
    "with stage_timer(\"Setting custom labels for noisy topics\"):\n",
    "    topic_model.set_topic_labels(labels_dict)  # uses BERTopic's custom_labels_ field\n",
    "    LOGGER.info(\"Updated labels for %d topics (noise candidates + existing)\", len(labels_dict))\n",
    "\n",
    "# Save updated model in BOTH formats: native BERTopic directory AND wrapper pickle\n",
    "# 1. Save as native BERTopic model (directory format)\n",
    "native_model_dir = BASE_DIR / EMBEDDING_MODEL / f\"model_{PARETO_RANK}_with_noise_labels\"\n",
    "# Remove directory if it exists (BERTopic.save() will recreate it)\n",
    "if native_model_dir.exists() and native_model_dir.is_dir():\n",
    "    import shutil\n",
    "    shutil.rmtree(native_model_dir)\n",
    "\n",
    "with stage_timer(f\"Saving native BERTopic model with noise labels to {native_model_dir}\"):\n",
    "    topic_model.save(str(native_model_dir))\n",
    "    LOGGER.info(\"Saved native BERTopic model with noise labels to %s\", native_model_dir)\n",
    "    print(f\"✓ Saved native BERTopic model to: {native_model_dir}\")\n",
    "\n",
    "# 2. Save as wrapper pickle (file format) - only if wrapper was loaded\n",
    "if wrapper is not None:\n",
    "    wrapper_pickle_path = BASE_DIR / EMBEDDING_MODEL / f\"model_{PARETO_RANK}_with_noise_labels.pkl\"\n",
    "    backup_existing_file(wrapper_pickle_path)\n",
    "    \n",
    "    with stage_timer(f\"Saving wrapper with noise labels to {wrapper_pickle_path.name}\"):\n",
    "        with open(wrapper_pickle_path, \"wb\") as f:\n",
    "            pickle.dump(wrapper, f)\n",
    "        LOGGER.info(\"Saved wrapper with noise labels to %s\", wrapper_pickle_path)\n",
    "        print(f\"✓ Saved wrapper pickle to: {wrapper_pickle_path}\")\n",
    "else:\n",
    "    print(\"⚠️  Wrapper not available (loaded native model), skipping wrapper save\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
