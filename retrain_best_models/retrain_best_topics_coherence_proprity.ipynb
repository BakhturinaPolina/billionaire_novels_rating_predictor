{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPK8CL0cWMoNrbz9hMUsy4M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c3a274f2610e41b8a1f3238af8dbec31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a41e6fbe92148719c89958ce5110f56","IPY_MODEL_873ab9b01bc04ef880c1130114f6b9b1","IPY_MODEL_bdcc119db5434afd8bc4d0e8ddd1db01"],"layout":"IPY_MODEL_b694a9c27ded451a890ba680d0f52a48"}},"6a41e6fbe92148719c89958ce5110f56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c247ad1c2004ad3ad4386456f773a85","placeholder":"​","style":"IPY_MODEL_46507baf47e24bf4a95c67ae56d7c0d5","value":"Batches: 100%"}},"873ab9b01bc04ef880c1130114f6b9b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_164bbe0b6679426a800219f2ec4d96db","max":10638,"min":0,"orientation":"horizontal","style":"IPY_MODEL_264fa02e95b646ce88b73fb7c256cfde","value":10638}},"bdcc119db5434afd8bc4d0e8ddd1db01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82bc5fb123af4ecaa14cf23351dfb467","placeholder":"​","style":"IPY_MODEL_b366e7a42d9c4abeb23d6b1cf4f3eb6b","value":" 10638/10638 [05:03&lt;00:00, 95.98it/s]"}},"b694a9c27ded451a890ba680d0f52a48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c247ad1c2004ad3ad4386456f773a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46507baf47e24bf4a95c67ae56d7c0d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"164bbe0b6679426a800219f2ec4d96db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"264fa02e95b646ce88b73fb7c256cfde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82bc5fb123af4ecaa14cf23351dfb467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b366e7a42d9c4abeb23d6b1cf4f3eb6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01470135a1864a999daff2ccfad59e8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14223c566e2c442084e7b08055519314","IPY_MODEL_83968cbbc4e449a8bcb23310d39d30df","IPY_MODEL_9648bc57cf4348ebb33cd4cfc514ce04"],"layout":"IPY_MODEL_3c2267d952c04278a25dcd7c68c534df"}},"14223c566e2c442084e7b08055519314":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26e015bfd6024fac9568d5d11d7162b8","placeholder":"​","style":"IPY_MODEL_6b83bc82d3fc4d748732d846cfb0dc35","value":"Batches: 100%"}},"83968cbbc4e449a8bcb23310d39d30df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61ad7cc8851c46eeb046cc3b1f4f9037","max":10638,"min":0,"orientation":"horizontal","style":"IPY_MODEL_551edcc5d73943368158d0b1b370c992","value":10638}},"9648bc57cf4348ebb33cd4cfc514ce04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89137fc8840748e5959d84eac9391677","placeholder":"​","style":"IPY_MODEL_7236259cbf69425cae07ae67f71a1737","value":" 10638/10638 [17:21&lt;00:00, 69.14it/s]"}},"3c2267d952c04278a25dcd7c68c534df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e015bfd6024fac9568d5d11d7162b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b83bc82d3fc4d748732d846cfb0dc35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61ad7cc8851c46eeb046cc3b1f4f9037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"551edcc5d73943368158d0b1b370c992":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89137fc8840748e5959d84eac9391677":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7236259cbf69425cae07ae67f71a1737":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"969a2a0fd7184fd884cb48dc36a5d5bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d5f1a52409d42a6b77487f532c4d47d","IPY_MODEL_e00a752be0544832911e407d95ce9de3","IPY_MODEL_4453e0f57ea240a885dec7fd5125c24f"],"layout":"IPY_MODEL_60f87a8f29ad48948f48202114e509e5"}},"8d5f1a52409d42a6b77487f532c4d47d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3453b7d26d204ccfa96e1c7bec145916","placeholder":"​","style":"IPY_MODEL_172d59ccbd284a78b2b5424715f9812e","value":"Batches: 100%"}},"e00a752be0544832911e407d95ce9de3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc86dcdeb9d14263a0ccaa492bfe4f33","max":10638,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cd8bd631ee848bcb0905a51967e0f93","value":10638}},"4453e0f57ea240a885dec7fd5125c24f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eb3e97fc9bf48b692fbeb47f03376c8","placeholder":"​","style":"IPY_MODEL_e4f107bed7544114905f6d222960347e","value":" 10638/10638 [02:42&lt;00:00, 157.45it/s]"}},"60f87a8f29ad48948f48202114e509e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3453b7d26d204ccfa96e1c7bec145916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"172d59ccbd284a78b2b5424715f9812e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc86dcdeb9d14263a0ccaa492bfe4f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cd8bd631ee848bcb0905a51967e0f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7eb3e97fc9bf48b692fbeb47f03376c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f107bed7544114905f6d222960347e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64d3d3b516f14aa19f71b5b9f8ad86e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b6667a1b8bd44f0a0576b83e0f87ba0","IPY_MODEL_73336f49a5184f3da1ad49e87d99b9a4","IPY_MODEL_84d410abc42c4467a22febd98d32be42"],"layout":"IPY_MODEL_ae6754e0c43d4bf89e4b356b086a0daf"}},"5b6667a1b8bd44f0a0576b83e0f87ba0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd81bddc0e664142a0f316c0a3b212cc","placeholder":"​","style":"IPY_MODEL_99e601e3cffc45c2a759a00d21bfe535","value":"Batches: 100%"}},"73336f49a5184f3da1ad49e87d99b9a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b73158391409451793cef89ba9a303d9","max":10638,"min":0,"orientation":"horizontal","style":"IPY_MODEL_478b62332cb640c78bd097133e606b84","value":10638}},"84d410abc42c4467a22febd98d32be42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d6152f5f1c49418cc16257b7fa2c88","placeholder":"​","style":"IPY_MODEL_de79bb319a58401d983d752a3c366595","value":" 10638/10638 [17:21&lt;00:00, 68.66it/s]"}},"ae6754e0c43d4bf89e4b356b086a0daf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd81bddc0e664142a0f316c0a3b212cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99e601e3cffc45c2a759a00d21bfe535":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b73158391409451793cef89ba9a303d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"478b62332cb640c78bd097133e606b84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86d6152f5f1c49418cc16257b7fa2c88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de79bb319a58401d983d752a3c366595":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# COPY TO RETRAIN ONLY ONE MODEL"],"metadata":{"id":"joKHoNIR9_kp"}},{"cell_type":"code","source":["# Install RAPIDS (if not already installed)\n","# Note: Run these commands in a separate cell before running the rest of the code.\n","\n","# !wget -nc https://raw.githubusercontent.com/rapidsai/rapidsai/main/utils/colab/rapids-colab.sh\n","# !bash rapids-colab.sh stable\n","\n","# Update environment variables\n","import sys\n","sys.path.append('/usr/local/lib/python3.10/site-packages')\n","\n","# Import necessary libraries\n","import os\n","import time\n","import logging\n","import pandas as pd\n","import numpy as np\n","import re\n","from datetime import datetime\n","import torch\n","import gc\n","from tqdm import tqdm\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Import cuML's UMAP and HDBSCAN\n","from cuml.manifold import UMAP  # GPU-accelerated UMAP\n","from cuml.cluster import HDBSCAN  # GPU-accelerated HDBSCAN\n","import cupy as cp  # For GPU arrays\n","\n","def load_dataset(path, chunksize=None):\n","    \"\"\"\n","    Load and preprocess the dataset.\n","\n","    Args:\n","        path (str): Path to the dataset CSV file.\n","        chunksize (int, optional): If specified, read the CSV in chunks of this size.\n","\n","    Returns:\n","        list: A list of preprocessed sentences.\n","    \"\"\"\n","    print(\"Loading and preprocessing dataset...\")\n","    start_time = time.time()\n","\n","    if chunksize:\n","        # Read the CSV file in chunks if chunksize is specified\n","        df_iter = pd.read_csv(path, chunksize=chunksize)\n","        df = pd.concat([chunk for chunk in df_iter])\n","    else:\n","        # Read the entire CSV file\n","        df = pd.read_csv(path)\n","\n","    # Preprocess the sentences\n","    df['Sentence'] = df['Sentence'].str.replace(r'\\n+', ' ', regex=True)\n","    df['Sentence'] = df['Sentence'].str.replace(r'\\s+', ' ', regex=True).str.strip().str.lower()\n","\n","    # Convert the 'Sentence' column to a list\n","    corpus = df['Sentence'].tolist()\n","    print(f\"Dataset loaded. Total sentences: {len(corpus)}\")\n","    print(f\"Time taken for loading and preprocessing: {time.time() - start_time:.2f} seconds\")\n","    return corpus\n","\n","def create_dataframe():\n","    \"\"\"\n","    Create the dataframe containing the top 10 models based on the updated combined score.\n","\n","    Returns:\n","        pd.DataFrame: The dataframe with model parameters.\n","    \"\"\"\n","    print(\"Creating dataframe with model parameters...\")\n","    # Define the data as a dictionary\n","    data = {\n","        'Embeddings_Model': [\n","            'all-MiniLM-L12-v2',\n","            'paraphrase-mpnet-base-v2',\n","            'all-MiniLM-L12-v2',\n","            'paraphrase-mpnet-base-v2',\n","            'paraphrase-MiniLM-L6-v2',\n","            'paraphrase-mpnet-base-v2',\n","            'multi-qa-mpnet-base-cos-v1',\n","            'all-MiniLM-L12-v2',\n","            'multi-qa-mpnet-base-cos-v1',\n","            'multi-qa-mpnet-base-cos-v1'\n","        ],\n","        'Iteration': [66, 14, 75, 0, 19, 13, 23, 67, 28, 11],\n","        'Coherence': [0.577551, 0.469187, 0.543105, 0.463245, 0.425237, 0.452912, 0.430575, 0.489419, 0.439447, 0.419208],\n","        'Topic_Diversity': [0.45, 0.8, 0.466667, 0.82, 0.94, 0.8, 0.797059, 0.527273, 0.749474, 0.828571],\n","        'bertopic__min_topic_size': [102, 63, 142, 127, 64, 14, 28, 99, 29, 105],\n","        'bertopic__top_n_words': [30, 22, 10, 31, 27, 18, 28, 24, 14, 24],\n","        'hdbscan__min_cluster_size': [281, 500, 473, 494, 143, 497, 492, 258, 427, 497],\n","        'hdbscan__min_samples': [72, 72, 14, 28, 32, 32, 12, 37, 11, 13],\n","        'umap__min_dist': [0.005022, 0.077818, 0.004634, 0.058341, 0.085702, 0.086975, 0.095922, 0.004852, 0.008103, 0.022149],\n","        'umap__n_components': [2, 9, 5, 10, 9, 8, 9, 7, 9, 8],\n","        'umap__n_neighbors': [7, 11, 15, 11, 44, 9, 19, 42, 18, 14],\n","        'vectorizer__min_df': [0.001504, 0.009372, 0.001947, 0.007313, 0.005932, 0.009857, 0.008294, 0.001174, 0.005862, 0.009229],\n","    }\n","\n","    # Create a DataFrame from the data dictionary\n","    df = pd.DataFrame(data)\n","    print(\"Dataframe created with model parameters.\")\n","    return df\n","\n","def load_embedding_models(model_names):\n","    \"\"\"\n","    Load all unique embedding models.\n","\n","    Args:\n","        model_names (list): List of embedding model names to load.\n","\n","    Returns:\n","        dict: A dictionary mapping model names to loaded embedding models.\n","    \"\"\"\n","    embedding_models = {}\n","    # Use GPU if available\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using device: {device}\")\n","\n","    for model_name in model_names:\n","        print(f\"Loading embedding model: {model_name}\")\n","        try:\n","            # Load the embedding model onto the specified device\n","            embedding_model = SentenceTransformer(model_name, device=device)\n","            embedding_models[model_name] = embedding_model\n","            print(f\"Model {model_name} loaded successfully.\")\n","        except Exception as e:\n","            print(f\"Failed to load embedding model {model_name}: {e}\")\n","    print(\"All embedding models loaded.\")\n","    return embedding_models\n","\n","def compute_embeddings(corpus, embedding_models):\n","    \"\"\"\n","    Compute embeddings for each embedding model.\n","\n","    Args:\n","        corpus (list): List of preprocessed sentences.\n","        embedding_models (dict): Dictionary of embedding models.\n","\n","    Returns:\n","        dict: A dictionary mapping model names to their computed embeddings.\n","    \"\"\"\n","    precalculated_embeddings = {}\n","    for model_name, embedding_model in embedding_models.items():\n","        print(f\"Computing embeddings for model: {model_name}\")\n","        start_time = time.time()\n","        try:\n","            # Compute embeddings using the embedding model\n","            embeddings = embedding_model.encode(corpus, show_progress_bar=True, batch_size=64)\n","            # Keep embeddings as NumPy arrays\n","            precalculated_embeddings[model_name] = embeddings\n","            print(f\"Embeddings computed for model: {model_name} in {time.time() - start_time:.2f} seconds.\")\n","        except Exception as e:\n","            print(f\"Failed to compute embeddings for model {model_name}: {e}\")\n","    print(\"All embeddings computed.\")\n","    return precalculated_embeddings\n","\n","def train_and_save_models(corpus, params_df, embedding_models, precalculated_embeddings):\n","    \"\"\"\n","    Train and save BERTopic models based on parameters.\n","\n","    Args:\n","        corpus (list): List of preprocessed sentences.\n","        params_df (pd.DataFrame): DataFrame containing model parameters.\n","        embedding_models (dict): Dictionary of embedding models.\n","        precalculated_embeddings (dict): Dictionary of embeddings.\n","    \"\"\"\n","    for idx, row in tqdm(params_df.iterrows(), total=params_df.shape[0]):\n","        iteration = row['Iteration']\n","        embedding_model_name = row['Embeddings_Model']\n","        print(f\"\\nStarting training for model {idx+1}/{len(params_df)} with embedding: {embedding_model_name}\")\n","\n","        # Check if embeddings are available\n","        if embedding_model_name not in precalculated_embeddings:\n","            print(f\"Embeddings for model {embedding_model_name} not available. Skipping this model.\")\n","            continue\n","\n","        # Retrieve embeddings\n","        embeddings = precalculated_embeddings[embedding_model_name]\n","\n","        # Convert embeddings to CuPy array for cuML processing\n","        embeddings_cupy = cp.asarray(embeddings)\n","\n","        # Parameter validation and conversion\n","        try:\n","            umap_n_neighbors = int(row['umap__n_neighbors'])\n","            umap_n_components = int(row['umap__n_components'])\n","            umap_min_dist = float(row['umap__min_dist'])\n","            hdbscan_min_cluster_size = int(row['hdbscan__min_cluster_size'])\n","            hdbscan_min_samples = int(row['hdbscan__min_samples'])\n","            vectorizer_min_df = float(row['vectorizer__min_df'])\n","            bertopic_top_n_words = int(row['bertopic__top_n_words'])\n","            bertopic_min_topic_size = int(row['bertopic__min_topic_size'])\n","        except ValueError as e:\n","            print(f\"Parameter conversion error: {e}\")\n","            continue\n","\n","        # Initialize cuML's UMAP model\n","        print(\"Initializing UMAP model...\")\n","        umap_model = UMAP(\n","            n_neighbors=umap_n_neighbors,\n","            n_components=umap_n_components,\n","            min_dist=umap_min_dist,\n","            metric='cosine',\n","            random_state=42\n","        )\n","\n","        # Initialize cuML's HDBSCAN model\n","        print(\"Initializing HDBSCAN model...\")\n","        hdbscan_model = HDBSCAN(\n","            min_cluster_size=hdbscan_min_cluster_size,\n","            min_samples=hdbscan_min_samples,\n","            cluster_selection_method='eom',\n","            prediction_data=True,\n","            gen_min_span_tree=True\n","        )\n","\n","        # Initialize CountVectorizer (CPU-based)\n","        print(\"Initializing CountVectorizer...\")\n","        vectorizer_model = CountVectorizer(\n","            stop_words='english',\n","            min_df=vectorizer_min_df\n","        )\n","\n","        # Initialize BERTopic model\n","        print(\"Initializing BERTopic model...\")\n","        topic_model = BERTopic(\n","            embedding_model=None,  # We provide embeddings directly\n","            umap_model=umap_model,\n","            hdbscan_model=hdbscan_model,\n","            vectorizer_model=vectorizer_model,\n","            top_n_words=bertopic_top_n_words,\n","            min_topic_size=bertopic_min_topic_size,\n","            language='english',\n","            calculate_probabilities=True,\n","            verbose=True\n","        )\n","\n","        # Train BERTopic model\n","        print(\"Training BERTopic model...\")\n","        start_train_time = time.time()\n","        try:\n","            # Fit the model with embeddings and corpus\n","            topics, probs = topic_model.fit_transform(corpus, embeddings)\n","            print(f\"Model training completed in {time.time() - start_train_time:.2f} seconds.\")\n","        except Exception as e:\n","            print(f\"Error during model training: {e}\")\n","            continue\n","\n","        # Save the model\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        model_filename = f\"bertopic_model_{idx}_iter_{iteration}_{timestamp}.pkl\"\n","        model_path = os.path.join(\"/content/drive/MyDrive/\", model_filename)\n","        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n","        try:\n","            topic_model.save(model_path)\n","            print(f\"Model saved at: {model_path}\")\n","        except Exception as e:\n","            print(f\"Failed to save model: {e}\")\n","            continue\n","\n","        # Cleanup\n","        del topic_model\n","        gc.collect()\n","\n","    print(\"All models trained and saved.\")\n","\n","def main():\n","    \"\"\"\n","    Main function to orchestrate data loading, model training, and saving.\n","    \"\"\"\n","    # Path to the dataset CSV file\n","    dataset_path = '/content/drive/MyDrive/processed_novels_sentences_new_romantic.gsheet'\n","\n","    # Load and preprocess the dataset\n","    corpus = load_dataset(dataset_path)\n","\n","    # Create the dataframe containing model parameters\n","    params_df = create_dataframe()\n","\n","    # Extract unique embedding model names from the dataframe\n","    embedding_model_names = params_df['Embeddings_Model'].unique()\n","    print(f\"Unique embedding models to load: {embedding_model_names}\")\n","\n","    # Load the embedding models\n","    embedding_models = load_embedding_models(embedding_model_names)\n","\n","    # Compute embeddings for each embedding model\n","    precalculated_embeddings = compute_embeddings(corpus, embedding_models)\n","\n","    # Train and save BERTopic models based on parameters and embeddings\n","    train_and_save_models(corpus, params_df, embedding_models, precalculated_embeddings)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c3a274f2610e41b8a1f3238af8dbec31","6a41e6fbe92148719c89958ce5110f56","873ab9b01bc04ef880c1130114f6b9b1","bdcc119db5434afd8bc4d0e8ddd1db01","b694a9c27ded451a890ba680d0f52a48","3c247ad1c2004ad3ad4386456f773a85","46507baf47e24bf4a95c67ae56d7c0d5","164bbe0b6679426a800219f2ec4d96db","264fa02e95b646ce88b73fb7c256cfde","82bc5fb123af4ecaa14cf23351dfb467","b366e7a42d9c4abeb23d6b1cf4f3eb6b","01470135a1864a999daff2ccfad59e8f","14223c566e2c442084e7b08055519314","83968cbbc4e449a8bcb23310d39d30df","9648bc57cf4348ebb33cd4cfc514ce04","3c2267d952c04278a25dcd7c68c534df","26e015bfd6024fac9568d5d11d7162b8","6b83bc82d3fc4d748732d846cfb0dc35","61ad7cc8851c46eeb046cc3b1f4f9037","551edcc5d73943368158d0b1b370c992","89137fc8840748e5959d84eac9391677","7236259cbf69425cae07ae67f71a1737","969a2a0fd7184fd884cb48dc36a5d5bc","8d5f1a52409d42a6b77487f532c4d47d","e00a752be0544832911e407d95ce9de3","4453e0f57ea240a885dec7fd5125c24f","60f87a8f29ad48948f48202114e509e5","3453b7d26d204ccfa96e1c7bec145916","172d59ccbd284a78b2b5424715f9812e","bc86dcdeb9d14263a0ccaa492bfe4f33","4cd8bd631ee848bcb0905a51967e0f93","7eb3e97fc9bf48b692fbeb47f03376c8","e4f107bed7544114905f6d222960347e","64d3d3b516f14aa19f71b5b9f8ad86e4","5b6667a1b8bd44f0a0576b83e0f87ba0","73336f49a5184f3da1ad49e87d99b9a4","84d410abc42c4467a22febd98d32be42","ae6754e0c43d4bf89e4b356b086a0daf","bd81bddc0e664142a0f316c0a3b212cc","99e601e3cffc45c2a759a00d21bfe535","b73158391409451793cef89ba9a303d9","478b62332cb640c78bd097133e606b84","86d6152f5f1c49418cc16257b7fa2c88","de79bb319a58401d983d752a3c366595"]},"executionInfo":{"status":"ok","timestamp":1730507759919,"user_tz":-60,"elapsed":7897810,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"a8209cfe-0ee4-44ea-fd65-5dbad69278bd","id":"YXyHgn0Q-JOu"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing dataset...\n","Dataset loaded. Total sentences: 680822\n","Time taken for loading and preprocessing: 6.08 seconds\n","Creating dataframe with model parameters...\n","Dataframe created with model parameters.\n","Unique embedding models to load: ['all-MiniLM-L12-v2' 'paraphrase-mpnet-base-v2' 'paraphrase-MiniLM-L6-v2'\n"," 'multi-qa-mpnet-base-cos-v1']\n","Using device: cuda\n","Loading embedding model: all-MiniLM-L12-v2\n","Model all-MiniLM-L12-v2 loaded successfully.\n","Loading embedding model: paraphrase-mpnet-base-v2\n","Model paraphrase-mpnet-base-v2 loaded successfully.\n","Loading embedding model: paraphrase-MiniLM-L6-v2\n","Model paraphrase-MiniLM-L6-v2 loaded successfully.\n","Loading embedding model: multi-qa-mpnet-base-cos-v1\n","Model multi-qa-mpnet-base-cos-v1 loaded successfully.\n","All embedding models loaded.\n","Computing embeddings for model: all-MiniLM-L12-v2\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/10638 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a274f2610e41b8a1f3238af8dbec31"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Embeddings computed for model: all-MiniLM-L12-v2 in 311.89 seconds.\n","Computing embeddings for model: paraphrase-mpnet-base-v2\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/10638 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01470135a1864a999daff2ccfad59e8f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Embeddings computed for model: paraphrase-mpnet-base-v2 in 1052.22 seconds.\n","Computing embeddings for model: paraphrase-MiniLM-L6-v2\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/10638 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"969a2a0fd7184fd884cb48dc36a5d5bc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Embeddings computed for model: paraphrase-MiniLM-L6-v2 in 172.57 seconds.\n","Computing embeddings for model: multi-qa-mpnet-base-cos-v1\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/10638 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d3d3b516f14aa19f71b5b9f8ad86e4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Embeddings computed for model: multi-qa-mpnet-base-cos-v1 in 1051.52 seconds.\n","All embeddings computed.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting training for model 1/10 with embedding: all-MiniLM-L12-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:07:42,116 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:12:09,409 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:12:09,416 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:14:12,498 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:14:12,601 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:14:19,538 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:14:19,796 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 397.93 seconds.\n","Model saved at: /content/drive/MyDrive/bertopic_model_0_iter_66_20241101_231419.pkl\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 1/10 [06:43<1:00:30, 403.42s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting training for model 2/10 with embedding: paraphrase-mpnet-base-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:14:26,433 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:22:09,869 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:22:09,895 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:24:18,739 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:24:18,840 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:24:26,162 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:24:26,332 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 600.14 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [16:54<1:10:06, 525.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_1_iter_14_20241101_232426.pkl\n","\n","Starting training for model 3/10 with embedding: all-MiniLM-L12-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:24:36,745 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:29:04,222 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:29:04,237 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:31:08,316 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:31:08,415 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:31:15,541 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:31:15,712 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 399.22 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [23:41<54:59, 471.33s/it]  "]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_2_iter_75_20241101_233115.pkl\n","\n","Starting training for model 4/10 with embedding: paraphrase-mpnet-base-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:31:23,449 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:39:07,819 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:39:07,848 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:41:10,287 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:41:10,393 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:41:17,902 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:41:20,821 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 597.62 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [33:52<52:39, 526.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_3_iter_0_20241101_234120.pkl\n","\n","Starting training for model 5/10 with embedding: paraphrase-MiniLM-L6-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:41:34,592 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:46:20,808 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:46:20,834 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:48:25,097 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:48:25,197 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:48:32,481 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:48:32,959 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 418.61 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [41:01<40:56, 491.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_4_iter_19_20241101_234832.pkl\n","\n","Starting training for model 6/10 with embedding: paraphrase-mpnet-base-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:48:43,529 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-01 23:56:23,572 - BERTopic - Dimensionality - Completed ✓\n","2024-11-01 23:56:23,593 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-01 23:58:27,494 - BERTopic - Cluster - Completed ✓\n","2024-11-01 23:58:27,602 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-01 23:58:34,868 - BERTopic - Representation - Completed ✓\n","2024-11-01 23:58:37,821 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 594.54 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [51:06<35:19, 529.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_5_iter_13_20241101_235837.pkl\n","\n","Starting training for model 7/10 with embedding: multi-qa-mpnet-base-cos-v1\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-01 23:58:48,349 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-02 00:06:33,204 - BERTopic - Dimensionality - Completed ✓\n","2024-11-02 00:06:33,229 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-02 00:08:32,331 - BERTopic - Cluster - Completed ✓\n","2024-11-02 00:08:32,433 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-02 00:08:40,004 - BERTopic - Representation - Completed ✓\n","2024-11-02 00:08:42,393 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 594.29 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [1:01:11<27:43, 554.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_6_iter_23_20241102_000842.pkl\n","\n","Starting training for model 8/10 with embedding: all-MiniLM-L12-v2\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-02 00:08:53,584 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-02 00:13:32,237 - BERTopic - Dimensionality - Completed ✓\n","2024-11-02 00:13:32,257 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-02 00:15:39,695 - BERTopic - Cluster - Completed ✓\n","2024-11-02 00:15:39,797 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-02 00:15:46,925 - BERTopic - Representation - Completed ✓\n","2024-11-02 00:15:47,189 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 413.85 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [1:08:12<17:03, 511.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_7_iter_67_20241102_001547.pkl\n","\n","Starting training for model 9/10 with embedding: multi-qa-mpnet-base-cos-v1\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-02 00:15:54,135 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-02 00:23:38,593 - BERTopic - Dimensionality - Completed ✓\n","2024-11-02 00:23:38,619 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-02 00:25:38,021 - BERTopic - Cluster - Completed ✓\n","2024-11-02 00:25:38,133 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-02 00:25:45,980 - BERTopic - Representation - Completed ✓\n","2024-11-02 00:25:48,028 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 594.15 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [1:18:17<09:01, 541.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_8_iter_28_20241102_002548.pkl\n","\n","Starting training for model 10/10 with embedding: multi-qa-mpnet-base-cos-v1\n","Initializing UMAP model...\n","Initializing HDBSCAN model...\n","Initializing CountVectorizer...\n","Initializing BERTopic model...\n","Training BERTopic model...\n"]},{"output_type":"stream","name":"stderr","text":["2024-11-02 00:25:59,784 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2024-11-02 00:33:40,110 - BERTopic - Dimensionality - Completed ✓\n","2024-11-02 00:33:40,131 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2024-11-02 00:35:40,219 - BERTopic - Cluster - Completed ✓\n","2024-11-02 00:35:40,323 - BERTopic - Representation - Extracting topics from clusters using representation models.\n","2024-11-02 00:35:48,109 - BERTopic - Representation - Completed ✓\n","2024-11-02 00:35:49,677 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"]},{"output_type":"stream","name":"stdout","text":["Model training completed in 590.14 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [1:28:18<00:00, 529.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Model saved at: /content/drive/MyDrive/bertopic_model_9_iter_11_20241102_003549.pkl\n","All models trained and saved.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# COPY TO RETRAIN WITH POS"],"metadata":{"id":"oIz_ngNK-Pxz"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FEYqj3zm1TIn","executionInfo":{"status":"ok","timestamp":1761945323516,"user_tz":-60,"elapsed":1204,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"e6d04b9f-c6af-4926-9f83-bfd922ff7542"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# ==============================\n","# 1. Setup and Installation\n","# ==============================\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Check CUDA and driver versions\n","!nvcc --version  # Check CUDA version\n","!nvidia-smi      # Check driver version\n","\n","# Install RAPIDS and other required libraries\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/pip-install.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"9OivEorQmNym","executionInfo":{"status":"ok","timestamp":1761945380861,"user_tz":-60,"elapsed":1626,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"4b4bded4-0413-4e74-fedb-2edf06cfcf52"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n","Fri Oct 31 21:16:20 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n","Installing RAPIDS remaining 25.08 libraries\n","Using Python 3.12.12 environment at: /usr\n","Audited 9 packages in 100ms\n","\n","        ***********************************************************************\n","        The pip install of RAPIDS is complete.\n","\n","        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n","\n","        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n","\n","        Troubleshooting:\n","            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n","            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n","        ***********************************************************************\n","        \n"]}]},{"cell_type":"code","source":["## After restarting, install remaining necessary libraries\n","# Run this cell after restarting the runtime\n","!pip install bertopic==0.16.3\n","!pip install octis\n","!pip install sentence-transformers\n","!pip install umap-learn==0.5.3  # Specify a compatible version\n","!pip install hdbscan\n","!pip install tqdm\n","!pip install pandas\n","!pip install gensim\n","!pip install wandb\n","!pip install umap\n","!pip install scipy\n","!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oco0mYO-p5BY","outputId":"d43cb7e9-c20c-4b1b-b09b-6e7d50e23f40","collapsed":true,"executionInfo":{"status":"ok","timestamp":1761945471112,"user_tz":-60,"elapsed":71071,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bertopic==0.16.3\n","  Downloading bertopic-0.16.3-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (0.8.40)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (2.0.2)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (5.24.1)\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (1.6.1)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (5.1.2)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (4.67.1)\n","Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from bertopic==0.16.3) (0.5.9.post2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic==0.16.3) (1.16.3)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic==0.16.3) (1.5.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic==0.16.3) (2025.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic==0.16.3) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic==0.16.3) (25.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2.post1->bertopic==0.16.3) (3.6.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (4.57.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (2.8.0+cu126)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (0.36.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.16.3) (4.15.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic==0.16.3) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic==0.16.3) (0.5.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic==0.16.3) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic==0.16.3) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (0.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic==0.16.3) (2025.10.5)\n","Downloading bertopic-0.16.3-py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bertopic\n","Successfully installed bertopic-0.16.3\n","Collecting octis\n","  Using cached octis-1.14.0-py2.py3-none-any.whl.metadata (27 kB)\n","Collecting gensim<5.0,>=4.2.0 (from octis)\n","  Using cached gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from octis) (3.9.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from octis) (2.2.2)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from octis) (3.8.7)\n","Collecting scikit-learn==1.1.0 (from octis)\n","  Using cached scikit-learn-1.1.0.tar.gz (6.8 MB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n","Collecting umap-learn==0.5.3\n","  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (2.0.2)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (1.6.1)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (1.16.3)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (0.5.13)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from umap-learn==0.5.3) (4.67.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.49->umap-learn==0.5.3) (0.43.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from pynndescent>=0.5->umap-learn==0.5.3) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22->umap-learn==0.5.3) (3.6.0)\n","Building wheels for collected packages: umap-learn\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82810 sha256=321b6e389ab13ae73a32a83268b26c5c2e834a014857f9120f8345a4374f07af\n","  Stored in directory: /root/.cache/pip/wheels/6b/a6/5c/466a36f7e4073169aa2ed85b749b1f56c0da07b0c86d0f7750\n","Successfully built umap-learn\n","Installing collected packages: umap-learn\n","  Attempting uninstall: umap-learn\n","    Found existing installation: umap-learn 0.5.9.post2\n","    Uninstalling umap-learn-0.5.9.post2:\n","      Successfully uninstalled umap-learn-0.5.9.post2\n","Successfully installed umap-learn-0.5.3\n","Requirement already satisfied: hdbscan in /usr/local/lib/python3.12/dist-packages (0.8.40)\n","Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.12/dist-packages (from hdbscan) (2.0.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan) (1.16.3)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.12/dist-packages (from hdbscan) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Collecting gensim\n","  Using cached gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n","Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gensim\n","Successfully installed gensim-4.4.0\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Collecting umap\n","  Using cached umap-0.1.1.tar.gz (3.2 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: umap\n","  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3541 sha256=86355f47c9130a7602f3ba61ac470dd36431955ef2668d05316197caa4f40855\n","  Stored in directory: /root/.cache/pip/wheels/48/4a/1c/1d511cbb0413a448d8546e958f8e82b98d9bb493038d19ece2\n","Successfully built umap\n","Installing collected packages: umap\n","Successfully installed umap-0.1.1\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n","Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"code","source":["!pip list | grep umap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfgbEMf7F6Ij","executionInfo":{"status":"ok","timestamp":1761945642812,"user_tz":-60,"elapsed":1502,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"7baaeaad-71e7-484f-b303-4571183db4d0","collapsed":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["umap                                     0.1.1\n","umap-learn                               0.5.3\n"]}]},{"cell_type":"code","source":["!pip uninstall -y umap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S93BrsZVF_Dv","executionInfo":{"status":"ok","timestamp":1761945643560,"user_tz":-60,"elapsed":712,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"cb8aa734-bdbb-4c1a-db28-98da08881420","collapsed":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: umap 0.1.1\n","Uninstalling umap-0.1.1:\n","  Successfully uninstalled umap-0.1.1\n"]}]},{"cell_type":"code","source":["!find . -type d -name \"__pycache__\" -exec rm -r {} +"],"metadata":{"id":"-aDvGu_mGBU5","executionInfo":{"status":"ok","timestamp":1761945723971,"user_tz":-60,"elapsed":65665,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade bertopic umap-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"jszgh5trGJuL","executionInfo":{"status":"ok","timestamp":1761945729490,"user_tz":-60,"elapsed":5515,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"ad56a369-aaa4-4404-d969-c881da7e72d2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bertopic in /usr/local/lib/python3.12/dist-packages (0.16.3)\n","Collecting bertopic\n","  Using cached bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (0.5.3)\n","Collecting umap-learn\n","  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.8.40)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.0.2)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.24.1)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (1.6.1)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.1.2)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (4.67.1)\n","Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.43.0)\n","Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (1.16.3)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.5.13)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.57.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.8.0+cu126)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.36.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.10.5)\n","Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: umap-learn, bertopic\n","  Attempting uninstall: umap-learn\n","    Found existing installation: umap-learn 0.5.3\n","    Uninstalling umap-learn-0.5.3:\n","      Successfully uninstalled umap-learn-0.5.3\n","  Attempting uninstall: bertopic\n","    Found existing installation: bertopic 0.16.3\n","    Uninstalling bertopic-0.16.3:\n","      Successfully uninstalled bertopic-0.16.3\n","Successfully installed bertopic-0.17.3 umap-learn-0.5.9.post2\n"]}]},{"cell_type":"code","source":["# path: scripts/train_bertopic_grid.py\n","\"\"\"\n","BERTopic grid training:\n","- c_npmi coherence + per-topic breakdown\n","- GPU/CPU fallback for UMAP/HDBSCAN\n","- Embedding caching per model\n","- Fixed PartOfSpeech construction for older/newer BERTopic (no embedding_model arg)\n","\"\"\"\n","\n","from __future__ import annotations\n","\n","import gc\n","import json\n","import logging\n","import math\n","import os\n","import re\n","import time\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Dict, List, Optional, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n","from tqdm import tqdm\n","\n","from bertopic import BERTopic\n","from bertopic.representation import KeyBERTInspired, PartOfSpeech\n","\n","# Optional coherence (auto-disabled if missing)\n","try:\n","    from gensim.corpora import Dictionary\n","    from gensim.models.coherencemodel import CoherenceModel\n","    HAS_GENSIM = True\n","except Exception:\n","    HAS_GENSIM = False\n","\n","import spacy\n","from spacy.cli import download as spacy_download\n","\n","\n","# -----------------------------\n","# 1) Configuration\n","# -----------------------------\n","@dataclass\n","class Config:\n","    dataset_csv: Path\n","    text_column: str = \"Sentence\"\n","    output_dir: Path = Path(\"./bertopic_runs\")\n","    custom_stopwords_file: Optional[Path] = None\n","    test_mode: bool = False\n","    sample_size: int = 10000\n","    min_doc_tokens: int = 3\n","    seed: int = 42\n","\n","    # scoring weights\n","    w_diversity: float = 0.5\n","    w_coherence: float = 0.5  # applied to c_npmi if available\n","    penalty_outliers: float = 0.25\n","    target_topics: Optional[Tuple[int, int]] = None\n","\n","    embeddings_dir: Path = Path(\"./bertopic_runs/embeddings\")\n","    pos_configs: Tuple[str, ...] = (\"a\", \"b\", \"c\")\n","    param_cols: Tuple[str, ...] = (\n","        \"Embeddings_Model\",\n","        \"Iteration\",\n","        \"bertopic__min_topic_size\",\n","        \"bertopic__top_n_words\",\n","        \"hdbscan__min_cluster_size\",\n","        \"hdbscan__min_samples\",\n","        \"umap__min_dist\",\n","        \"umap__n_components\",\n","        \"umap__n_neighbors\",\n","        \"vectorizer__min_df\",\n","    )\n","\n","\n","# -----------------------------\n","# 2) Logging\n","# -----------------------------\n","def setup_logging(out_dir: Path) -> logging.Logger:\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    log_file = out_dir / \"train.log\"\n","\n","    logger = logging.getLogger(\"bertopic_grid\")\n","    logger.setLevel(logging.INFO)\n","\n","    if logger.handlers:\n","        for h in list(logger.handlers):\n","            logger.removeHandler(h)\n","\n","    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n","\n","    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n","    fh.setLevel(logging.DEBUG)\n","    fh.setFormatter(fmt)\n","    logger.addHandler(fh)\n","\n","    ch = logging.StreamHandler()\n","    ch.setLevel(logging.INFO)\n","    ch.setFormatter(fmt)\n","    logger.addHandler(ch)\n","\n","    return logger\n","\n","\n","# -----------------------------\n","# 3) CPU/GPU fallback\n","# -----------------------------\n","def make_umap_hdbscan(\n","    n_neighbors: int,\n","    n_components: int,\n","    min_dist: float,\n","    hdb_min_cluster_size: int,\n","    hdb_min_samples: int,\n","    seed: int,\n","    logger: logging.Logger,\n","):\n","    try:\n","        from cuml.manifold import UMAP as GPU_UMAP  # type: ignore\n","        from cuml.cluster import HDBSCAN as GPU_HDBSCAN  # type: ignore\n","\n","        logger.info(\"Using GPU (cuML) UMAP + HDBSCAN\")\n","        umap_model = GPU_UMAP(\n","            n_neighbors=n_neighbors,\n","            n_components=n_components,\n","            min_dist=min_dist,\n","            metric=\"cosine\",\n","            random_state=seed,\n","        )\n","        hdbscan_model = GPU_HDBSCAN(\n","            min_cluster_size=hdb_min_cluster_size,\n","            min_samples=hdb_min_samples,\n","            cluster_selection_method=\"eom\",\n","            prediction_data=True,\n","            gen_min_span_tree=True,\n","        )\n","        return umap_model, hdbscan_model, True\n","    except Exception:\n","        from umap import UMAP as CPU_UMAP  # type: ignore\n","        import hdbscan as CPU_HDBSCAN  # type: ignore\n","\n","        logger.info(\"Using CPU UMAP + HDBSCAN (fallback)\")\n","        umap_model = CPU_UMAP(\n","            n_neighbors=n_neighbors,\n","            n_components=n_components,\n","            min_dist=min_dist,\n","            metric=\"cosine\",\n","            random_state=seed,\n","        )\n","        hdbscan_model = CPU_HDBSCAN.HDBSCAN(\n","            min_cluster_size=hdb_min_cluster_size,\n","            min_samples=hdb_min_samples,\n","            cluster_selection_method=\"eom\",\n","            prediction_data=True,\n","            gen_min_span_tree=True,\n","        )\n","        return umap_model, hdbscan_model, False\n","\n","\n","# -----------------------------\n","# 4) Stopwords\n","# -----------------------------\n","def load_stopwords(custom_file: Optional[Path]) -> set:\n","    stops = set(w.lower() for w in ENGLISH_STOP_WORDS)\n","    if custom_file and custom_file.exists():\n","        punct = re.compile(r\"[^\\w\\s]\")\n","        with custom_file.open(\"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                line = punct.sub(\"\", line.strip().lower())\n","                for token in line.split():\n","                    if token:\n","                        stops.add(token)\n","    return stops\n","\n","\n","# -----------------------------\n","# 5) Data loading & cleaning\n","# -----------------------------\n","def simple_clean(text: str) -> str:\n","    text = text.lower()\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n","    return \" \".join(tok for tok in text.split() if tok)\n","\n","\n","def load_corpus(cfg: Config, logger: logging.Logger, stopwords: set) -> List[str]:\n","    df = pd.read_csv(cfg.dataset_csv)\n","    if cfg.text_column not in df.columns:\n","        raise ValueError(f\"Missing column '{cfg.text_column}' in dataset\")\n","\n","    docs = [simple_clean(str(x)) for x in df[cfg.text_column].tolist()]\n","    docs = [\" \".join(tok for tok in d.split() if tok not in stopwords) for d in docs]\n","    docs = [d for d in docs if len(d.split()) >= cfg.min_doc_tokens]\n","\n","    if cfg.test_mode and len(docs) > cfg.sample_size:\n","        rng = np.random.default_rng(cfg.seed)\n","        idx = rng.choice(len(docs), size=cfg.sample_size, replace=False)\n","        docs = [docs[i] for i in idx]\n","\n","    logger.info(f\"Loaded {len(docs):,} cleaned documents\")\n","    return docs\n","\n","\n","# -----------------------------\n","# 6) spaCy (for POS repr)\n","# -----------------------------\n","def ensure_spacy(logger: logging.Logger):\n","    try:\n","        _ = spacy.load(\"en_core_web_sm\")\n","    except OSError:\n","        logger.info(\"Downloading spaCy model 'en_core_web_sm'...\")\n","        spacy_download(\"en_core_web_sm\")\n","\n","\n","# -----------------------------\n","# 7) Embeddings cache\n","# -----------------------------\n","def get_embeddings(\n","    model_name: str, docs: List[str], out_dir: Path, device: str, logger: logging.Logger\n",") -> np.ndarray:\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    key = model_name.replace(\"/\", \"_\")\n","    fpath = out_dir / f\"{key}.npy\"\n","\n","    if fpath.exists():\n","        logger.info(f\"Loading cached embeddings: {fpath}\")\n","        return np.load(fpath)\n","\n","    logger.info(f\"Encoding embeddings: {model_name} on {device}\")\n","    model = SentenceTransformer(model_name, device=device)\n","    embs = model.encode(docs, show_progress_bar=True, batch_size=64)\n","    np.save(fpath, embs)\n","    logger.info(f\"Saved embeddings: {fpath}\")\n","    return embs\n","\n","\n","# -----------------------------\n","# 8) Representation models (POS) — fixed\n","# -----------------------------\n","def build_representation(pos_cfg: str, logger: logging.Logger):\n","    \"\"\"Return a dict of representation models. No embedding_model passed to PartOfSpeech (compat).\"\"\"\n","    main = KeyBERTInspired(top_n_words=30)\n","\n","    # Older/newer BERTopic: PartOfSpeech takes only pos_patterns (+ optional spacy pipeline args)\n","    try:\n","        pos_map = {\n","            \"a\": {\"NOUNS\": PartOfSpeech(pos_patterns=[[{\"POS\": \"NOUN\"}]])},\n","            \"b\": {\n","                \"NOUNS\": PartOfSpeech(pos_patterns=[[{\"POS\": \"NOUN\"}]]),\n","                \"ADJECTIVES\": PartOfSpeech(pos_patterns=[[{\"POS\": \"ADJ\"}]]),\n","            },\n","            \"c\": {\n","                \"NOUNS\": PartOfSpeech(pos_patterns=[[{\"POS\": \"NOUN\"}]]),\n","                \"VERBS\": PartOfSpeech(pos_patterns=[[{\"POS\": \"VERB\"}]]),\n","            },\n","        }\n","        if pos_cfg not in pos_map:\n","            raise ValueError(f\"Invalid POS configuration: {pos_cfg}\")\n","        rep = {\"Main\": main}\n","        rep.update(pos_map[pos_cfg])\n","        return rep\n","    except Exception as e:\n","        # Why: ensure training continues even if POS component mismatches current BERTopic version\n","        logger.warning(f\"PartOfSpeech unavailable ({e}); falling back to KeyBERTInspired only.\")\n","        return {\"Main\": main}\n","\n","\n","# -----------------------------\n","# 9) Metrics: diversity + c_npmi\n","# -----------------------------\n","def topic_diversity(topics_dict: Dict[int, List[Tuple[str, float]]], top_k: int = 10) -> float:\n","    words: List[str] = []\n","    for k, items in topics_dict.items():\n","        if k == -1:\n","            continue\n","        words.extend([w for (w, _) in items[:top_k]])\n","    if not words:\n","        return 0.0\n","    return len(set(words)) / float(len(words))\n","\n","\n","def compute_coherence_npmi(\n","    docs: List[str],\n","    topics_dict: Dict[int, List[Tuple[str, float]]],\n","    top_k: int = 10,\n",") -> Tuple[Optional[float], Dict[int, float]]:\n","    if not HAS_GENSIM:\n","        return None, {}\n","\n","    tokenized = [d.split() for d in docs]\n","    topic_ids = sorted(t for t in topics_dict.keys() if t != -1)\n","    topic_words = [[w for (w, _) in topics_dict[t][:top_k]] for t in topic_ids]\n","    if not topic_words:\n","        return None, {}\n","\n","    dictionary = Dictionary(tokenized)\n","    corpus = [dictionary.doc2bow(toks) for toks in tokenized]\n","\n","    cm = CoherenceModel(\n","        topics=topic_words,\n","        texts=tokenized,\n","        corpus=corpus,\n","        dictionary=dictionary,\n","        coherence=\"c_npmi\",\n","    )\n","    overall = float(cm.get_coherence())\n","    per_topic_vals = cm.get_coherence_per_topic()\n","    per_topic = {tid: float(val) for tid, val in zip(topic_ids, per_topic_vals)}\n","    return overall, per_topic\n","\n","\n","# -----------------------------\n","# 10) Training single run\n","# -----------------------------\n","def train_single(\n","    *,\n","    docs: List[str],\n","    embs: np.ndarray,\n","    params_row: pd.Series,\n","    pos_cfg: str,\n","    cfg: Config,\n","    logger: logging.Logger,\n","    device: str,\n",") -> Dict:\n","    vectorizer = CountVectorizer(stop_words=\"english\", min_df=float(params_row[\"vectorizer__min_df\"]))\n","\n","    umap_model, hdbscan_model, using_gpu = make_umap_hdbscan(\n","        n_neighbors=int(params_row[\"umap__n_neighbors\"]),\n","        n_components=int(params_row[\"umap__n_components\"]),\n","        min_dist=float(params_row[\"umap__min_dist\"]),\n","        hdb_min_cluster_size=int(params_row[\"hdbscan__min_cluster_size\"]),\n","        hdb_min_samples=int(params_row[\"hdbscan__min_samples\"]),\n","        seed=cfg.seed,\n","        logger=logger,\n","    )\n","\n","    # No SentenceTransformer object for representations (not needed)\n","    rep = build_representation(pos_cfg, logger)\n","\n","    topic_model = BERTopic(\n","        embedding_model=None,  # using precomputed embeddings\n","        umap_model=umap_model,\n","        hdbscan_model=hdbscan_model,\n","        vectorizer_model=vectorizer,\n","        representation_model=rep,\n","        top_n_words=int(params_row[\"bertopic__top_n_words\"]),\n","        min_topic_size=int(params_row[\"bertopic__min_topic_size\"]),\n","        language=\"english\",\n","        calculate_probabilities=True,\n","        verbose=False,\n","    )\n","\n","    t0 = time.time()\n","    topics, probs = topic_model.fit_transform(docs, embs)\n","    fit_secs = time.time() - t0\n","\n","    topics_info = topic_model.get_topic_info()\n","    topics_dict = topic_model.get_topics()\n","\n","    n_outliers = int((np.array(topics) == -1).sum())\n","    outlier_ratio = n_outliers / max(1, len(topics))\n","    n_topics = int((topics_info[\"Topic\"] != -1).sum())\n","    diversity = topic_diversity(\n","        topics_dict, top_k=min(10, int(params_row[\"bertopic__top_n_words\"]))\n","    )\n","\n","    coh_overall, coh_per_topic = compute_coherence_npmi(docs, topics_dict, top_k=10)\n","\n","    safe_embed = str(params_row[\"Embeddings_Model\"]).replace(\"/\", \"_\")\n","    run_dir = (\n","        cfg.output_dir\n","        / f\"{safe_embed}\"\n","        / f\"iter_{int(params_row['Iteration'])}\"\n","        / f\"POS_{pos_cfg}\"\n","    )\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    topic_model.save(run_dir / \"bertopic_model.pkl\")\n","\n","    topics_info = topics_info.copy()\n","    topics_info[\"c_npmi\"] = topics_info[\"Topic\"].map(coh_per_topic)\n","    topics_info.to_csv(run_dir / \"topics_info.csv\", index=False)\n","\n","    if coh_per_topic:\n","        pd.DataFrame(\n","            [{\"Topic\": k, \"c_npmi\": v} for k, v in sorted(coh_per_topic.items())]\n","        ).to_csv(run_dir / \"per_topic_coherence.csv\", index=False)\n","\n","    topics_json = {\n","        str(k): [{w: float(p)} for (w, p) in v[: int(params_row[\"bertopic__top_n_words\"])]]\n","        for k, v in topics_dict.items()\n","        if k != -1 and v\n","    }\n","    with (run_dir / \"topics.json\").open(\"w\", encoding=\"utf-8\") as f:\n","        json.dump(topics_json, f, indent=2)\n","\n","    # Visuals (best-effort)\n","    try:\n","        topic_model.visualize_barchart(top_n_topics=15).write_html(str(run_dir / \"barchart.html\"))\n","    except Exception:\n","        pass\n","    try:\n","        topic_model.visualize_topics().write_html(str(run_dir / \"topics.html\"))\n","    except Exception:\n","        pass\n","    try:\n","        topic_model.visualize_hierarchy().write_html(str(run_dir / \"hierarchy.html\"))\n","    except Exception:\n","        pass\n","\n","    preview_rows = []\n","    for _, r in topics_info[topics_info[\"Topic\"] != -1].head(15).iterrows():\n","        t_id = int(r[\"Topic\"])\n","        top_words = \", \".join([w for w, _ in topics_dict[t_id]][:10])\n","        preview_rows.append(\n","            {\"Topic\": t_id, \"Count\": int(r[\"Count\"]), \"C_nPMI\": coh_per_topic.get(t_id, np.nan), \"TopWords\": top_words}\n","        )\n","    pd.DataFrame(preview_rows).to_csv(run_dir / \"top_topics_preview.csv\", index=False)\n","\n","    del topic_model, probs\n","    gc.collect()\n","\n","    return {\n","        \"run_dir\": str(run_dir),\n","        \"using_gpu\": using_gpu,\n","        \"n_topics\": n_topics,\n","        \"outlier_ratio\": outlier_ratio,\n","        \"topic_diversity\": diversity,\n","        \"coherence_c_npmi\": coh_overall,\n","        \"fit_seconds\": fit_secs,\n","    }\n","\n","\n","# -----------------------------\n","# 11) Scoring & ranking\n","# -----------------------------\n","def rank_runs(summary_df: pd.DataFrame, cfg: Config, logger: logging.Logger) -> pd.DataFrame:\n","    df = summary_df.copy()\n","\n","    def minmax(col):\n","        vals = df[col].astype(float)\n","        lo, hi = vals.min(), vals.max()\n","        if math.isclose(lo, hi):\n","            return pd.Series([0.5] * len(vals), index=vals.index)\n","        return (vals - lo) / (hi - lo)\n","\n","    df[\"norm_diversity\"] = minmax(\"topic_diversity\")\n","\n","    if \"coherence_c_npmi\" in df.columns and df[\"coherence_c_npmi\"].notna().any():\n","        df[\"coherence_c_npmi\"] = pd.to_numeric(df[\"coherence_c_npmi\"], errors=\"coerce\")\n","        df[\"norm_coherence\"] = minmax(\"coherence_c_npmi\")\n","        w_coh = cfg.w_coherence if HAS_GENSIM else 0.0\n","    else:\n","        df[\"norm_coherence\"] = 0.0\n","        w_coh = 0.0\n","\n","    df[\"penalty\"] = cfg.penalty_outliers * df[\"outlier_ratio\"]\n","\n","    if cfg.target_topics:\n","        lo, hi = cfg.target_topics\n","        center = (lo + hi) / 2.0\n","        span = max(1.0, (hi - lo) / 2.0)\n","        df[\"topics_window_penalty\"] = np.clip(np.abs(df[\"n_topics\"] - center) / span, 0, 1) * 0.2\n","    else:\n","        df[\"topics_window_penalty\"] = 0.0\n","\n","    df[\"score\"] = (\n","        cfg.w_diversity * df[\"norm_diversity\"]\n","        + w_coh * df[\"norm_coherence\"]\n","        - df[\"penalty\"]\n","        - df[\"topics_window_penalty\"]\n","    )\n","\n","    df = df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n","    return df\n","\n","\n","# -----------------------------\n","# 12) Main\n","# -----------------------------\n","def main():\n","    cfg = Config(\n","        dataset_csv=Path(\"/content/drive/MyDrive/Billionaire_CSV_Processed_Novels_Chapters_Sentences - processed_novels_sentences_new.csv\"),\n","        custom_stopwords_file=Path(\"/content/drive/MyDrive/Billionaire_Character_Names_Extracted.txt\"),\n","        output_dir=Path(\"/content/drive/MyDrive/Billionaire_BERTTopic_Models_POS_simplified\"),\n","        test_mode=False,\n","        sample_size=1000,\n","        target_topics=None,\n","    )\n","\n","    def create_params_df() -> pd.DataFrame:\n","        models = [\n","            {\"Embeddings_Model\": \"paraphrase-mpnet-base-v2\",\"Iteration\": 1,\"bertopic__min_topic_size\": 57,\"bertopic__top_n_words\": 37,\"hdbscan__min_cluster_size\": 132,\"hdbscan__min_samples\": 57,\"umap__min_dist\": 0.053015,\"umap__n_components\": 4,\"umap__n_neighbors\": 39,\"vectorizer__min_df\": 0.004806},\n","            {\"Embeddings_Model\": \"paraphrase-MiniLM-L6-v2\",\"Iteration\": 1,\"bertopic__min_topic_size\": 57,\"bertopic__top_n_words\": 37,\"hdbscan__min_cluster_size\": 132,\"hdbscan__min_samples\": 57,\"umap__min_dist\": 0.053015,\"umap__n_components\": 4,\"umap__n_neighbors\": 39,\"vectorizer__min_df\": 0.004806},\n","            {\"Embeddings_Model\": \"all-MiniLM-L12-v2\",\"Iteration\": 101,\"bertopic__min_topic_size\": 127,\"bertopic__top_n_words\": 26,\"hdbscan__min_cluster_size\": 128,\"hdbscan__min_samples\": 68,\"umap__min_dist\": 0.066224,\"umap__n_components\": 2,\"umap__n_neighbors\": 20,\"vectorizer__min_df\": 0.002257},\n","            {\"Embeddings_Model\": \"all-MiniLM-L12-v2\",\"Iteration\": 66,\"bertopic__min_topic_size\": 228,\"bertopic__top_n_words\": 27,\"hdbscan__min_cluster_size\": 105,\"hdbscan__min_samples\": 22,\"umap__min_dist\": 0.011738,\"umap__n_components\": 2,\"umap__n_neighbors\": 41,\"vectorizer__min_df\": 0.006573},\n","            {\"Embeddings_Model\": \"paraphrase-MiniLM-L6-v2\",\"Iteration\": 112,\"bertopic__min_topic_size\": 131,\"bertopic__top_n_words\": 22,\"hdbscan__min_cluster_size\": 86,\"hdbscan__min_samples\": 73,\"umap__min_dist\": 0.083975,\"umap__n_components\": 2,\"umap__n_neighbors\": 3,\"vectorizer__min_df\": 0.001335},\n","            {\"Embeddings_Model\": \"paraphrase-MiniLM-L6-v2\",\"Iteration\": 59,\"bertopic__min_topic_size\": 26,\"bertopic__top_n_words\": 38,\"hdbscan__min_cluster_size\": 120,\"hdbscan__min_samples\": 95,\"umap__min_dist\": 0.052454,\"umap__n_components\": 2,\"umap__n_neighbors\": 2,\"vectorizer__min_df\": 0.007219},\n","            {\"Embeddings_Model\": \"paraphrase-mpnet-base-v2\",\"Iteration\": 38,\"bertopic__min_topic_size\": 187,\"bertopic__top_n_words\": 35,\"hdbscan__min_cluster_size\": 60,\"hdbscan__min_samples\": 88,\"umap__min_dist\": 0.011252,\"umap__n_components\": 4,\"umap__n_neighbors\": 42,\"vectorizer__min_df\": 0.009032},\n","            {\"Embeddings_Model\": \"paraphrase-MiniLM-L6-v2\",\"Iteration\": 13,\"bertopic__min_topic_size\": 59,\"bertopic__top_n_words\": 16,\"hdbscan__min_cluster_size\": 54,\"hdbscan__min_samples\": 74,\"umap__min_dist\": 0.020023,\"umap__n_components\": 4,\"umap__n_neighbors\": 11,\"vectorizer__min_df\": 0.002484},\n","            {\"Embeddings_Model\": \"all-MiniLM-L12-v2\",\"Iteration\": 60,\"bertopic__min_topic_size\": 77,\"bertopic__top_n_words\": 13,\"hdbscan__min_cluster_size\": 144,\"hdbscan__min_samples\": 51,\"umap__min_dist\": 0.069807,\"umap__n_components\": 4,\"umap__n_neighbors\": 2,\"vectorizer__min_df\": 0.002135},\n","            {\"Embeddings_Model\": \"all-MiniLM-L12-v2\",\"Iteration\": 52,\"bertopic__min_topic_size\": 245,\"bertopic__top_n_words\": 38,\"hdbscan__min_cluster_size\": 98,\"hdbscan__min_samples\": 30,\"umap__min_dist\": 0.044896,\"umap__n_components\": 9,\"umap__n_neighbors\": 28,\"vectorizer__min_df\": 0.005585},\n","        ]\n","        return pd.DataFrame(models)\n","\n","    logger = setup_logging(cfg.output_dir)\n","    logger.info(\"Starting BERTopic grid with c_npmi coherence\")\n","\n","    if not HAS_GENSIM:\n","        logger.info(\"Gensim not found: c_npmi coherence will be skipped and weight set to 0\")\n","        cfg.w_coherence = 0.0\n","\n","    np.random.seed(cfg.seed)\n","    torch.manual_seed(cfg.seed)\n","\n","    ensure_spacy(logger)\n","    stops = load_stopwords(cfg.custom_stopwords_file)\n","    docs = load_corpus(cfg, logger, stops)\n","\n","    params_df = create_params_df()\n","    for col in cfg.param_cols:\n","        if col not in params_df.columns:\n","            raise ValueError(f\"Params DF missing column '{col}'\")\n","\n","    emb_models = params_df[\"Embeddings_Model\"].unique().tolist()\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    logger.info(f\"Embedding device: {device}\")\n","\n","    embed_cache: Dict[str, np.ndarray] = {}\n","    for name in emb_models:\n","        embs = get_embeddings(name, docs, cfg.embeddings_dir, device, logger)\n","        embed_cache[name] = embs\n","\n","    summary_rows: List[Dict] = []\n","    for _, row in tqdm(params_df.iterrows(), total=len(params_df), desc=\"Training grid\"):\n","        emb_name = str(row[\"Embeddings_Model\"])\n","        embs = embed_cache[emb_name]\n","\n","        for pos_cfg in cfg.pos_configs:\n","            try:\n","                result = train_single(\n","                    docs=docs,\n","                    embs=embs,\n","                    params_row=row,\n","                    pos_cfg=pos_cfg,\n","                    cfg=cfg,\n","                    logger=logger,\n","                    device=device,\n","                )\n","            except Exception as e:\n","                logger.exception(f\"Run failed for {emb_name} iter={row['Iteration']} POS={pos_cfg}: {e}\")\n","                continue\n","\n","            summary_rows.append(\n","                {\n","                    **{k: row[k] for k in cfg.param_cols},\n","                    \"POS_Config\": pos_cfg,\n","                    **result,\n","                }\n","            )\n","            gc.collect()\n","\n","    if not summary_rows:\n","        logger.error(\"No successful runs.\")\n","        return\n","\n","    summary_df = pd.DataFrame(summary_rows)\n","    summary_path = cfg.output_dir / \"summary_raw.csv\"\n","    summary_df.to_csv(summary_path, index=False)\n","    logger.info(f\"Wrote raw summary: {summary_path}\")\n","\n","    ranked = rank_runs(summary_df, cfg, logger)\n","    ranked_path = cfg.output_dir / \"summary_ranked.csv\"\n","    ranked.to_csv(ranked_path, index=False)\n","    logger.info(f\"Wrote ranked summary: {ranked_path}\")\n","\n","    top5 = ranked.head(5)[\n","        [\n","            \"Embeddings_Model\",\n","            \"Iteration\",\n","            \"POS_Config\",\n","            \"n_topics\",\n","            \"outlier_ratio\",\n","            \"topic_diversity\",\n","            \"coherence_c_npmi\",\n","            \"score\",\n","            \"run_dir\",\n","        ]\n","    ]\n","    top5.to_csv(cfg.output_dir / \"leaderboard_top5.csv\", index=False)\n","\n","    best = ranked.iloc[0].to_dict()\n","    with (cfg.output_dir / \"best_overall.txt\").open(\"w\", encoding=\"utf-8\") as f:\n","        json.dump(best, f, indent=2)\n","\n","    logger.info(\"Done. Best run:\")\n","    logger.info(json.dumps(best, indent=2))\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Su_gUjTTdKXK","executionInfo":{"status":"error","timestamp":1761951395445,"user_tz":-60,"elapsed":522058,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"6aa3bab1-0e96-41b4-ef30-31652fa36bd2"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["2025-10-31 22:47:53,395 - INFO - Starting BERTopic grid with c_npmi coherence\n","INFO:bertopic_grid:Starting BERTopic grid with c_npmi coherence\n","2025-10-31 22:48:03,553 - INFO - Loaded 391,594 cleaned documents\n","INFO:bertopic_grid:Loaded 391,594 cleaned documents\n","2025-10-31 22:48:03,589 - INFO - Embedding device: cuda\n","INFO:bertopic_grid:Embedding device: cuda\n","2025-10-31 22:48:03,590 - INFO - Loading cached embeddings: bertopic_runs/embeddings/paraphrase-mpnet-base-v2.npy\n","INFO:bertopic_grid:Loading cached embeddings: bertopic_runs/embeddings/paraphrase-mpnet-base-v2.npy\n","2025-10-31 22:48:03,980 - INFO - Loading cached embeddings: bertopic_runs/embeddings/paraphrase-MiniLM-L6-v2.npy\n","INFO:bertopic_grid:Loading cached embeddings: bertopic_runs/embeddings/paraphrase-MiniLM-L6-v2.npy\n","2025-10-31 22:48:04,170 - INFO - Loading cached embeddings: bertopic_runs/embeddings/all-MiniLM-L12-v2.npy\n","INFO:bertopic_grid:Loading cached embeddings: bertopic_runs/embeddings/all-MiniLM-L12-v2.npy\n","Training grid:   0%|          | 0/10 [00:00<?, ?it/s]2025-10-31 22:48:04,372 - INFO - Using GPU (cuML) UMAP + HDBSCAN\n","INFO:bertopic_grid:Using GPU (cuML) UMAP + HDBSCAN\n"]},{"output_type":"stream","name":"stdout","text":["[2025-10-31 22:48:04.375] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n"]},{"output_type":"stream","name":"stderr","text":["2025-10-31 22:51:33,904 - ERROR - Run failed for paraphrase-mpnet-base-v2 iter=1 POS=a: 'NoneType' object has no attribute 'embed_documents'\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1103535406.py\", line 546, in main\n","    result = train_single(\n","             ^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1103535406.py\", line 358, in train_single\n","    topics, probs = topic_model.fit_transform(docs, embs)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 515, in fit_transform\n","    self._extract_topics(\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4049, in _extract_topics\n","    self.topic_representations_ = self._extract_words_per_topic(\n","                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4376, in _extract_words_per_topic\n","    topics = main_model.extract_topics(self, documents, c_tf_idf, topics)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 104, in extract_topics\n","    sim_matrix, words = self._extract_embeddings(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 183, in _extract_embeddings\n","    repr_embeddings = topic_model._extract_embeddings(representative_docs, method=\"document\", verbose=False)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 3729, in _extract_embeddings\n","    embeddings = self.embedding_model.embed_documents(documents, verbose=verbose)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'embed_documents'\n","ERROR:bertopic_grid:Run failed for paraphrase-mpnet-base-v2 iter=1 POS=a: 'NoneType' object has no attribute 'embed_documents'\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1103535406.py\", line 546, in main\n","    result = train_single(\n","             ^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1103535406.py\", line 358, in train_single\n","    topics, probs = topic_model.fit_transform(docs, embs)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 515, in fit_transform\n","    self._extract_topics(\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4049, in _extract_topics\n","    self.topic_representations_ = self._extract_words_per_topic(\n","                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4376, in _extract_words_per_topic\n","    topics = main_model.extract_topics(self, documents, c_tf_idf, topics)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 104, in extract_topics\n","    sim_matrix, words = self._extract_embeddings(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 183, in _extract_embeddings\n","    repr_embeddings = topic_model._extract_embeddings(representative_docs, method=\"document\", verbose=False)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 3729, in _extract_embeddings\n","    embeddings = self.embedding_model.embed_documents(documents, verbose=verbose)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'embed_documents'\n","2025-10-31 22:51:33,930 - INFO - Using GPU (cuML) UMAP + HDBSCAN\n","INFO:bertopic_grid:Using GPU (cuML) UMAP + HDBSCAN\n"]},{"output_type":"stream","name":"stdout","text":["[2025-10-31 22:51:33.934] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n"]},{"output_type":"stream","name":"stderr","text":["2025-10-31 22:55:07,605 - ERROR - Run failed for paraphrase-mpnet-base-v2 iter=1 POS=b: 'NoneType' object has no attribute 'embed_documents'\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1103535406.py\", line 546, in main\n","    result = train_single(\n","             ^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1103535406.py\", line 358, in train_single\n","    topics, probs = topic_model.fit_transform(docs, embs)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 515, in fit_transform\n","    self._extract_topics(\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4049, in _extract_topics\n","    self.topic_representations_ = self._extract_words_per_topic(\n","                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4376, in _extract_words_per_topic\n","    topics = main_model.extract_topics(self, documents, c_tf_idf, topics)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 104, in extract_topics\n","    sim_matrix, words = self._extract_embeddings(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 183, in _extract_embeddings\n","    repr_embeddings = topic_model._extract_embeddings(representative_docs, method=\"document\", verbose=False)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 3729, in _extract_embeddings\n","    embeddings = self.embedding_model.embed_documents(documents, verbose=verbose)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'embed_documents'\n","ERROR:bertopic_grid:Run failed for paraphrase-mpnet-base-v2 iter=1 POS=b: 'NoneType' object has no attribute 'embed_documents'\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1103535406.py\", line 546, in main\n","    result = train_single(\n","             ^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1103535406.py\", line 358, in train_single\n","    topics, probs = topic_model.fit_transform(docs, embs)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 515, in fit_transform\n","    self._extract_topics(\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4049, in _extract_topics\n","    self.topic_representations_ = self._extract_words_per_topic(\n","                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 4376, in _extract_words_per_topic\n","    topics = main_model.extract_topics(self, documents, c_tf_idf, topics)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 104, in extract_topics\n","    sim_matrix, words = self._extract_embeddings(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/representation/_keybert.py\", line 183, in _extract_embeddings\n","    repr_embeddings = topic_model._extract_embeddings(representative_docs, method=\"document\", verbose=False)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\", line 3729, in _extract_embeddings\n","    embeddings = self.embedding_model.embed_documents(documents, verbose=verbose)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'NoneType' object has no attribute 'embed_documents'\n","2025-10-31 22:55:07,633 - INFO - Using GPU (cuML) UMAP + HDBSCAN\n","INFO:bertopic_grid:Using GPU (cuML) UMAP + HDBSCAN\n"]},{"output_type":"stream","name":"stdout","text":["[2025-10-31 22:55:07.636] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining grid:   0%|          | 0/10 [08:30<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1103535406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1103535406.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpos_cfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 result = train_single(\n\u001b[0m\u001b[1;32m    547\u001b[0m                     \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0membs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1103535406.py\u001b[0m in \u001b[0;36mtrain_single\u001b[0;34m(docs, embs, params_row, pos_cfg, cfg, logger, device)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mfit_secs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# Reduce dimensionality and fit UMAP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;31m# Zero-shot Topic Modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3792\u001b[0m                 \u001b[0;31m# cuml umap needs y to be an numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3794\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3795\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3796\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cuml/manifold/umap.pyx\u001b[0m in \u001b[0;36mcuml.manifold.umap.UMAP.fit\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cuml/manifold/umap_utils.pyx\u001b[0m in \u001b[0;36mcuml.manifold.umap_utils.GraphHolder.get_cupy_coo\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cupyx/scipy/sparse/_coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     60\u001b[0m         ''', 'cupyx_scipy_sparse_coo_sum_duplicates_diff')\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             raise ValueError(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}