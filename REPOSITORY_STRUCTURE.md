# Repository Structure Map

**Romantic Novels NLP Research Project**

This document provides a comprehensive map of the repository structure, organized by research stage and component type.

**Last updated:** After stage-first restructure (2025-01-XX)

---

## Root Directory

### Configuration Files
- **`requirements.txt`** - Python package dependencies
- **`.gitignore`** - Git ignore patterns (excludes venv, __pycache__, etc.)
- **`Makefile`** - Stage-aware build targets for pipeline execution

### Configuration Directories
- **`.cursor/`** - Cursor IDE configuration
- **`.rules/`** - Project-specific coding rules and protocols
- **`configs/`** - YAML configuration files (see Configuration section below)

### Documentation
- **`REPOSITORY_STRUCTURE.md`** - This file
- **`MIGRATION_PLAN.md`** - Migration plan document
- **`MIGRATION_LOG.md`** - Complete migration log

---

## Data Directory (`data/`)

### `data/raw/` - Raw Input Data
**Purpose:** Unprocessed source materials (novels, EPUB files)

#### `Billionaire_Full_Novels_TXT/`
- **Description:** Full text files of romantic novels organized by author
- **Structure:** Author subdirectories (Asher, Bowen, Clare, Day, etc.) containing `.txt` files
- **Content:** Complete novel texts extracted from EPUBs

#### `Billionaire_Novels_EPUB/`
- **Description:** Original EPUB files organized by author
- **Structure:** 
  - Author subdirectories (35+ authors) with `.epub` files
  - `booknlp_outputs/` - BookNLP processing outputs (105 books)
  - `txt_books/` - Extracted text files from EPUBs (105 files)

---

### `data/interim/` - Intermediate Processing Data
**Purpose:** Data generated during processing steps, not final outputs

#### `data/interim/booknlp/`
- **Description:** BookNLP processing outputs (entities, tokens, quotes, etc.)
- **Structure:** `Bilionaire_Romantic_Novels_EPUB_BookNLP/` organized by author
- **File Types:**
  - `.book` - BookNLP book format files
  - `.entities` - Named entity recognition results
  - `.tokens` - Tokenized text
  - `.quotes` - Extracted dialogue/quotes
  - `.supersense` - Semantic supersense annotations
  - `.txt` - Token files by POS (adj_noun, verb_noun, noun, all)

#### `data/interim/octis/`
- **Description:** OCTIS framework datasets and optimization results
- **Structure:**
  - `optimization_results/` - Bayesian optimization results by embedding model
  - Dataset files for OCTIS framework

---

### `data/processed/` - Final Processed Data
**Purpose:** Cleaned, processed data ready for analysis and modeling

#### Files:
- **`chapters.csv`** - Processed novel chapters with sentences (main dataset, ~707K rows)
- **`chapters_subset_10000.csv`** - Subset of chapters for testing (10K rows)
- **`goodreads.csv`** - Cleaned Goodreads dataset with ratings and metadata
- **`custom_stoplist.txt`** - Custom stopwords list (extracted character names)
- **`prepared_books.parquet`** - Prepared books data with topic distributions and metadata (generated by Stage 07)

#### `Billionaire_ALL_MERGED_TXTs_by_POS/`
- **Description:** Merged token files organized by part-of-speech
- **Files:**
  - `tokens_all.txt` - All tokens
  - `tokens_noun.txt` - Noun tokens only
  - `tokens_adj_noun.txt` - Adjective-noun pairs
  - `tokens_verb_noun.txt` - Verb-noun pairs

---

## Source Code (`src/`)

### Stage-Based Pipeline Structure

The pipeline is organized into **seven stages**, each with its own directory and entrypoint:

#### `src/stage01_ingestion/` - Data Ingestion
**Purpose:** Load texts, Goodreads data, BookNLP I/O operations

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for ingestion stage
- (Additional ingestion modules to be added)

**Usage:**
```bash
python -m src.stage01_ingestion.main --config configs/paths.yaml
```

#### `src/stage02_preprocessing/` - Preprocessing
**Purpose:** Text cleaning, tokenization/lemmatization, custom stoplist building

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for preprocessing stage
- (Additional preprocessing modules to be added)

**Usage:**
```bash
python -m src.stage02_preprocessing.main --config configs/paths.yaml
```

#### `src/stage03_modeling/` - Modeling
**Purpose:** BERTopic fit/retrain, OCTIS adapter integration

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint (train, retrain commands)
- **`bertopic_runner.py`** - Main BERTopic wrapper integrating with OCTIS, handles model training with various embedding models
- **`retrain_from_tables.py`** - Retrain BERTopic models from topic tables, includes coherence priority and representation improvements

**Usage:**
```bash
python -m src.stage03_modeling.main train --config configs/bertopic.yaml
python -m src.stage03_modeling.main retrain --dataset_csv data/processed/chapters.csv --out_dir models/
```

#### `src/stage04_experiments/` - Experiments
**Purpose:** Bayesian hyperparameter search; experiment ledgers

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for experiments stage
- **`hparam_search.py`** - Bayesian hyperparameter search using Optuna, optimizes BERTopic parameters

**Usage:**
```bash
python -m src.stage04_experiments.main --config configs/optuna.yaml
```

#### `src/stage05_selection/` - Selection
**Purpose:** Pareto efficiency analysis + constraints (nr_topics >= 200)

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for selection stage (enforces min_nr_topics >= 200 constraint)
- (Selection modules to be added)

**Usage:**
```bash
python -m src.stage05_selection.main --config configs/selection.yaml
```

#### `src/stage06_labeling/` - Labeling
**Purpose:** Semi-supervised topic labeling; composite building

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for labeling stage
- **`composites.py`** - Builds AP (Appreciation Pattern) composites from topic groups

**Usage:**
```bash
python -m src.stage06_labeling.main --config configs/labeling.yaml
```

#### `src/stage07_analysis/` - Analysis
**Purpose:** Goodreads scoring/strata, statistical analysis, FDR correction

- **`__init__.py`** - Package initialization
- **`main.py`** - CLI entrypoint for analysis stage
- **`scoring_and_strata.py`** - Prepares data and groups for statistical analysis, builds quartiles
- **`bh_fdr.py`** - Benjamini-Hochberg FDR correction for multiple comparisons, group delta analysis
- **`03_interaction_plots_PD.py`** - Generates interaction plots for probability distributions
- **`04_star_charts_and_pdf.py`** - Creates star charts and PDF visualizations

**Usage:**
```bash
python -m src.stage07_analysis.main --config configs/scoring.yaml
```

### `src/common/` - Shared Utilities
**Purpose:** Common utilities shared across all pipeline stages

- **`__init__.py`** - Common package exports
- **`config.py`** - Configuration loading utilities (`load_config`, `resolve_path`)
- **`io.py`** - I/O utilities for CSV, JSON, pickle, parquet files
- **`logging.py`** - Logging setup utilities
- **`metrics.py`** - Metrics computation utilities
- **`seed.py`** - Random seed utilities for reproducibility
- **`training_utils.py`** - Training-related utilities (GPU checks, output dirs, logging setup)
- **`check_gpu_setup.py`** - GPU availability and configuration checks
- **`hw_utils.py`** - Hardware utility functions
- **`thermal_monitor.py`** - GPU thermal monitoring

**Usage:**
```python
from src.common.config import load_config, resolve_path
from src.common.io import load_data, save_data
from src.common.logging import setup_logging
from src.common.training_utils import check_gpu_availability, setup_output_dirs
```

---

## Configuration (`configs/`)

**Purpose:** Centralized YAML configuration files for all pipeline components

All paths in configs are relative to project root and resolved via `src.common.config.resolve_path()`.

### `configs/paths.yaml`
- Data directory paths (raw, interim, processed)
- Input file paths (chapters_csv, goodreads_csv, custom_stoplist, etc.)
- Output directory paths (models, results, experiments, pareto, topics, figures, logs)

### `configs/bertopic.yaml`
- BERTopic model configuration
- Embedding model settings
- UMAP, HDBSCAN, vectorizer parameters
- Training settings (coherence_sample, chunk_size, seed)

### `configs/octis.yaml`
- OCTIS framework configuration
- Dataset settings
- Model wrapper configuration
- Evaluation metrics
- Output settings (save_path, optimization_results_dir)

### `configs/optuna.yaml`
- Optuna/Bayesian optimization configuration
- Optimization settings (number_of_call, n_random_starts, early_stop)
- Surrogate model settings
- Search space definitions

### `configs/selection.yaml`
- Model selection configuration
- **Constraints:** `min_nr_topics: 200` (enforced in stage05)
- Pareto optimization objectives
- Selection criteria (top_k, tie_breaker)

### `configs/labeling.yaml`
- Topic labeling configuration
- Composite building settings
- Topic grouping configuration
- Output paths

### `configs/scoring.yaml`
- Scoring and statistical analysis configuration
- Goodreads integration settings
- Stratification settings (quartiles, terciles, deciles)
- Statistical analysis settings (FDR correction, alpha, method)
- Group comparison settings
- Input/output file paths

**All hardcoded paths have been removed from source code and replaced with config-based paths.**

---

## Notebooks (`notebooks/`)

**Purpose:** Jupyter notebooks organized by research stage (01-07)

### `notebooks/01_ingestion/` - Data Ingestion & Initial EDA
- **`dataset_building/`**
  - `bookNLP_parse_text_of_books.ipynb` - BookNLP text parsing pipeline
  - `parse_structured_text_from_epub.ipynb` - EPUB extraction and parsing
  - `scraping_ratings_information_romantic.ipynb` - Goodreads data scraping
- **`romantic novels dataset EDA/`**
  - `EDA romantic novels.ipynb` - Initial exploratory data analysis

### `notebooks/02_preprocessing/` - Preprocessing
- (Preprocessing notebooks to be added)

### `notebooks/03_modeling/` - Model Training & Optimization
- **`retrain_best_models/`**
  - `retrain_best_topics_coherence_proprity.ipynb` - Retrain models with coherence priority
  - `retrain_best_topics_coherence_proprity_with_add_repr.ipynb` - Retrain with additional representation improvements
  - `Copy of retrain_best_topics_coherence_proprity_with_add_repr.ipynb` - Backup copy

### `notebooks/04_experiments/` - Hyperparameter Experiments
- (Experiment notebooks to be added)

### `notebooks/05_selection/` - Pareto Analysis & Model Selection
- **`post_processing_and_choosing_best_model/`**
  - `Billionaire_Project_Pareto_Efficiency_Analysis_Choose_Top_Models.ipynb` - Main Pareto efficiency analysis
  - `pareto_ten_best_models.ipynb` - Top 10 Pareto-efficient models
  - `hyperparameters_corr.ipynb` - Hyperparameter correlation analysis
  - `coherence_diversity_calculations.ipynb` - Coherence and diversity metrics
  - `cleaning_topics_by_coherence_score.ipynb` - Topic cleaning by coherence
  - `post_processing_best_topics_coherence_priority.ipynb` - Post-processing with coherence priority
  - `choose_best_models_plot_stars.ipynb` - Model selection visualization
  - `best models.ipynb` - Best model identification
  - `1o__best_bert_model.ipynb` - Best BERTopic model analysis
  - `Another copy of 1o__best_bert_model.ipynb` - Backup copy

### `notebooks/06_labeling/` - Topic Labeling
- (Labeling notebooks to be added)

### `notebooks/07_analysis/` - Statistical Analysis & Findings
- **`goodreads dataset EDA and score adjustment/`**
  - `EDA_goodreads.ipynb` - Goodreads exploratory data analysis
  - `Exploratory Data Analysis (EDA) of the Goodreads Dataset.ipynb` - Main EDA notebook
  - `RIGHT Exploratory Data Analysis (EDA) of the Goodreads Dataset.ipynb` - Corrected EDA version
  - `bayesian_score_adjustment.ipynb` - Bayesian score adjustment for ratings
  - Multiple copies/variants of EDA notebooks

- **`correlation analysis of topics/`**
  - `topics_analysis.ipynb` - Main topic correlation analysis
  - `topics_correlation_with_all_topics.ipynb` - Cross-topic correlations
  - `bert_topic_similarity_analysis.ipynb` - BERTopic similarity analysis
  - `clustering_topics_per_book_BERTopic.ipynb` - Topic clustering by book
  - `Topics by Broader Thematic Gropus.ipynb` - Thematic grouping analysis
  - `Fanfuction_Example_full_code_for_topics_BERTopic.ipynb` - Example code
  - `Copy of topics_analysis.ipynb` - Backup copy

- **`mapping back probabilities to books ans statistical analysis of probabilities distribution/`**
  - `books_by_probabilities_analysis.ipynb` - Probability distribution analysis
  - `Copy of books_by_probabilities_analysis.ipynb` - Backup copy

- **`clusterisation by subgenres/`**
  - `clusterisation of topis by subgenres.ipynb` - Topic clustering by subgenre

---

## Results (`results/`)

### `results/experiments/` - Experimental Results & Ledgers
- **`model_evaluation_results.csv`** - Summary ledger of all model evaluation results
- **`Billionaire_OCTIS_ALL_Models_Results_CSV_Only/`** - OCTIS experiment results by embedding model
  - Subdirectories: `all-MiniLM-L12-v2/`, `multi-qa-mpnet-base-cos-v1/`, `paraphrase-distilroberta-base-v1/`, etc.
  - Files: `result.json`, `result_2.json`, `result_3.json` (experiment outputs)
- **`Billionaire_OCTIS_Two_Pretrained_Models_CSV_With_Evaluation_Results/`** - Evaluation results with pretrained models
  - `optimization_results/` - Optuna optimization results (`.npz` files)
- **`reproducible_scripts topics vs readers appreciation/`** - Analysis outputs
  - `signature_themes_by_group_long.csv` - Signature themes by reader group
  - `top20_signature_themes_*.csv` - Top 20 themes by quartile (top, middle, trash)
  - `top216_group_classification_refined.csv` - Refined topic group classifications
  - `topic_correlation_matrix.csv` - Topic correlation matrix
  - `topic_group_prevalence_with_eta2.csv` - Group prevalence with effect sizes
  - `top_correlated_pairs.csv` - Most correlated topic pairs
  - `all_topics_grouped_bars.pdf` - Visualization PDF
  - `Billionaire_Manual_Mapping_Topics_by_Thematic_Clusters.md/pdf` - Manual topic mapping documentation
  - `README.txt` - Documentation

### `results/pareto/` - Pareto-Efficient Models
- **`pareto.csv`** - Pareto efficiency analysis results (to be generated)
- **`topics/pareto_efficient_per_model/`** - Topic sets for Pareto-efficient models
  - 28 JSON files, one per Pareto-efficient model configuration
  - Format: `{model_name}__{trial_id}__topics.json`

### `results/topics/` - Topic Model Outputs
- **`by_book.csv`** - Topic distribution by book (main output)
- **`ap_composites.parquet`** - AP (Appreciation Pattern) composites (generated by Stage 06)
- **`Bilionaires_JSON_Top_Models_Topics_with_Coherence_Scores/`** - Top models with coherence scores
  - 14 JSON files with cleaned topics and coherence metrics
  - Format: `{model}__{trial}__coh_{score}__topics_clean.json`
- **`Bilionaires_JSON_Topics_NO_Coherence_Scores/`** - Topics without coherence scores
  - 14 JSON files
- **`Billionaire_Top_Models_JSON_Topics/`** - Top model topic outputs
  - 8 JSON files with topic representations

### `results/analysis/` - Statistical Analysis Outputs
- **`AP_composites_deltas_with_BHFDR.csv`** - FDR-corrected composite deltas (generated by Stage 07)
- **`strata.csv`** - Stratification results (to be generated)

### `results/figures/` - Generated Visualizations
- **`charts_with_stars/`** - Star charts and delta visualizations (generated by Stage 07)
- **`Consolidated_Table_Plots.pdf`** - Consolidated PDF with all visualizations (generated by Stage 07)
- EDA plots, correlation visualizations, and analysis figures (generated on-demand)

---

## Reports (`reports/`)

### `reports/eda/` - Exploratory Data Analysis Reports
- **Purpose:** EDA documentation and findings
- **Status:** Currently empty

### `reports/findings/` - Research Findings & Documentation
- **`CHARACTER_NAMES_ANALYSIS.md`** - Analysis of character name extraction
- **`README_CHARACTER_NAMES.md`** - Character names documentation
- **`PIPELINE_OVERVIEW.md`** - Pipeline overview and methodology

---

## Scripts (`scripts/`)

**Purpose:** Standalone utility scripts

- **`convert_topics.py`** - Converts topic `.npy` files to JSON format
- **`restart_script.py`** - Script for restarting interrupted experiments
- **`ml_heat_diag.sh`** - Shell script for ML hardware diagnostics

---

## Models (`models/`)

**Purpose:** Saved trained models

- **Status:** Currently empty
- **Usage:** Will store serialized BERTopic models after training
- **Structure:** Organized by embedding model and trial ID

---

## Build System

### Makefile Targets

The `Makefile` provides convenient targets for running pipeline stages:

```bash
make inventory      # Repository structure inventory
make stage01        # Run Stage 01: Ingestion
make stage02        # Run Stage 02: Preprocessing
make stage03        # Run Stage 03: Modeling
make stage04        # Run Stage 04: Experiments
make stage05        # Run Stage 05: Selection
make stage06        # Run Stage 06: Labeling
make stage07        # Run Stage 07: Analysis
make experiments    # Alias for stage04
make pareto         # Alias for stage05
make topics         # Alias for stage06
make analysis       # Alias for stage07
make all            # Run all stages sequentially
```

---

## Research Pipeline Flow

1. **Stage 01: Ingestion** → Load raw texts, Goodreads data, BookNLP I/O
2. **Stage 02: Preprocessing** → Text cleaning, tokenization, stoplist building
3. **Stage 03: Modeling** → BERTopic training → Hyperparameter optimization → Model evaluation
4. **Stage 04: Experiments** → Bayesian hyperparameter search → Experiment ledgers
5. **Stage 05: Selection** → Pareto efficiency analysis → Best model selection (enforces min_nr_topics >= 200)
6. **Stage 06: Labeling** → Semi-supervised topic labeling → Composite building
7. **Stage 07: Analysis** → Goodreads integration → Statistical analysis → FDR correction → Findings

---

## File Naming Conventions

- **Models:** `{embedding_model}__{trial_id}__{additional_info}__topics.json`
- **Results:** `result.json`, `result_{n}.json` for multiple runs
- **Data:** Descriptive names (e.g., `chapters.csv`, `goodreads.csv`)
- **Notebooks:** Descriptive names indicating purpose and stage
- **Configs:** `{component}.yaml` (e.g., `bertopic.yaml`, `scoring.yaml`)

---

## Configuration-Based Paths

**All hardcoded paths have been removed from source code.** All file paths are now loaded from YAML configuration files:

- **Data paths:** `configs/paths.yaml`
- **Model configs:** `configs/bertopic.yaml`, `configs/octis.yaml`
- **Experiment configs:** `configs/optuna.yaml`
- **Selection configs:** `configs/selection.yaml`
- **Analysis configs:** `configs/scoring.yaml`, `configs/labeling.yaml`

Files use `src.common.config.load_config()` and `resolve_path()` to load and resolve paths relative to project root.

---

## Notes

- All file moves were performed using `git mv` to preserve history
- Repository structure follows **stage-first** organization (01-07)
- Python packages use `__init__.py` for proper module structure
- Stage entrypoints are thin CLI wrappers using Click/typer
- Common utilities consolidated in `/src/common` to avoid duplication
- Large files (parquet, npy, pt, mm, ipynb) should use Git LFS (not currently configured)
- All paths are config-based with fallback defaults for robustness

---

*Last updated: After stage-first restructure and config-based path migration (2025-01-XX)*
