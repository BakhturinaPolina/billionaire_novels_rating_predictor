{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsVOSsXT7CmBy0cQBXIRJ5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Step 1: Install necessary libraries\n","!pip install ebooklib beautifulsoup4 nltk pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFvoudD_LZz7","executionInfo":{"status":"ok","timestamp":1717259459210,"user_tz":-120,"elapsed":6176,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"c6a15662-5c00-41d5-e644-5a2f7ec7a2a1","collapsed":true},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ebooklib in /usr/local/lib/python3.10/dist-packages (0.18)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from ebooklib) (4.9.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ebooklib) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n"]}]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sRO5kcADD00","executionInfo":{"status":"ok","timestamp":1717256375299,"user_tz":-120,"elapsed":1843,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"d654e14b-5385-48fc-8308-342f788f4dcb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import ebooklib\n","from ebooklib import epub, ITEM_DOCUMENT\n","from bs4 import BeautifulSoup, Tag\n","from pathlib import Path\n","import json\n","import os\n","import pprint\n","import pandas as pd\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize"],"metadata":{"id":"lznMGtWqJCHq","executionInfo":{"status":"ok","timestamp":1717259463969,"user_tz":-120,"elapsed":601,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"29905146-7904-4245-f58e-930614509c3f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["corpus = {}\n","\n","# Expected directory structure:\n","# rootdir\n","#         author_name_dir\n","#                   book.epub\n","\n","rootdir = \"/content/drive/MyDrive/full_list_of_romantic_novels\"\n","\n","total_authors_count = len(os.listdir(rootdir))\n","\n","output_csv = []\n","\n","# Traverse through the directory structure\n","for index, author_name in enumerate(os.listdir(rootdir)):\n","    author_path = os.path.join(rootdir, author_name)\n","    if os.path.isdir(author_path):\n","        print(\"Processing author\", author_name, \"(\", index + 1, \"/\", total_authors_count, \")\")\n","        corpus[author_name] = {}\n","        for filename in os.listdir(author_path):\n","            if filename.endswith(\".epub\"):\n","                book_name = filename.replace(\".epub\", \"\")\n","                file_path = os.path.join(author_path, filename)\n","\n","                # Chapters will be stored here\n","                corpus[author_name][book_name] = []\n","\n","                 # read content from each book in a subdir using ebooklib module\n","                book = epub.read_epub(file_path)\n","                 # Extract chapters with .get_items_of_type method of epub, and map them to chapters list\n","                chapters = list(book.get_items_of_type(ITEM_DOCUMENT))\n","                 # This part of the code is optional and needed to log which book is being processed.\n","                for (chapter) in (chapters):  # apply the function \"extract_poems_and_titles()\" for each chapter.\n","\n","                    # HTML body of the chaoter\n","                    chapter_body = BeautifulSoup(chapter.get_body_content(), \"html.parser\")\n","\n","                     # Whole text of a chapter as a string\n","                    chapter_text = chapter_body.get_text(separator=\" \", strip=True)\n","\n","                    # Split text by a dot to get the sentences\n","                    chapter_sentences = sent_tokenize(chapter_text)\n","\n","                     # Filter out empty strings and strip the sentences\n","                    chapter_sentences = [sentence.strip() for sentence in chapter_sentences if sentence]\n","\n","\n","                     # Only include chapters with certain length to filter out meta chapters\n","                    if len(chapter_sentences) > 10:\n","                        corpus[author_name][book_name].append(chapter_sentences)\n","                        for sentence in chapter_sentences:\n","                          output_csv.append({'Author': author_name, 'Book Title': book_name, 'Chapter': len(corpus[author_name][book_name]),  'Sentence': sentence})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":812},"id":"N_DfoktwQAIi","executionInfo":{"status":"error","timestamp":1717259338478,"user_tz":-120,"elapsed":225208,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"90fdf99c-04de-4230-9be7-bbd77dc67876"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing author Ann_Cole ( 1 / 35 )\n","Processing author Meghan_Quinn ( 2 / 35 )\n","Processing author Rose_Shain ( 3 / 35 )\n","Processing author Jessica_Lemmon ( 4 / 35 )\n","Processing author Lisa_Kleypas ( 5 / 35 )\n","Processing author Louise_Bay ( 6 / 35 )\n","Processing author Laurelin_Paige ( 7 / 35 )\n","Processing author Sam_Crescent ( 8 / 35 )\n","Processing author Sandi_Lynn ( 9 / 35 )\n","Processing author Sarina_Bowen ( 10 / 35 )\n","Processing author Emma_Bray ( 11 / 35 )\n","Processing author Jessica_Clare ( 12 / 35 )\n","Processing author LT_Swan ( 13 / 35 )\n","Processing author Melanie_Harlow ( 14 / 35 )\n","Processing author GT_Geissinger ( 15 / 35 )\n","Processing author Stella_Rhys ( 16 / 35 )\n","Processing author Sara_Cate ( 17 / 35 )\n","Processing author KA_Linde ( 18 / 35 )\n","Processing author Alexa_Riley ( 19 / 35 )\n","Processing author JS_Scott ( 20 / 35 )\n","Processing author Diane_Alberts ( 21 / 35 )\n","Processing author Penny_Wylder ( 22 / 35 )\n","Processing author L_Steele ( 23 / 35 )\n","Processing author Anne_Melody ( 24 / 35 )\n","Processing author Jennifer_Probst ( 25 / 35 )\n","Processing author Ana_Huang ( 26 / 35 )\n","Processing author Max_Monroe ( 27 / 35 )\n","Processing author Pippa_Grant ( 28 / 35 )\n","Processing author Annika_Martin ( 29 / 35 )\n","Processing author Leslie_North ( 30 / 35 )\n","Processing author Kendra_Little ( 31 / 35 )\n","Processing author LJ_Shen ( 32 / 35 )\n","Processing author Tia_Louise ( 33 / 35 )\n","Processing author Katy_Evans ( 34 / 35 )\n","Processing author Catharina_Maura ( 35 / 35 )\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'extract_chapters_from_epubs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-af23cea3eca6>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                           \u001b[0moutput_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Author'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mauthor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Book Title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Chapter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mextract_chapters_from_epubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{rootdir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'extract_chapters_from_epubs' is not defined"]}]},{"cell_type":"code","source":["df = pd.DataFrame(output_csv)\n","print(df.head())\n","df.to_csv('/content/drive/MyDrive/processed_novels_sentences_new.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjO63qL6X4Yk","executionInfo":{"status":"ok","timestamp":1717259470300,"user_tz":-120,"elapsed":4084,"user":{"displayName":"Polina Bakhturina","userId":"07881954271877753559"}},"outputId":"a219d515-7c76-4d78-85e0-950d7d601807"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["     Author               Book Title  Chapter  \\\n","0  Ann_Cole  Mr. Mysterious In Black        1   \n","1  Ann_Cole  Mr. Mysterious In Black        1   \n","2  Ann_Cole  Mr. Mysterious In Black        1   \n","3  Ann_Cole  Mr. Mysterious In Black        1   \n","4  Ann_Cole  Mr. Mysterious In Black        1   \n","\n","                                            Sentence  \n","0                            Prologue H e was tired.  \n","1                                         Dog-tired.  \n","2  Amped up by pleasure mere minutes ago, his hea...  \n","3  Unfortunately, he was yanked back from the bec...  \n","4  Languid, he opened his eyes to the annoying re...  \n"]}]}]}