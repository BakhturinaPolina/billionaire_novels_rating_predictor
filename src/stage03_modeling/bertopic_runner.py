# -*- coding: utf-8 -*-
"""mine_bertopic_plus_octis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STfqRpfRG5lQs0TDVgMFp41LvwIavhFw
"""

# Enable fast Hugging Face downloads (must be set before importing sentence_transformers)
import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

# Set persistent cache directories for Hugging Face models
# This prevents re-downloading models on every run
cache_dir = os.path.join(os.getcwd(), "cache", "huggingface")
os.makedirs(cache_dir, exist_ok=True)
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['HF_HOME'] = cache_dir

# Load paths from config
print("[STEP 1] Starting configuration loading...")
try:
    print("[STEP 1.1] Importing Path and config utilities...")
    from pathlib import Path
    from src.common.config import load_config, resolve_path
    print("[STEP 1.2] Loading paths.yaml config...")
    paths_cfg = load_config(Path("configs/paths.yaml"))
    print(f"[STEP 1.3] paths_cfg loaded: {list(paths_cfg.keys())}")
    print("[STEP 1.4] Loading octis.yaml config...")
    octis_cfg = load_config(Path("configs/octis.yaml"))
    print(f"[STEP 1.5] octis_cfg loaded: {list(octis_cfg.keys())}")
    
    print("[STEP 1.6] Resolving dataset path...")
    dataset_path = str(resolve_path(Path(paths_cfg["inputs"]["processed_novels_sentences"])))
    print(f"[STEP 1.7] dataset_path resolved: {dataset_path}")
    print("[STEP 1.8] Resolving OCTIS dataset path...")
    octis_dataset_path = str(resolve_path(Path(paths_cfg["inputs"]["octis_dataset"])))
    print(f"[STEP 1.9] octis_dataset_path resolved: {octis_dataset_path}")
    print("[STEP 1.10] Resolving optimization results directory...")
    optimization_results_dir = str(resolve_path(Path(octis_cfg["output"]["optimization_results_dir"])))
    print(f"[STEP 1.11] optimization_results_dir resolved: {optimization_results_dir}")
    print("[STEP 1] ✓ Configuration loading completed successfully")
except Exception as e:
    # Fallback to defaults if config loading fails
    print(f"[STEP 1] ⚠️ Warning: Could not load config, using defaults: {e}")
    print("[STEP 1] Setting default paths...")
    dataset_path = "./data/processed/chapters.csv"
    print(f"[STEP 1] Default dataset_path: {dataset_path}")
    octis_dataset_path = "./data/interim/octis"
    print(f"[STEP 1] Default octis_dataset_path: {octis_dataset_path}")
    optimization_results_dir = "./data/interim/octis/optimization_results"
    print(f"[STEP 1] Default optimization_results_dir: {optimization_results_dir}")

print("[STEP 2] Importing spacy...")
import spacy

# Load the SpaCy model
print("[STEP 2.1] Loading SpaCy model 'en_core_web_sm'...")
nlp = spacy.load("en_core_web_sm")
print("[STEP 2.2] ✓ SpaCy model loaded successfully")

# Check RAPIDS availability (required)
print("[STEP 3] Checking RAPIDS availability...")
print("[STEP 3.1] Importing GPU utilities...")
from src.common.gpu_models import print_gpu_status, check_rapids_availability
from src.stage03_modeling.memory_utils import (
    get_gpu_memory_usage, print_gpu_memory_usage, cleanup_gpu_memory,
    check_memory_available, log_memory_usage, handle_oom_error, track_memory_peak,
    enforce_memory_limit, adjust_batch_size
)
from src.common.thermal_monitor import ThermalMonitor

print("[STEP 3.2] Printing GPU status...")
print_gpu_status()
print("[STEP 3.3] Checking RAPIDS availability...")
rapids_available = check_rapids_availability()
print(f"[STEP 3.4] RAPIDS available: {rapids_available}")
if not rapids_available:
    print("[STEP 3] ❌ ERROR: RAPIDS not available!")
    raise ImportError(
        "RAPIDS (cuML) is required but not available. "
        "Install with: pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
    )

print("[STEP 3.5] Importing cudf...")
import cudf
print("[STEP 3.6] ✓ RAPIDS cuDF available")
print("[STEP 3.7] Testing RAPIDS cuDF with sample Series...")
test_series = cudf.Series([1, 2, 3])
print(f"[STEP 3.8] RAPIDS Test result: {test_series}")
print("[STEP 3] ✓ RAPIDS check completed")

import subprocess
import sys
import importlib
import threading
from datetime import datetime
from pathlib import Path

# https://github.com/anderskm/gputil#monitor-gpu-in-a-separate-thread
import GPUtil
from numba import cuda
from threading import Thread
import time
import os
import pickle
import traceback
import gensim.corpora as corpora

# To monitor GPU/RAM usage
class Monitor(Thread):
    def __init__(self, delay):
        super(Monitor, self).__init__()
        self.stopped = False
        self.delay = delay  # Time between calls to GPUtil
        self.start()

    def run(self):
        while not self.stopped:
            GPUtil.showUtilization()
            time.sleep(self.delay)

    def stop(self):
        self.stopped = True


# List of required libraries with their import names if different
required_libraries = {
    "bertopic": "bertopic",
    "octis": "octis",
    "sentence-transformers": "sentence_transformers",
    "umap-learn": "umap",
    "hdbscan": "hdbscan",
    "tqdm": "tqdm",
    "pandas": "pandas",
    "gensim": "gensim",
    "scipy": "scipy"
}


# Function to install a library
def install_and_import(package, import_name=None):
    print(f"[LIB] Processing package: {package} (import_name: {import_name})")
    if import_name is None:
        import_name = package
        print(f"[LIB] Using package name as import_name: {import_name}")
    try:
        print(f"[LIB] Attempting to import {import_name}...")
        importlib.import_module(import_name)
        print(f"[LIB] ✓ Successfully imported {import_name}")
    except ImportError:
        print(f"[LIB] ⚠️ Import failed, installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"[LIB] ✓ Installation completed for {package}")
    finally:
        print(f"[LIB] Adding {import_name} to globals...")
        globals()[import_name] = importlib.import_module(import_name)
        print(f"[LIB] ✓ {import_name} added to globals")


# Install and import required libraries
print("[STEP 4] Installing and importing required libraries...")
print(f"[STEP 4.1] Total libraries to process: {len(required_libraries)}")
for idx, (package, import_name) in enumerate(required_libraries.items(), 1):
    print(f"[STEP 4.{idx+1}] Processing library {idx}/{len(required_libraries)}: {package}")
    install_and_import(package, import_name)
    print(f"[STEP 4.{idx+1}] ✓ Completed: {package}")
print("[STEP 4] ✓ All required libraries processed")

# Import libs

import pandas as pd
import re
import numpy as np
from tqdm import tqdm
import csv
import json
import pprint

from skopt.space.space import Real, Categorical, Integer

from scipy.sparse import csr_matrix

# from umap import UMAP
# from hdbscan import HDBSCAN

from cuml.cluster import HDBSCAN
from cuml.manifold import UMAP

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech
from bertopic.vectorizers import ClassTfidfTransformer

from octis.optimization.optimizer import Optimizer
from octis.evaluation_metrics.diversity_metrics import TopicDiversity
from octis.evaluation_metrics.coherence_metrics import Coherence
from octis.dataset.dataset import Dataset
from octis.models.model import AbstractModel

# Import BERTopicOctisModelWithEmbeddings from stage03_modeling
from src.stage03_modeling.bertopic_octis_model import (
    BERTopicOctisModelWithEmbeddings,
    load_embedding_model
)

import pandas as pd
import csv
import re

import gc
import torch

with torch.no_grad():
    try:

        # Dataset paths (already loaded from config at module level)
        # dataset_path and octis_dataset_path are set above


        # Initialize variables
        print("[STEP 5] Initializing data loading...")
        print("[STEP 5.1] Initializing all_rows list...")
        all_rows = []
        print(f"[STEP 5.2] all_rows initialized (length: {len(all_rows)})")

        # Read the file using the csv module to handle quoting correctly
        print(f"[STEP 5.3] Opening dataset file: {dataset_path}")
        print("[STEP 5.4] Reading CSV with latin1 encoding...")
        row_count = 0
        with open(dataset_path, 'r', encoding='latin1', errors='ignore') as file:
            print("[STEP 5.5] Creating CSV reader...")
            reader = csv.reader(file, quotechar='"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True)
            print("[STEP 5.6] Reading headers...")
            headers = next(reader)  # Get the headers
            print(f"[STEP 5.7] Headers read: {headers}")
            print("[STEP 5.8] Reading rows...")
            for row_idx, row in enumerate(reader):
                row_count += 1
                if row_idx % 10000 == 0 and row_idx > 0:
                    print(f"[STEP 5.8] Processed {row_idx:,} rows, valid rows: {len(all_rows):,}")
                if len(row) == 4:
                    all_rows.append(row)
            print(f"[STEP 5.9] Total rows read: {row_count:,}, valid rows (len==4): {len(all_rows):,}")

        # Convert all rows to DataFrame
        print("[STEP 6] Converting rows to DataFrame...")
        print(f"[STEP 6.1] Creating DataFrame with {len(all_rows):,} rows and {len(headers)} columns...")
        df = pd.DataFrame(all_rows, columns=headers)
        print(f"[STEP 6.2] ✓ DataFrame created: shape {df.shape}")
        print(f"[STEP 6.3] DataFrame columns: {list(df.columns)}")

        # Logging the number of all rows
        print(f"[STEP 6.4] Total lines (including header): {len(all_rows) + 1}")
        print(f"[STEP 6.5] All rows count: {len(all_rows):,}")

        # Ensure 'Sentence' column exists and is correctly named
        print("[STEP 7] Checking for 'Sentence' column...")
        if 'Sentence' not in df.columns:
            print("[STEP 7] ❌ ERROR: The 'Sentence' column does not exist. Check column names and separator.")
            print(f"[STEP 7] Available columns: {list(df.columns)}")
        else:
            print("[STEP 7] ✓ 'Sentence' column found")
            # Cleaning the 'Sentence' column
            print("[STEP 8] Cleaning 'Sentence' column...")
            print("[STEP 8.1] Removing newlines...")
            df['Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'\n+', ' ', str(x)) if isinstance(x, str) else str(x))
            print("[STEP 8.2] Normalizing whitespace and converting to lowercase...")
            df['Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'\s+', ' ', x).strip().lower())
            print("[STEP 8.3] ✓ Sentence cleaning completed")

            # Print the cleaned data to verify
            print("[STEP 9] Displaying sample of cleaned data...")
            print(df.head())
            print("[STEP 9] ✓ Sample data displayed")

            # Get sentences as list
            print("[STEP 10] Converting DataFrame to list of strings...")
            dataset_as_list_of_strings = df['Sentence'].tolist()
            print(f"[STEP 10.1] ✓ Converted to list: {len(dataset_as_list_of_strings):,} sentences")
            print(f"[STEP 10.2] Total sentences in dataset: {df.shape[0]:,}")

            # Debug: Check for duplicates before converting to lists
            print("[STEP 11] Checking for duplicate sentences...")
            duplicate_sentences = df['Sentence'].duplicated().sum()
            print(f"[STEP 11.1] Number of duplicate sentences before conversion: {duplicate_sentences:,} ({duplicate_sentences/len(df)*100:.2f}%)")

            # Convert sentences to list of lists
            print("[STEP 12] Tokenizing sentences (converting to list of lists)...")
            print("[STEP 12.1] Starting tokenization...")
            dataset_as_list_of_lists = []
            for idx, sentence in enumerate(dataset_as_list_of_strings):
                if idx % 10000 == 0 and idx > 0:
                    print(f"[STEP 12.1] Tokenized {idx:,}/{len(dataset_as_list_of_strings):,} sentences...")
                dataset_as_list_of_lists.append(sentence.split())
            print(f"[STEP 12.2] ✓ Tokenization completed: {len(dataset_as_list_of_lists):,} tokenized sentences")

            # Debug: Check for duplicates after conversion
            print("[STEP 13] Checking duplicates after conversion...")
            print("[STEP 13.1] Reconstructing strings from tokenized lists...")
            dataset_as_strings_from_lists = [' '.join(sentence) for sentence in dataset_as_list_of_lists]
            print(f"[STEP 13.2] Reconstructed {len(dataset_as_strings_from_lists):,} strings")
            duplicates_after_conversion = len(dataset_as_strings_from_lists) - len(set(dataset_as_strings_from_lists))
            print(f"[STEP 13.3] Number of duplicate sentences after conversion: {duplicates_after_conversion:,}")

        # Prepare the dataset for OCTIS
        # https://github.com/MIND-Lab/OCTIS?tab=readme-ov-file#load-a-custom-dataset
        print("[STEP 14] Preparing dataset for OCTIS...")
        print("[STEP 14.1] Constructing OCTIS corpus path...")
        # Save the properly formatted dataset to a file, for OCTIS to import it
        octis_corpus_path = octis_dataset_path + '/corpus.tsv'
        print(f"[STEP 14.2] OCTIS corpus path: {octis_corpus_path}")

        # Read the CSV file
        print("[STEP 15] Reading CSV file for OCTIS format conversion...")
        print(f"[STEP 15.1] Opening file: {dataset_path}")
        with open(dataset_path, mode='r', newline='', encoding='latin1') as csv_file:
            print("[STEP 15.2] Creating CSV reader...")
            csv_reader = csv.reader(csv_file)
            print("[STEP 15.3] Reading header...")
            header = next(csv_reader)  # Skip the header
            print(f"[STEP 15.4] Header: {header}")

            # Prepare the data for the TSV file
            print("[STEP 15.5] Initializing tsv_data list...")
            tsv_data = []
            print("[STEP 15.6] Processing rows for TSV format...")
            tsv_row_count = 0
            for row_idx, row in enumerate(csv_reader):
                if row_idx % 10000 == 0 and row_idx > 0:
                    print(f"[STEP 15.6] Processed {row_idx:,} rows, valid TSV rows: {len(tsv_data):,}")
                if len(row) == 4:
                    author, book_title, chapter, sentence = row
                    partition = 'train'
                    label = author + "," + book_title
                    tsv_data.append([sentence, partition, label])
                    tsv_row_count += 1
            print(f"[STEP 15.7] ✓ Processed {row_idx+1:,} rows, created {len(tsv_data):,} TSV entries")

        # Write the data to a TSV file
        print("[STEP 16] Writing TSV file...")
        print(f"[STEP 16.1] Opening output file: {octis_corpus_path}")
        with open(octis_corpus_path, mode='w', newline='', encoding='utf-8') as tsv_file:
            print("[STEP 16.2] Creating TSV writer...")
            tsv_writer = csv.writer(tsv_file, delimiter='\t')
            print("[STEP 16.3] Writing {len(tsv_data):,} rows to TSV...")
            for idx, row in enumerate(tsv_data):
                if idx % 10000 == 0 and idx > 0:
                    print(f"[STEP 16.3] Written {idx:,}/{len(tsv_data):,} rows...")
                tsv_writer.writerow(row)
            print(f"[STEP 16.4] ✓ TSV file written: {len(tsv_data):,} rows")

        print("[STEP 17] Loading OCTIS dataset...")
        print("[STEP 17.1] Creating Dataset instance...")
        octis_dataset = Dataset()
        print("[STEP 17.2] Loading custom dataset from folder...")
        print(f"[STEP 17.3] Dataset folder: {octis_dataset_path}")
        octis_dataset.load_custom_dataset_from_folder(octis_dataset_path)
        print("[STEP 17.4] ✓ OCTIS dataset loaded successfully")

        import spacy
        from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance

        print("[STEP 18] Creating representation models...")
        # Additional Representations
        print("[STEP 18.1] Creating KeyBERTInspired model...")
        keybert_model = KeyBERTInspired()
        print("[STEP 18.2] ✓ KeyBERTInspired created")

        # Part-of-Speech
        print("[STEP 18.3] Creating PartOfSpeech model with 'en_core_web_sm'...")
        pos_model = PartOfSpeech("en_core_web_sm")
        print("[STEP 18.4] ✓ PartOfSpeech created")

        # MMR
        print("[STEP 18.5] Creating MaximalMarginalRelevance model (diversity=0.3)...")
        mmr_model = MaximalMarginalRelevance(diversity=0.3)
        print("[STEP 18.6] ✓ MaximalMarginalRelevance created")

        # All representation models
        print("[STEP 18.7] Combining representation models into dictionary...")
        representation_model = {
            "KeyBERT": keybert_model,
            "MMR": mmr_model,
            "POS": pos_model
        }
        print(f"[STEP 18.8] ✓ Representation models dictionary created with {len(representation_model)} models")

        # Implementing custom Model for OCTIS
        # https://github.com/MIND-Lab/OCTIS?tab=readme-ov-file#implement-your-own-model

        print("[STEP 19] Initializing training tracking variables...")
        trainings_count = 0
        print(f"[STEP 19.1] trainings_count initialized: {trainings_count}")
        training_errors = []
        print(f"[STEP 19.2] training_errors list initialized: {len(training_errors)} errors")
        print("[STEP 19.3] ✓ Training tracking initialized")

        print("[STEP 20] Using BERTopicOctisModelWithEmbeddings class...")
        # Note: BERTopicOctisModelWithEmbeddings is imported from src.stage03_modeling.bertopic_octis_model
        # This removes code duplication and ensures consistency across the codebase.
        # The class is now located in the stage03_modeling module since it's specific to this stage.
        print("[STEP 20] ✓ Using shared BERTopicOctisModelWithEmbeddings class")
        
        # Remove old class definition - it's now in src.common.bertopic_octis_model
        # All old class methods (__init__, train_model, set_hyperparameters) have been removed


        print("[STEP 21] Setting up embedding models...")
        # SANITY CHECK: Using smaller model for full dataset test
        # Changed from all-MiniLM-L12-v2 and multi-qa-mpnet-base-cos-v1 to smaller paraphrase-MiniLM-L6-v2
        embedding_model_names = [
            "paraphrase-MiniLM-L6-v2",  # Smaller model for sanity check with full dataset
            # "paraphrase-mpnet-base-v2",
            # "all-MiniLM-L12-v2",  # Original model - uncomment for production
            # "paraphrase-distilroberta-base-v1",
            # "whaleloops/phrase-bert",
            # "multi-qa-mpnet-base-cos-v1"  # Original model - uncomment for production
        ]
        print(f"[STEP 21.1] Embedding model names: {embedding_model_names}")
        print(f"[STEP 21.2] Total models to load: {len(embedding_model_names)}")
        print(f"[STEP 21.2.1] ⚠️ SANITY CHECK MODE: Using smaller model (paraphrase-MiniLM-L6-v2) for full dataset test")

        embedding_models = []
        print("[STEP 21.3] Initializing embedding_models list...")

        for idx, embedding_model_name in enumerate(embedding_model_names, 1):
            print(f"[STEP 21.{idx+3}] Loading embedding model {idx}/{len(embedding_model_names)}: {embedding_model_name}")
            # Use shared load_embedding_model to ensure models are downloaded and cached locally
            embedding_model = load_embedding_model(embedding_model_name, device="cuda", cache_folder=cache_dir)
            print(f"[STEP 21.{idx+3}] ✓ Model loaded and cached locally: {embedding_model_name}")

            print(f"[STEP 21.{idx+3}] Appending to embedding_models list...")
            embedding_models.append(embedding_model)
            print(f"[STEP 21.{idx+3}] ✓ Model added (total: {len(embedding_models)})")
        print(f"[STEP 21] ✓ All {len(embedding_models)} embedding models loaded")

        # Store the pre-calculated embeddings
        # https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2#scrollTo=sVfnYtUaxyLT
        print("[STEP 22] Initializing precalculated_embeddings list...")
        precalculated_embeddings = []
        print(f"[STEP 22.1] precalculated_embeddings initialized (length: {len(precalculated_embeddings)})")


        def save_embeddings(embeddings, file_path):
            """
            Save embeddings using numpy native format (much faster than pickle).
            Automatically changes extension from .pkl to .npy.
            Supports both single arrays and lists of arrays.

            Args:
            embeddings (numpy.ndarray or list of numpy.ndarray): Embedding array(s) to save
            file_path (str): Path to save the embeddings
            """
            # Change extension from .pkl to .npy automatically
            file_path = file_path.replace('.pkl', '.npy')
            print(f"[SAVE_EMB] Saving embeddings to: {file_path}")
            print(f"[SAVE_EMB] Embeddings type: {type(embeddings)}")
            
            # Handle list of embeddings (preserve structure for compatibility)
            if isinstance(embeddings, list):
                print(f"[SAVE_EMB] Embeddings is list with {len(embeddings)} items")
                if len(embeddings) == 1:
                    # Single item list - save as array for efficiency
                    embeddings = np.array(embeddings[0])
                    print(f"[SAVE_EMB] Converted single-item list to array, shape: {embeddings.shape}")
                else:
                    # Multiple embeddings - numpy.save can handle lists, but for large files
                    # it's better to save as a single array if shapes match
                    # For now, save the list structure (numpy handles this)
                    print(f"[SAVE_EMB] Saving list of {len(embeddings)} embedding arrays")
                    for idx, emb in enumerate(embeddings):
                        if hasattr(emb, 'shape'):
                            print(f"[SAVE_EMB]   Item {idx}: shape {emb.shape}")
            elif not isinstance(embeddings, np.ndarray):
                print(f"[SAVE_EMB] Converting to numpy array...")
                embeddings = np.array(embeddings)
            
            if hasattr(embeddings, 'shape'):
                print(f"[SAVE_EMB] Final embeddings shape: {embeddings.shape}")
            
            print(f"[SAVE_EMB] Saving with numpy native format (fast)...")
            np.save(file_path, embeddings)
            file_size = os.path.getsize(file_path) / (1024**2)
            print(f"[SAVE_EMB] ✓ Embeddings saved to {file_path} ({file_size:.2f} MB)")


        def load_embeddings(file_path):
            """
            Load embeddings from numpy file.
            Supports memory mapping for instant loading without reading full file to RAM.
            Handles both single arrays and lists (for backward compatibility).

            Args:
            file_path (str): Path to the saved embeddings file

            Returns:
            numpy.ndarray or list: Loaded embeddings
            """
            # Try both .npy and .pkl extensions for backward compatibility
            npy_path = file_path.replace('.pkl', '.npy')
            pkl_path = file_path
            
            # Check for .npy file first (preferred)
            if os.path.exists(npy_path):
                file_path = npy_path
                print(f"[LOAD_EMB] Loading embeddings from: {file_path}")
                file_size = os.path.getsize(file_path) / (1024**2)
                print(f"[LOAD_EMB] File exists, size: {file_size:.2f} MB")
                # Use memory mapping for large files - instant loading without full RAM read
                print(f"[LOAD_EMB] Loading with memory mapping (fast, efficient)...")
                embeddings = np.load(file_path, mmap_mode='r')
                # Memory-mapped array - efficient, only loads data when accessed
                if hasattr(embeddings, 'shape'):
                    print(f"[LOAD_EMB] ✓ Embeddings loaded via Memory Map. Shape: {embeddings.shape}")
                else:
                    # Could be a list or other structure (unlikely with .npy)
                    print(f"[LOAD_EMB] ✓ Embeddings loaded. Type: {type(embeddings)}")
                return embeddings
            # Fallback to .pkl for backward compatibility
            elif os.path.exists(pkl_path):
                print(f"[LOAD_EMB] Loading embeddings from legacy pickle file: {pkl_path}")
                file_size = os.path.getsize(pkl_path) / (1024**2)
                print(f"[LOAD_EMB] File exists, size: {file_size:.2f} MB")
                print(f"[LOAD_EMB] ⚠️ Using legacy pickle format (slower). Consider converting to .npy")
                with open(pkl_path, 'rb') as f:
                    embeddings = pickle.load(f)
                if isinstance(embeddings, list):
                    print(f"[LOAD_EMB] Loaded list with {len(embeddings)} items")
                    for idx, emb in enumerate(embeddings):
                        if hasattr(emb, 'shape'):
                            print(f"[LOAD_EMB]   Item {idx}: shape {emb.shape}")
                elif hasattr(embeddings, 'shape'):
                    print(f"[LOAD_EMB] Loaded array with shape: {embeddings.shape}")
                print(f"[LOAD_EMB] ✓ Embeddings loaded from {pkl_path}")
                return embeddings
            else:
                print(f"[LOAD_EMB] ⚠️ No saved embeddings found at {file_path} (checked .npy and .pkl)")
                return None


        # Example usage
        print("[STEP 23] Setting up embedding file path...")
        embedding_file = "precalculated_embeddings.npy"  # Changed from .pkl to .npy for faster I/O
        print(f"[STEP 23.1] Embedding file: {embedding_file}")

        # BERTopic works by converting documents into numerical values, called embeddings.
        # This process can be very costly, especially if we want to iterate over parameters.
        # Instead, we can calculate those embeddings once and feed them to BERTopic
        # to skip calculating embeddings each time.
        print("[STEP 24] Processing embedding models for pre-calculation...")
        print(f"[STEP 24.1] Total embedding models: {len(embedding_models)}")
        print(f"[STEP 24.2] Total documents to encode: {len(dataset_as_list_of_strings):,}")
        
        for idx, embedding_model in enumerate(embedding_models, 1):
            print(f"[STEP 24.{idx+2}] Processing embedding model {idx}/{len(embedding_models)}")
            # Check if embeddings already exist
            print(f"[STEP 24.{idx+2}] Checking if embedding file exists: {embedding_file}")
            if os.path.exists(embedding_file):
                print(f"[STEP 24.{idx+2}] ✓ Embedding file exists, loading...")
                precalculated_embeddings = load_embeddings(embedding_file)
                print(f"[STEP 24.{idx+2}] ✓ Loaded embeddings, breaking loop")
                break
            else:
                print(f"[STEP 24.{idx+2}] Embedding file does not exist, encoding...")
                print(f"[STEP 24.{idx+2}] Starting encoding with {len(dataset_as_list_of_strings):,} documents...")
                print(f"[STEP 24.{idx+2}] This may take several minutes...")
                
                # GPU-optimized batch size (increased for faster processing)
                if torch.cuda.is_available():
                    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
                    if gpu_memory_gb >= 8:
                        initial_batch_size = 512  # Aggressive batch size for 8GB+ GPUs
                    elif gpu_memory_gb >= 6:
                        initial_batch_size = 384
                    else:
                        initial_batch_size = 256
                    
                    # Dynamically adjust batch size based on available memory
                    # Estimate: ~0.01 GB per batch for embedding encoding
                    batch_size = adjust_batch_size(initial_batch_size, required_memory_per_batch_gb=0.01)
                    print(f"[STEP 24.{idx+2}] Using GPU-optimized batch size: {batch_size} (GPU: {gpu_memory_gb:.1f}GB)")
                    if batch_size != initial_batch_size:
                        print(f"[STEP 24.{idx+2}]   (Adjusted from {initial_batch_size} based on available memory)")
                    
                    # Enable FP16 for 2x speedup
                    try:
                        embedding_model = embedding_model.half()
                        print(f"[STEP 24.{idx+2}] ✅ FP16 (half precision) enabled - 2x faster")
                    except Exception as e:
                        print(f"[STEP 24.{idx+2}] ⚠️ Could not enable FP16: {e}")
                else:
                    batch_size = 32  # CPU batch size
                    print(f"[STEP 24.{idx+2}] Using CPU batch size: {batch_size}")
                
                import time
                start_time = time.time()
                embeddings = embedding_model.encode(
                    dataset_as_list_of_strings, 
                    show_progress_bar=True,
                    batch_size=batch_size,
                    convert_to_numpy=True,
                    normalize_embeddings=False,
                    device="cuda" if torch.cuda.is_available() else "cpu"
                )
                elapsed_time = time.time() - start_time
                print(f"[STEP 24.{idx+2}] ⚡ Encoding completed in {elapsed_time/60:.1f} minutes")
                print(f"[STEP 24.{idx+2}] Throughput: {len(dataset_as_list_of_strings)/elapsed_time:.0f} sentences/second")
                print(f"[STEP 24.{idx+2}] ✓ Encoding completed, shape: {embeddings.shape}")
                print(f"[STEP 24.{idx+2}] Appending to precalculated_embeddings...")
                precalculated_embeddings.append(embeddings)
                print(f"[STEP 24.{idx+2}] ✓ Added embeddings (total: {len(precalculated_embeddings)})")

        # Save embeddings after calculation (if they didn't exist before)
        print("[STEP 25] Checking if embeddings need to be saved...")
        if not os.path.exists(embedding_file):
            print("[STEP 25.1] Embedding file does not exist, saving...")
            save_embeddings(precalculated_embeddings, embedding_file)
            print("[STEP 25.2] ✓ Embeddings saved")
        else:
            print("[STEP 25.1] Embedding file already exists, skipping save")
        print(f"[STEP 25] ✓ Embedding processing completed, total embeddings: {len(precalculated_embeddings)}")

        def optimize_hyperparameters(model, label, dataset_as_list_of_lists):
            print(f"[OPTIMIZE] ========== Starting optimize_hyperparameters() ==========")
            print(f"[OPTIMIZE] Model label: {label}")
            print(f"[OPTIMIZE] Dataset size: {len(dataset_as_list_of_lists):,} documents")
            
            print("[OPTIMIZE] Defining search space...")
            search_space = {
                'umap__n_neighbors': Integer(2, 50),
                'umap__n_components': Integer(2, 10),
                'umap__min_dist': Real(0.0, 0.1),
                'hdbscan__min_cluster_size': Integer(50, 500),
                'hdbscan__min_samples': Integer(10, 100),
                'vectorizer__min_df': Real(0.001, 0.01),
                'bertopic__top_n_words': Integer(10, 40),
                'bertopic__min_topic_size': Integer(10, 250)
            }
            print(f"[OPTIMIZE] ✓ Search space defined with {len(search_space)} parameters")
            for key, value in search_space.items():
                print(f"[OPTIMIZE]   {key}: {value}")

            # As a rule of thumb, if you have N hyperparameters to optimize, then you should make at least 15 times N iterations.
            # https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb#scrollTo=njjkNjl9CJW8&line=1&uniqifier=1
            print("[OPTIMIZE] Calculating optimization runs...")
            optimization_runs = len(search_space) * 15 # should be 15
            print(f"[OPTIMIZE] optimization_runs: {optimization_runs} (15 × {len(search_space)} parameters)")

            # Topic models are usually probabilistic and thus produce different results even with the same hyperparameter configuration.
            # So we run the model multiple times and then take the median of the evaluated metric to get a more reliable result.
            model_runs = 1 # 1 is enough
            print(f"[OPTIMIZE] model_runs: {model_runs}")

            topk = 10
            print(f"[OPTIMIZE] topk: {topk}")

            print("[OPTIMIZE] Filtering empty documents...")
            docs_before = len(dataset_as_list_of_lists)
            dataset_as_list_of_lists = [doc for doc in dataset_as_list_of_lists if len(doc) > 0]
            docs_after = len(dataset_as_list_of_lists)
            print(f"[OPTIMIZE] Documents before filter: {docs_before:,}, after: {docs_after:,}")

            # Use a different coherence measure
            print("[OPTIMIZE] Creating Coherence metric (c_v)...")
            npmi = Coherence(texts=dataset_as_list_of_lists, topk=topk, measure='c_v')
            print("[OPTIMIZE] ✓ Coherence metric created")
            print("[OPTIMIZE] Creating TopicDiversity metric...")
            diversity = TopicDiversity(topk=10)  # Initialize metric
            print("[OPTIMIZE] ✓ TopicDiversity metric created")

            print("[OPTIMIZE] Creating Optimizer instance...")
            optimizer = Optimizer()
            print("[OPTIMIZE] ✓ Optimizer created")

            print("[OPTIMIZE] Constructing results path...")
            results_path = os.path.join(optimization_results_dir, label) + '/'
            print(f"[OPTIMIZE] Results path: {results_path}")
            result_json_path = results_path + 'result.json'
            print(f"[OPTIMIZE] Result JSON path: {result_json_path}")

            # Check if result file exists to resume, otherwise start new optimization
            optimization_result = None
            if os.path.isfile(result_json_path):
                print("[OPTIMIZE] Result file found - attempting to resume optimization...")
                print(f"[OPTIMIZE] Loading from: {result_json_path}")
                try:
                    optimization_result = optimizer.resume_optimization(result_json_path)
                    print("[OPTIMIZE] ✓ Optimization resumed/completed")
                    print(f"[OPTIMIZE] ========== optimize_hyperparameters() completed ==========")
                    return optimization_result
                except (FileNotFoundError, ImportError, AttributeError, ModuleNotFoundError) as e:
                    print(f"[OPTIMIZE] ⚠️ Could not resume optimization: {e}")
                    print("[OPTIMIZE] This may be due to:")
                    print("  - Model class path mismatch (custom model class)")
                    print("  - Missing model class file")
                    print("  - Incompatible saved optimization state")
                    print("[OPTIMIZE] Starting new optimization instead...")
                    # Remove the problematic result file to avoid this issue in future runs
                    print(f"[OPTIMIZE] Removing problematic result file: {result_json_path}")
                    try:
                        os.remove(result_json_path)
                        print("[OPTIMIZE] ✓ Result file removed")
                    except Exception as rm_e:
                        print(f"[OPTIMIZE] ⚠️ Could not remove result file: {rm_e}")
                    # Fall through to start new optimization
                    optimization_result = None
            
            # Start new optimization (either no result file, or resume failed)
            if optimization_result is None:
                print("[OPTIMIZE] Starting new optimization...")
                print(f"[OPTIMIZE] Results will be saved to: {results_path}")
                # Ensure results directory exists
                os.makedirs(results_path, exist_ok=True)
                optimization_result = optimizer.optimize(
                    model, octis_dataset, npmi, search_space, number_of_call=optimization_runs,
                    model_runs=model_runs, save_models=True,
                    extra_metrics=[diversity],  # to keep track of other metrics
                    save_path=results_path
                )
                print("[OPTIMIZE] ✓ New optimization completed")
            
            print(f"[OPTIMIZE] ========== optimize_hyperparameters() completed ==========")
            return optimization_result

        # @title
        # Original Code!
        optimization_results = []

        # for index, embedding_model in enumerate(embedding_models):
        #     embedding_model_name = embedding_model_names[index]
        #     print("Instantiating BERTopicOctisModelWithEmbeddings for ", embedding_model_name)
        #     model = BERTopicOctisModelWithEmbeddings(embedding_model=embedding_model,
        #                                              embeddings=precalculated_embeddings[index],
        #                                              dataset_as_list_of_strings=dataset_as_list_of_strings)
        #
        #     print("Optimizing hyperparameters for ", embedding_model_name)
        #     optimization_result = optimize_hyperparameters(model, embedding_model_name)
        #     optimization_results.append(optimization_result)
        #     print("=== Optimized hyperparameters for embedding_model_name:", embedding_model_name)

        optimization_errors = []
        
        # Initialize thermal monitoring
        print("[STEP 25.5] Initializing thermal monitoring...")
        thermal_monitor = ThermalMonitor(alert_cpu=85.0, alert_gpu=80.0)
        thermal_status = thermal_monitor.get_status()
        print("[STEP 25.6] Thermal status check:")
        if thermal_status['cpu_temp'] is not None:
            print(f"   CPU temperature: {thermal_status['cpu_temp']:.1f}°C")
        if thermal_status['gpu_temp'] is not None:
            print(f"   GPU temperature: {thermal_status['gpu_temp']:.1f}°C")
        if thermal_status['alerts']:
            print("   ⚠️ WARNING: High temperatures detected:")
            for alert in thermal_status['alerts']:
                print(f"      {alert}")
            print("   Consider waiting for cooling or reducing workload")
        else:
            print("   ✓ Temperatures within safe range")
        
        # Initialize GPU memory monitoring
        print("[STEP 25.7] Checking initial GPU memory...")
        initial_memory = print_gpu_memory_usage("Initial GPU Memory", verbose=True)
        
        # Set up memory logging file
        memory_log_file = Path("logs/memory_usage.jsonl")
        memory_log_file.parent.mkdir(parents=True, exist_ok=True)
        print(f"[STEP 25.7.1] Memory log file: {memory_log_file}")
        log_memory_usage(memory_log_file, "Initial GPU Memory")
        
        # Check memory limit before starting (require at least 2 GB free)
        print("[STEP 25.7.2] Checking memory availability...")
        memory_sufficient, mem_info = enforce_memory_limit(required_gb=2.0, abort_on_insufficient=False)
        if not memory_sufficient:
            print("[STEP 25.7.2] ⚠️ WARNING: Low GPU memory - optimization may fail or be slow")
        
        # Enable GPU resource monitoring (GPUtil)
        print("[STEP 25.8] Enabling GPU resource monitoring...")
        monitor = Monitor(10)  # Monitor every 10 seconds
        print("[STEP 25.9] ✓ GPU monitoring enabled (will run in background)")
        
        # Start background thermal monitoring
        print("[STEP 25.10] Starting background thermal monitoring...")
        thermal_log_file = Path("logs/thermal_history.jsonl")
        thermal_log_file.parent.mkdir(parents=True, exist_ok=True)
        print(f"[STEP 25.10.1] Thermal log file: {thermal_log_file}")
        
        # Create a thread-safe way to stop thermal monitoring
        thermal_monitoring_active = threading.Event()
        thermal_monitoring_active.set()  # Start as active
        
        def thermal_monitor_thread():
            """Background thread for thermal monitoring."""
            try:
                while thermal_monitoring_active.is_set():
                    status = thermal_monitor.get_status()
                    
                    # Log to file
                    timestamp = datetime.now().isoformat()
                    log_entry = {
                        'timestamp': timestamp,
                        'status': status
                    }
                    with open(thermal_log_file, 'a') as f:
                        f.write(json.dumps(log_entry) + '\n')
                    
                    # Check for throttling
                    if status.get('throttling'):
                        print(f"⚠️ Thermal throttling detected: {status['throttling'][:80]}")
                    
                    # Sleep for interval (30 seconds)
                    thermal_monitoring_active.wait(30.0)
            except Exception as e:
                print(f"⚠️ Thermal monitoring thread error: {e}")
        
        thermal_thread = threading.Thread(target=thermal_monitor_thread, daemon=True)
        thermal_thread.start()
        print("[STEP 25.10.2] ✓ Background thermal monitoring started (30s interval)")


        def optimize_with_restart(model, embedding_model_name, dataset_as_list_of_lists, max_retries=3, delay=5):
            """
            Attempts to run optimize_hyperparameters and restarts if an exception occurs.
            Includes memory cleanup and OOM error handling.

            :param model: The model to optimize
            :param embedding_model_name: Name of the embedding model
            :param dataset_as_list_of_lists: The dataset
            :param max_retries: Maximum number of retry attempts
            :param delay: Delay in seconds between retries
            :return: The optimization result or None if all attempts fail
            """
            print(f"[OPT_RESTART] ========== Starting optimize_with_restart() ==========")
            print(f"[OPT_RESTART] Model: {embedding_model_name}")
            print(f"[OPT_RESTART] Max retries: {max_retries}")
            print(f"[OPT_RESTART] Delay between retries: {delay} seconds")
            
            # Check memory before optimization
            print(f"[OPT_RESTART] Checking GPU memory before optimization...")
            mem_before = print_gpu_memory_usage("Pre-optimization Memory", verbose=True)
            log_memory_usage(memory_log_file, f"Pre-optimization: {embedding_model_name}")
            
            # Enforce memory limit before optimization
            memory_sufficient, _ = enforce_memory_limit(required_gb=1.5, abort_on_insufficient=False)
            if not memory_sufficient:
                print(f"[OPT_RESTART] ⚠️ WARNING: Low memory before optimization - may fail")
            
            for attempt in range(max_retries):
                print(f"[OPT_RESTART] Attempt {attempt + 1} of {max_retries}")
                
                # Cleanup memory before each attempt (except first)
                if attempt > 0:
                    print(f"[OPT_RESTART] Cleaning up GPU memory before retry...")
                    cleanup_gpu_memory(verbose=True)
                
                try:
                    print(f"[OPT_RESTART] Calling optimize_hyperparameters()...")
                    
                    # Track memory peak during optimization
                    with track_memory_peak(f"Optimization: {embedding_model_name}"):
                        optimization_result = optimize_hyperparameters(model, embedding_model_name,
                                                                       dataset_as_list_of_lists)
                    
                    print(f"[OPT_RESTART] ✓ Optimization completed successfully on attempt {attempt + 1}")
                    
                    # Check memory after optimization
                    print(f"[OPT_RESTART] Checking GPU memory after optimization...")
                    mem_after = print_gpu_memory_usage("Post-optimization Memory", verbose=True)
                    log_memory_usage(memory_log_file, f"Post-optimization: {embedding_model_name}")
                    
                    print(f"[OPT_RESTART] ========== optimize_with_restart() completed successfully ==========")
                    return optimization_result
                    
                except RuntimeError as e:
                    # Check if it's an OOM error
                    if "out of memory" in str(e).lower():
                        print(f"[OPT_RESTART] ❌ CUDA Out of Memory error (Attempt {attempt + 1}):")
                        oom_recovery = handle_oom_error(e, current_batch_size=None)
                        
                        if not oom_recovery['recovered']:
                            print(f"[OPT_RESTART] ❌ Cannot recover from OOM - insufficient memory")
                            if attempt < max_retries - 1:
                                print(f"[OPT_RESTART] Will retry after cleanup...")
                            else:
                                print(f"[OPT_RESTART] ❌ Max retries reached. Optimization failed.")
                    else:
                        print(f"[OPT_RESTART] ❌ RuntimeError (Attempt {attempt + 1}): {e}")
                    
                    if attempt < max_retries - 1:
                        print(f"[OPT_RESTART] Restarting in {delay} seconds...")
                        time.sleep(delay)
                        print(f"[OPT_RESTART] ✓ Delay completed, retrying...")
                    else:
                        print(f"[OPT_RESTART] ❌ Max retries reached. Optimization failed.")
                        
                except Exception as e:
                    print(f"[OPT_RESTART] ❌ An error occurred during optimization (Attempt {attempt + 1}):")
                    print(f"[OPT_RESTART] Error type: {type(e).__name__}")
                    print(f"[OPT_RESTART] Error message: {e}")
                    print("[OPT_RESTART] Full traceback:")
                    print(traceback.format_exc())
                    
                    if attempt < max_retries - 1:
                        print(f"[OPT_RESTART] Restarting in {delay} seconds...")
                        # Cleanup before retry
                        cleanup_gpu_memory(verbose=True)
                        time.sleep(delay)
                        print(f"[OPT_RESTART] ✓ Delay completed, retrying...")
                    else:
                        print(f"[OPT_RESTART] ❌ Max retries reached. Optimization failed.")

            print(f"[OPT_RESTART] ❌ All {max_retries} attempts failed")
            print(f"[OPT_RESTART] ========== optimize_with_restart() failed ==========")
            return None

        print("[STEP 26] Starting optimization loop for embedding models...")
        print(f"[STEP 26.1] Total embedding models to process: {len(embedding_models)}")
        
        for index, embedding_model in enumerate(embedding_models):
            print(f"[STEP 26.{index+2}] ========== Processing embedding model {index+1}/{len(embedding_models)} ==========")
            try:
                embedding_model_name = embedding_model_names[index]
                print(f"[STEP 26.{index+2}] Embedding model name: {embedding_model_name}")
                print(f"[STEP 26.{index+2}] Instantiating BERTopicOctisModelWithEmbeddings for {embedding_model_name}...")
                print(f"[STEP 26.{index+2}] Using embeddings index: {index}")
                print(f"[STEP 26.{index+2}] Embeddings shape: {precalculated_embeddings[index].shape if hasattr(precalculated_embeddings[index], 'shape') else type(precalculated_embeddings[index])}")
                model = BERTopicOctisModelWithEmbeddings(
                    embedding_model=embedding_model,
                    embedding_model_name=embedding_model_names[index],
                    embeddings=precalculated_embeddings[index],
                    dataset_as_list_of_strings=dataset_as_list_of_strings,
                    optimization_results_dir=optimization_results_dir,
                    verbose=True
                )
                print(f"[STEP 26.{index+2}] ✓ Model instantiated")

                print(f"[STEP 26.{index+2}] Optimizing hyperparameters for {embedding_model_name}...")
                optimization_result = optimize_with_restart(model, embedding_model_name, dataset_as_list_of_lists)
                # optimization_results.append(optimization_result)
                print(f"[STEP 26.{index+2}] === Optimized hyperparameters for embedding_model_name: {embedding_model_name} ===")

                # Enhanced cleanup with memory management
                print(f"[STEP 26.{index+2}] Cleaning up model and optimization_result...")
                del model
                print(f"[STEP 26.{index+2}] ✓ Model deleted")
                del optimization_result
                print(f"[STEP 26.{index+2}] ✓ Optimization result deleted")
                
                # Comprehensive GPU memory cleanup
                print(f"[STEP 26.{index+2}] Performing GPU memory cleanup...")
                cleanup_result = cleanup_gpu_memory(verbose=True)
                log_memory_usage(memory_log_file, f"After cleanup: {embedding_model_name}")
                
                # Check thermal status periodically
                if (index + 1) % 2 == 0:  # Check every 2 models
                    print(f"[STEP 26.{index+2}] Checking thermal status...")
                    thermal_status = thermal_monitor.get_status()
                    if thermal_status['alerts']:
                        print(f"[STEP 26.{index+2}] ⚠️ Thermal alert:")
                        for alert in thermal_status['alerts']:
                            print(f"   {alert}")
                    else:
                        if thermal_status['gpu_temp']:
                            print(f"[STEP 26.{index+2}] ✓ GPU temperature: {thermal_status['gpu_temp']:.1f}°C")
                    
                    # Check for throttling
                    if thermal_status.get('throttling'):
                        print(f"[STEP 26.{index+2}] ⚠️ Thermal throttling detected: {thermal_status['throttling'][:80]}")
                
                print(f"[STEP 26.{index+2}] ✓ Completed processing {embedding_model_name}")

            except Exception as ex:
                embedding_model_name = embedding_model_names[index]
                print(f"[STEP 26.{index+2}] ❌❌❌ OPTIMIZATION ERROR ❌❌❌")
                print(f"[STEP 26.{index+2}] Model name: {embedding_model_name}")
                print(f"[STEP 26.{index+2}] Error type: {type(ex).__name__}")
                print(f"[STEP 26.{index+2}] Exception message: {ex}")
                print("[STEP 26.{index+2}] Full traceback:")
                print(traceback.format_exc())
                print("=" * 80)
                optimization_errors.append({embedding_model_name: ex})
                print(f"[STEP 26.{index+2}] Added error to optimization_errors (total: {len(optimization_errors)})")
        
        print(f"[STEP 26] ✓ Optimization loop completed for all {len(embedding_models)} models")
        
        # Stop background thermal monitoring
        print("[STEP 26.5] Stopping background thermal monitoring...")
        thermal_monitoring_active.clear()  # Signal thread to stop
        thermal_thread.join(timeout=5.0)  # Wait for thread to finish (max 5 seconds)
        print("[STEP 26.5.1] ✓ Background thermal monitoring stopped")
        
        # Stop GPU monitoring
        print("[STEP 26.6] Stopping GPU resource monitoring...")
        monitor.stop()
        print("[STEP 26.6.1] ✓ GPU monitoring stopped")
        
        # Final memory check
        print("[STEP 26.7] Final GPU memory check...")
        final_memory = print_gpu_memory_usage("Final GPU Memory", verbose=True)
        log_memory_usage(memory_log_file, "Final GPU Memory")
        
        # Final thermal check
        print("[STEP 26.8] Final thermal status check...")
        final_thermal = thermal_monitor.get_status()
        if final_thermal['alerts']:
            print("   ⚠️ Final thermal alerts:")
            for alert in final_thermal['alerts']:
                print(f"      {alert}")
        else:
            print("   ✓ Final temperatures within safe range")
        
        # Log final thermal status
        timestamp = datetime.now().isoformat()
        log_entry = {
            'timestamp': timestamp,
            'label': 'Final Thermal Status',
            'status': final_thermal
        }
        with open(thermal_log_file, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
        
        print(f"[STEP 26.9] ✓ Monitoring data saved:")
        print(f"   Memory log: {memory_log_file}")
        print(f"   Thermal log: {thermal_log_file}")

        # Inspect the optimization results
        print("[STEP 27] Inspecting optimization results...")
        print(f"[STEP 27.1] Total embedding models to inspect: {len(embedding_model_names)}")
        
        for index, embedding_model_name in enumerate(embedding_model_names):
            print(f"[STEP 27.{index+2}] Inspecting results for {embedding_model_name}...")
            file_path = os.path.join(optimization_results_dir, embedding_model_name, 'result.json')
            print(f"[STEP 27.{index+2}] Result file path: {file_path}")
            
            if os.path.exists(file_path):
                print(f"[STEP 27.{index+2}] File exists, loading JSON...")
                with open(file_path, 'r') as file:
                    res = json.load(file)
                    print(f"[STEP 27.{index+2}] ✓ JSON loaded")
                    print(f"[STEP 27.{index+2}] Result keys: {list(res.keys())}")
                    print(f"[STEP 27.{index+2}] Result content:")
                    pprint.pprint(res)
            else:
                print(f"[STEP 27.{index+2}] ⚠️ Result file does not exist: {file_path}")

        # send_mail(subject='✅ Optimization SUCCESS', contents='Saved optimization results successfully.')

        print("[STEP 28] Summary of errors...")
        print(f"[STEP 28.1] Training errors: {len(training_errors)} errors during {trainings_count} training runs")
        if training_errors:
            print("[STEP 28.2] Training error details:")
            for idx, error in enumerate(training_errors, 1):
                print(f"[STEP 28.2]   Error {idx}: {error}")
        print(f"[STEP 28.3] Optimization errors: {len(optimization_errors)} errors during {len(embedding_models)} optimization runs")
        if optimization_errors:
            print("[STEP 28.4] Optimization error details:")
            for idx, error in enumerate(optimization_errors, 1):
                print(f"[STEP 28.4]   Error {idx}: {error}")

        # Assuming you want to check the keys of the last loaded result
        if 'res' in locals():
            print("[STEP 29] Checking keys of last loaded result...")
            print(f"[STEP 29.1] Result keys: {list(res.keys())}")
        else:
            print("[STEP 29] No result loaded (res variable not available)")

        # res["f_val"]

    except Exception as ex:
        print("[MAIN] ❌❌❌ MAIN EXCEPTION OCCURRED ❌❌❌")
        print(f"[MAIN] Exception type: {type(ex).__name__}")
        print(f"[MAIN] Exception arguments: {ex.args}")
        template = "An exception of type {0} occurred. Arguments:\n{1!r}"
        message = template.format(type(ex).__name__, ex.args)
        print(f"[MAIN] Formatted message:")
        print(message)
        print("[MAIN] Full traceback:")
        traceback.print_exc()
        print("=" * 80)
