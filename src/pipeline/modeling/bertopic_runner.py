# -*- coding: utf-8 -*-
"""mine_bertopic_plus_octis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STfqRpfRG5lQs0TDVgMFp41LvwIavhFw
"""

dataset_path = "./data/processed_novels_sentences_new.csv"
octis_dataset_path = "./data/octis"

import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

import cudf
print("RAPIDS Test:")
print(cudf.Series([1, 2, 3]))

import subprocess
import sys
import importlib

# https://github.com/anderskm/gputil#monitor-gpu-in-a-separate-thread
import GPUtil
from numba import cuda
from threading import Thread
import time
import os
import pickle
import traceback
import gensim.corpora as corpora

# To monitor GPU/RAM usage
class Monitor(Thread):
    def __init__(self, delay):
        super(Monitor, self).__init__()
        self.stopped = False
        self.delay = delay  # Time between calls to GPUtil
        self.start()

    def run(self):
        while not self.stopped:
            GPUtil.showUtilization()
            time.sleep(self.delay)

    def stop(self):
        self.stopped = True


# List of required libraries with their import names if different
required_libraries = {
    "bertopic": "bertopic",
    "octis": "octis",
    "sentence-transformers": "sentence_transformers",
    "umap-learn": "umap",
    "hdbscan": "hdbscan",
    "tqdm": "tqdm",
    "pandas": "pandas",
    "gensim": "gensim",
    "scipy": "scipy"
}


# Function to install a library
def install_and_import(package, import_name=None):
    if import_name is None:
        import_name = package
    try:
        importlib.import_module(import_name)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[import_name] = importlib.import_module(import_name)


# Install and import required libraries
for package, import_name in required_libraries.items():
    install_and_import(package, import_name)

# Import libs

import pandas as pd
import re
import numpy as np
from tqdm import tqdm
import csv
import json
import pprint

from skopt.space.space import Real, Categorical, Integer

from scipy.sparse import csr_matrix

# from umap import UMAP
# from hdbscan import HDBSCAN

from cuml.cluster import HDBSCAN
from cuml.manifold import UMAP

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech
from bertopic.vectorizers import ClassTfidfTransformer

from octis.optimization.optimizer import Optimizer
from octis.evaluation_metrics.diversity_metrics import TopicDiversity
from octis.evaluation_metrics.coherence_metrics import Coherence
from octis.dataset.dataset import Dataset
from octis.models.model import AbstractModel

import pandas as pd
import csv
import re

import smtplib
from email.message import EmailMessage

import smtplib
from email.mime.text import MIMEText

import gc
import torch

def send_mail(contents, subject):
   from_email = "bakhpol@gmail.com"
   from_password = "sknnwtkvivppckzy"
   to_email = "bakhpol@gmail.com"

   msg = MIMEText(contents, 'html')
   msg['Subject'] = subject
   msg['To'] = to_email
   msg['From'] = from_email


   # Create SMTP session for sending the mail
   gmail = smtplib.SMTP('smtp.gmail.com', 587)
   gmail.ehlo()
   gmail.starttls()
   # Login to gmail account
   gmail.login(from_email, from_password)
   # Send mail
   gmail.send_message(msg)




with torch.no_grad():
    try:
        # send_mail(subject='⏳ Optimization BEGIN', contents='Begin run, test email.')

        # Dataset paths
        dataset_path = "./data/processed_novels_sentences_new.csv"
        octis_dataset_path = "./data/octis"


        # Initialize variables
        all_rows = []

        # Read the file using the csv module to handle quoting correctly
        with open(dataset_path, 'r', encoding='latin1', errors='ignore') as file:
            reader = csv.reader(file, quotechar='"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True)
            headers = next(reader)  # Get the headers
            for row in reader:
                if len(row) == 4:
                    all_rows.append(row)

        # Convert all rows to DataFrame
        df = pd.DataFrame(all_rows, columns=headers)

        # Logging the number of all rows
        print(f"Total lines: {len(all_rows) + 1}")  # +1 for the header
        print(f"All rows: {len(all_rows)}")

        # Ensure 'Sentence' column exists and is correctly named
        if 'Sentence' not in df.columns:
            print("The 'Sentence' column does not exist. Check column names and separator.")
        else:
            # Cleaning the 'Sentence' column
            df['Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'\n+', ' ', str(x)) if isinstance(x, str) else str(x))
            df['Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'\s+', ' ', x).strip().lower())

            # Print the cleaned data to verify
            print(df.head())

            # Get sentences as list
            dataset_as_list_of_strings = df['Sentence'].tolist()
            print(f"Total sentences in dataset: {df.shape[0]}")

            # Debug: Check for duplicates before converting to lists
            duplicate_sentences = df['Sentence'].duplicated().sum()
            print(f"Number of duplicate sentences before conversion: {duplicate_sentences}")

            # Convert sentences to list of lists
            dataset_as_list_of_lists = [sentence.split() for sentence in dataset_as_list_of_strings]
            print(len(dataset_as_list_of_lists))

            # Debug: Check for duplicates after conversion
            dataset_as_strings_from_lists = [' '.join(sentence) for sentence in dataset_as_list_of_lists]
            duplicates_after_conversion = len(dataset_as_strings_from_lists) - len(set(dataset_as_strings_from_lists))
            print(f"Number of duplicate sentences after conversion: {duplicates_after_conversion}")

        # Prepare the dataset for OCTIS
        # https://github.com/MIND-Lab/OCTIS?tab=readme-ov-file#load-a-custom-dataset

        # Save the properly formatted dataset to a file, for OCTIS to import it
        octis_corpus_path = octis_dataset_path + '/corpus.tsv'

        # Read the CSV file
        with open(dataset_path, mode='r', newline='', encoding='latin1') as csv_file:
            csv_reader = csv.reader(csv_file)
            header = next(csv_reader)  # Skip the header

            # Prepare the data for the TSV file
            tsv_data = []
            for row in csv_reader:
                if len(row) == 4:
                    author, book_title, chapter, sentence = row
                    partition = 'train'
                    label = author + "," + book_title
                    tsv_data.append([sentence, partition, label])

        # Write the data to a TSV file
        with open(octis_corpus_path, mode='w', newline='', encoding='utf-8') as tsv_file:
            tsv_writer = csv.writer(tsv_file, delimiter='\t')
            for row in tsv_data:
                tsv_writer.writerow(row)

        octis_dataset = Dataset()
        octis_dataset.load_custom_dataset_from_folder(octis_dataset_path)

        import spacy
        from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance

        # Additional Representations
        keybert_model = KeyBERTInspired()

        # Part-of-Speech
        pos_model = PartOfSpeech("en_core_web_sm")

        # MMR
        mmr_model = MaximalMarginalRelevance(diversity=0.3)

        # All representation models
        representation_model = {
            "KeyBERT": keybert_model,
            "MMR": mmr_model,
            "POS": pos_model
        }

        # Implementing custom Model for OCTIS
        # https://github.com/MIND-Lab/OCTIS?tab=readme-ov-file#implement-your-own-model

        trainings_count = 0
        training_errors = []

        class BERTopicOctisModelWithEmbeddings(AbstractModel):
            def __init__(self, embedding_model, embedding_model_name, embeddings, dataset_as_list_of_strings):
                super().__init__()

                self.embedding_model = embedding_model
                self.embedding_model_name = embedding_model_name
                self.embeddings = embeddings

                self.use_partitions = False

                # Default parameters
                self.hyperparameters = {
                    'umap': {
                        'n_neighbors': 11,
                        'n_components': 5,
                        'min_dist': 0.05,
                        'metric': 'cosine',
                        'random_state': 42
                    },
                    'hdbscan': {
                        'min_cluster_size': 150,
                        'metric': 'euclidean',
                        'cluster_selection_method': 'eom',
                        'prediction_data': True,
                        'gen_min_span_tree': True,
                        'min_samples': 20
                    },
                    'vectorizer': {
                        'stop_words': 'english',
                        'min_df': 0.005,
                        'ngram_range': (1, 1)
                    },
                    'tfdf_vectorizer': {
                        'reduce_frequent_words': True,
                        'bm25_weighting': True
                    },
                    'bertopic': {
                        'language': "english",
                        'top_n_words': 30,
                        'n_gram_range': (1, 1),
                        'min_topic_size': 127,
                        'nr_topics': None,
                        'low_memory': False,
                        'calculate_probabilities': True,
                        'verbose': True
                    }
                }

            # Override OCTIS AbstractModel train_model method
            def train_model(self, dataset, hyperparameters={}, top_words=10):
                global trainings_count
                trainings_count += 1

                print("Training #", trainings_count)
                file_path = os.path.join('./data/octis/optimization_results/', self.embedding_model_name, 'result.json')

                if os.path.exists(file_path):
                    with open(file_path, 'r') as file:
                        data = json.load(file)
                        result_current_call = data.get('current_call', 0)
                        # print("Result current call:",result_current_call)
                        if result_current_call > 1 and trainings_count == 1:
                            print("!!! Skipping training run to avoid potential error")
                            output_dict = {'topics': None}
                            return output_dict


                self.set_hyperparameters(hyperparameters)
                print("Training with parameters:")
                pprint.pprint(self.hyperparameters)

                # Preventing Stochastic Behavior
                # https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2#scrollTo=28_EVoOfyZLb
                umap_model = UMAP(**self.hyperparameters['umap'])

                # Controlling Number of Topics
                # https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2#scrollTo=TH6vZPGU2zpg
                hdbscan_model = HDBSCAN(**self.hyperparameters['hdbscan'])

                # Improving Default Representation
                # https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2#scrollTo=66zgeCyf0jy3&line=1&uniqifier=1
                vectorizer_model = CountVectorizer(**self.hyperparameters['vectorizer'])

                # Using ClassTfidfTransformer instead of CountVectorizer
                tfdf_model = ClassTfidfTransformer(**self.hyperparameters['tfdf_vectorizer'])

                topic_model = BERTopic(

                    # Pipeline models
                    embedding_model=self.embedding_model,
                    umap_model=umap_model,
                    hdbscan_model=hdbscan_model,
                    vectorizer_model=vectorizer_model,
                    representation_model=representation_model,

                    # Hyperparameters
                    **self.hyperparameters['bertopic']
                )

                try:
                    topics, probabilities = topic_model.fit_transform(dataset_as_list_of_strings, embeddings=self.embeddings)

                    print("Cleaning CUDA cache...")
                    torch.cuda.empty_cache()
                    gc.collect()  # Force garbage collection

                    # cuda.select_device(0)
                    # cuda.close()
                    # cuda.select_device(0)

                    # Creating the required output dictionary
                    output_dict = {}
                    output_dict['topics'] = []

                    # cleaned_docs = topic_model._preprocess_text(dataset_as_list_of_strings)
                    #
                    # # Extract vectorizer and analyzer from BERTopic
                    # vectorizer = topic_model.vectorizer_model
                    # analyzer = vectorizer.build_analyzer()
                    # tokens = [analyzer(doc) for doc in cleaned_docs]
                    dictionary = corpora.Dictionary(dataset_as_list_of_lists)

                    # https://github.com/MaartenGr/BERTopic/issues/90#issuecomment-820915389
                    for topic in range(len(set(topics)) - topic_model._outliers):
                        words = list(zip(*topic_model.get_topic(topic)))[0]
                        words = [word for word in words if word in dictionary.token2id]
                        words = [word for word in words if word.lower() != "mr"]
                        words = [word for word in words if word.lower() != "ms"]
                        output_dict['topics'].append(words)

                    output_dict['topics'] = [words for words in output_dict['topics'] if len(words) > 0]

                    # # 1. topics: list of most significant words for each topic
                    # for topic_number in set(topics):
                    #     if topic_number != -1:
                    #         topic_words = topic_model.get_topic(topic_number)
                    #         if topic_words:
                    #             output_dict['topics'].append(topic_words)
                    #
                    # output_dict['topics'] = [[word for word, _ in topic if word] for topic in output_dict['topics'] if
                    #                         all(word.strip() for word, _ in topic)]
                    #
                    # output_dict['topics'] = [words for words in output_dict['topics'] if len(words) > 0]


                    # If the array is empty, return None, so that OCTIS doesn't process this result
                    if not output_dict['topics']:
                        output_dict['topics'] = None

                    # 2. topic-word-matrix: c-TF-IDF matrix
                    output_dict['topic-word-matrix'] = np.array(topic_model.c_tf_idf_)

                    # 3. topic-document-matrix: probabilities matrix
                    output_dict['topic-document-matrix'] = np.array(probabilities)

                    pprint.pprint(output_dict['topics'])

                    del topic_model

                    return output_dict
                except Exception as ex:
                    print(">>>>>>>>>>>>>>>>>>>>>****TRAINING ERROR****<<<<<<<<<<<<<<<<<<<")
                    print("Hyperparameters:", hyperparameters)
                    print("Exception:", ex)
                    print("==================================================================")
                    training_errors.append({'hyperparameters': hyperparameters, 'exception': ex, 'run_count': trainings_count})
                    output_dict = {'topics': None}
                    return output_dict

            def set_hyperparameters(self, hyperparameters):
                # Set hyperparameter from a flat dictionary where keys are in the form 'section__hyperparameter', e.g. 'umap__n_neighbors'.
                for key, value in hyperparameters.items():
                    if '__' in key:
                        section, hyperparameter = key.split('__', 1)
                        if section in self.hyperparameters and hyperparameter in self.hyperparameters[section]:
                            self.hyperparameters[section][hyperparameter] = value
                        else:
                            print(f"Warning: Parameter '{key}' is not recognized.")
                    else:
                        print(f"Warning: Parameter '{key}' does not match the expected format 'section__hyperparameter'.")


        embedding_model_names = [
            # "paraphrase-MiniLM-L6-v2",
            # "paraphrase-mpnet-base-v2",
            "all-MiniLM-L12-v2",
            # "paraphrase-distilroberta-base-v1",
            # "whaleloops/phrase-bert",
            "multi-qa-mpnet-base-cos-v1"
        ]

        embedding_models = []

        for embedding_model_name in embedding_model_names:
            #embedding_model = SentenceTransformer(embedding_model_name)
            embedding_model = SentenceTransformer(embedding_model_name, device="cuda")

            embedding_models.append(embedding_model)

        # Store the pre-calculated embeddings
        # https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2#scrollTo=sVfnYtUaxyLT
        precalculated_embeddings = []


        def save_embeddings(embeddings, file_path):
            """
            Save embeddings to a file using pickle.

            Args:
            embeddings (numpy.ndarray or list of numpy.ndarray): Embedding array(s) to save
            file_path (str): Path to save the embeddings
            """
            if not isinstance(embeddings, (list, np.ndarray)):
                raise ValueError("Embeddings must be a numpy array or a list of numpy arrays")

            with open(file_path, 'wb') as f:
                pickle.dump(embeddings, f)
            print(f"Embeddings saved to {file_path}")


        def load_embeddings(file_path):
            """
            Load embeddings from a file.

            Args:
            file_path (str): Path to the saved embeddings file

            Returns:
            numpy.ndarray or list of numpy.ndarray: Loaded embeddings
            """
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    embeddings = pickle.load(f)
                print(f"Embeddings loaded from {file_path}")
                return embeddings
            else:
                print(f"No saved embeddings found at {file_path}")
                return None


        # Example usage
        embedding_file = "precalculated_embeddings.pkl"

        # BERTopic works by converting documents into numerical values, called embeddings.
        # This process can be very costly, especially if we want to iterate over parameters.
        # Instead, we can calculate those embeddings once and feed them to BERTopic
        # to skip calculating embeddings each time.
        for embedding_model in embedding_models:
            # Check if embeddings already exist
            if os.path.exists(embedding_file):
                precalculated_embeddings = load_embeddings(embedding_file)
                break
            else:
                embeddings = embedding_model.encode(dataset_as_list_of_strings, show_progress_bar=True)
                precalculated_embeddings.append(embeddings)

        # Save embeddings after calculation (if they didn't exist before)
        if not os.path.exists(embedding_file):
            save_embeddings(precalculated_embeddings, embedding_file)

        def optimize_hyperparameters(model, label, dataset_as_list_of_lists):
            search_space = {
                'umap__n_neighbors': Integer(2, 50),
                'umap__n_components': Integer(2, 10),
                'umap__min_dist': Real(0.0, 0.1),
                'hdbscan__min_cluster_size': Integer(50, 500),
                'hdbscan__min_samples': Integer(10, 100),
                'vectorizer__min_df': Real(0.001, 0.01),
                'bertopic__top_n_words': Integer(10, 40),
                'bertopic__min_topic_size': Integer(10, 250)
            }

            # As a rule of thumb, if you have N hyperparameters to optimize, then you should make at least 15 times N iterations.
            # https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb#scrollTo=njjkNjl9CJW8&line=1&uniqifier=1
            optimization_runs = len(search_space) * 15 # should be 15

            # Topic models are usually probabilistic and thus produce different results even with the same hyperparameter configuration.
            # So we run the model multiple times and then take the median of the evaluated metric to get a more reliable result.
            model_runs = 1 # 1 is enough

            topk = 10

            dataset_as_list_of_lists = [doc for doc in dataset_as_list_of_lists if len(doc) > 0]

            # Use a different coherence measure
            npmi = Coherence(texts=dataset_as_list_of_lists, topk=topk, measure='c_v')
            diversity = TopicDiversity(topk=10)  # Initialize metric

            optimizer = Optimizer()

            results_path = './data/octis/optimization_results/' + label + '/'

            # if os.path.isfile(results_path):
            optimization_result = optimizer.resume_optimization(results_path + 'result.json', model=model)
            return optimization_result
            # else:
            #     optimization_result = optimizer.optimize(
            #         model, octis_dataset, npmi, search_space, number_of_call=optimization_runs,
            #         model_runs=model_runs, save_models=True,
            #         extra_metrics=[diversity],  # to keep track of other metrics
            #         save_path=results_path
            #     )
            #     return optimization_result

        # @title
        # Original Code!
        optimization_results = []

        # for index, embedding_model in enumerate(embedding_models):
        #     embedding_model_name = embedding_model_names[index]
        #     print("Instantiating BERTopicOctisModelWithEmbeddings for ", embedding_model_name)
        #     model = BERTopicOctisModelWithEmbeddings(embedding_model=embedding_model,
        #                                              embeddings=precalculated_embeddings[index],
        #                                              dataset_as_list_of_strings=dataset_as_list_of_strings)
        #
        #     print("Optimizing hyperparameters for ", embedding_model_name)
        #     optimization_result = optimize_hyperparameters(model, embedding_model_name)
        #     optimization_results.append(optimization_result)
        #     print("=== Optimized hyperparameters for embedding_model_name:", embedding_model_name)

        optimization_errors = []
        # monitor = Monitor(10)


        def optimize_with_restart(model, embedding_model_name, dataset_as_list_of_lists, max_retries=3, delay=5):
            """
            Attempts to run optimize_hyperparameters and restarts if an exception occurs.

            :param model: The model to optimize
            :param embedding_model_name: Name of the embedding model
            :param dataset_as_list_of_lists: The dataset
            :param max_retries: Maximum number of retry attempts
            :param delay: Delay in seconds between retries
            :return: The optimization result or None if all attempts fail
            """
            for attempt in range(max_retries):
                try:
                    print(f"Attempt {attempt + 1} of {max_retries}")
                    optimization_result = optimize_hyperparameters(model, embedding_model_name,
                                                                   dataset_as_list_of_lists)
                    return optimization_result
                except Exception as e:
                    print(f"An error occurred during optimization (Attempt {attempt + 1}):")
                    print(traceback.format_exc())
                    # if attempt < max_retries - 1:
                    print(f"Restarting in {delay} seconds...")
                    time.sleep(delay)
                    # else:
                    #     print("Max retries reached. Optimization failed.")

            return None

        for index, embedding_model in enumerate(embedding_models):
            try:
                embedding_model_name = embedding_model_names[index]
                print("Instantiating BERTopicOctisModelWithEmbeddings for ", embedding_model_name)
                model = BERTopicOctisModelWithEmbeddings(embedding_model=embedding_model,
                                                         embedding_model_name=embedding_model_names[index],
                                                        embeddings=precalculated_embeddings[index],
                                                        dataset_as_list_of_strings=dataset_as_list_of_strings)

                print("Optimizing hyperparameters for ", embedding_model_name)
                optimization_result = optimize_with_restart(model, embedding_model_name, dataset_as_list_of_lists)
                # optimization_results.append(optimization_result)
                print("=== Optimized hyperparameters for embedding_model_name:", embedding_model_name)

                # Clean up
                del model
                del optimization_result

            except Exception as ex:
                embedding_model_name = embedding_model_names[index]
                print(">>>>>>>>>>>>>>>>>>>>>****OPTIMIZATION ERROR****<<<<<<<<<<<<<<<<<<<")
                print("Model name:", embedding_model_name)
                print("Exception:")
                print(ex)
                print("==================================================================")
                optimization_errors.append({embedding_model_name: ex})

        # monitor.stop()

        # Inspect the optimization results
        for index, embedding_model_name in enumerate(embedding_model_names):
            file_path = f"./data/octis/optimization_results/{embedding_model_name}/result.json"
            with open(file_path, 'r') as file:
                res = json.load(file)
                pprint.pprint(res)

        # send_mail(subject='✅ Optimization SUCCESS', contents='Saved optimization results successfully.')

        print(f"The following errors ({len(training_errors)}) have occurred during the individual training runs ({trainings_count})", training_errors)
        print(f"The following errors ({len(optimization_errors)}) have occurred during the whole optimization runs ({len(embedding_models)}):", optimization_errors)

        # Assuming you want to check the keys of the last loaded result
        print(res.keys())

        # res["f_val"]

    except Exception as ex:
        template = "An exception of type {0} occurred. Arguments:\n{1!r}"
        message = template.format(type(ex).__name__, ex.args)
        print(message)
        send_mail(subject='⛔ Optimization ERROR', contents=message)
