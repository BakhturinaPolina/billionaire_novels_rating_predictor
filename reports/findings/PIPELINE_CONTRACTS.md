# Pipeline Output Contracts

This document defines the expected schema and location of key pipeline outputs to prevent "silent drift" in downstream stages.

## Results Directory Structure

### `results/experiments/model_evaluation_results.csv`

**Purpose:** Summary ledger of all model evaluation results across all experiments.

**Expected Columns:**
- `embedding_model` (str): Name of embedding model (e.g., "all-MiniLM-L12-v2")
- `study_id` (str): Study identifier (e.g., "study_001")
- `nr_topics` (int): Number of topics discovered
- `coherence` (float): Coherence score (c_v or c_npmi)
- `diversity` (float): Topic diversity score
- `timestamp` (str): Experiment timestamp
- `config_hash` (str): Hash of hyperparameters used

**Location:** `results/experiments/model_evaluation_results.csv`

**Generated by:** Stage 04 (Experiments)

---

### `results/pareto/pareto.csv`

**Purpose:** Pareto-efficient models selected from all experiments, filtered by constraints.

**Expected Columns:**
- `embedding_model` (str): Name of embedding model
- `study_id` (str): Study identifier
- `nr_topics` (int): Number of topics (must be >= 200 per `configs/selection.yaml`)
- `coherence` (float): Coherence score
- `diversity` (float): Topic diversity score
- `pareto_rank` (int): Rank in Pareto frontier (1 = best)
- `is_pareto_efficient` (bool): Whether model is on Pareto frontier

**Location:** `results/pareto/pareto.csv`

**Generated by:** Stage 05 (Selection)

**Constraints:**
- `min_nr_topics >= 200` (enforced from `configs/selection.yaml`)

---

### `results/topics/by_book.csv`

**Purpose:** Topic probabilities per book, mapping topics to individual books.

**Expected Columns:**
- `book_id` (str): Unique book identifier
- `topic_0`, `topic_1`, ..., `topic_N` (float): Probability of each topic for this book
- `dominant_topic` (int): Index of topic with highest probability
- `topic_entropy` (float): Entropy of topic distribution (diversity measure)

**Location:** `results/topics/by_book.csv`

**Generated by:** Stage 03 (Modeling) or Stage 05 (Selection)

---

## Results Directory Organization

### Experiments
```
results/experiments/
├── model_evaluation_results.csv          # Summary ledger
├── <embedding_model>/
│   └── <study_id>/
│       ├── result.json                   # Full experiment log
│       └── optimization_results/        # OCTIS results (if applicable)
```

### Topics
```
results/topics/
├── by_book.csv                           # Topic probabilities per book
├── top_models/                           # Top model topics
├── top_models_with_coherence/            # Topics with coherence scores
└── top_models_no_coherence/              # Topics without coherence scores
```

### Pareto
```
results/pareto/
├── pareto.csv                            # Pareto-efficient models
└── topics/
    └── pareto_efficient_per_model/       # Topic JSONs for Pareto models
```

### Analysis
```
results/analysis/
└── reproducible_scripts_topics_vs_readers_appreciation/  # Analysis outputs
```

---

## Validation

Run `make contracts` to display expected output locations.

Each stage entrypoint (`src/stage0X/main.py`) displays its inputs and outputs when run, providing a clear contract for what it expects and produces.

